\section{Conclusion}

\written{We conclude that the key to storing large files is to break them into smaller chunks, and that the key to storing many small chunks is to aggregate them into larger files.}

We have performed experiments to probe the scalability limits of existing \gls{DAG}-based \acrlongpl{DVCS}.
We have shown that the maximum size of file that Git and Mercurial can store is limited by the amount of available memory in the system.
We conclude that this is because those systems calculate deltas of files to de-duplicate data, and they load the entire file into memory in order to do so.

We have also rediscovered the limits of the Unix filesystem for storing many small files.
We saw that writing files smaller than the filesystem block size incurs storage overhead, that splitting files among too many subdirectories takes \glspl{inode} that are needed to store files, and that jumping between directories when writing files incurs write-speed penalties.

We have shown that any \gls{VCS} that stores objects as individual files on the filesystem will encounter these filesystem limitations as they try to scale in terms of number of files.
A \gls{VCS} that also breaks files into chunks will turn the problem of storing large files into the problem of storing many files, again encountering these limitations.
However, the limitations can be avoided by aggregating objects into \glspl{packfile} as Bup does.

\towrite{We intend to incorporate these insights into future versions of DMV.}
