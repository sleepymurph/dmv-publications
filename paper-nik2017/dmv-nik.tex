\documentclass[
    usenglish,
    %final,
]{nik}

\input{dmv-nik-packages.tex}
\input{dmv-nik-custom-commands.tex}

\title{Exploring the Scalability of Distributed Version Control Systems}
\author{Michael J. Murphy \and Otto J. Anshus \and John Markus Bj√∏rndalen}
\date{November 2017}

\begin{document}
\maketitle

\begin{abstract}
\input{dmv-nik-abstract.txt}
\end{abstract}

\section{Introduction}

\towrite{Distributed version control is an interesting form of distributed system because it takes eventual consistency to the extreme.}

\towrite{Every replica of a repository contains the full history in an append-only data structure, any replica may add new commits, and conflicting updates are reconciled later in a merge operation.}


\written{These systems are popular, but their use is generally limited to the small text files of source code.}

\Glspl{DVCS} are designed primarily to store program source code: plain text files in the range of tens of kilobytes.
Checking in larger binary files such as images, sound, or video affects performance.
Actions that require copying data in and out of the system slow from hundredths of a second to full seconds or minutes.
And since a \gls{DVCS} keeps every version of every file in every \gls{repository}, forever, the disk space needs compound.

This has lead to a conventional wisdom that binary files should never be stored in version control, inspiring blog posts with titles such as
"Don't ever commit binary files to Git! Or what to do if you do"~\cite{dont_ever_commit_binaries_to_version_control},
even as the modern software development practice of continuous delivery was commanding teams to "keep absolutely everything in version control."~\cite[p.33]{continuousdeliverybook}

\towrite{This paper explores the challenges of using version control to store larger binary files, with the goal of building a scalable, highly-available, distributed storage system for media files such as images, audio, and video.}

\section{Distributed Media Versioning}

\towrite{We developed an early prototype of such a system, which we call Distributed Media Versioning (DMV).}

\section{Experiments}

\written{We perform experiments with the popular version control systems Git and Mercurial, the Git-based backup tool Bup, and our DMV prototype.}

\written{We measured commit times and repository sizes when storing single files of increasing size, and when storing increasing numbers of single-kilobyte files.}

\subsection{Methodology}

We conducted two major experiments.
In order to measure the effect of file size, we would \gls{commit} a single file of increasing size to each target \gls{VCS}.
And to measure the effect of numbers of files, we would \gls{commit} increasing number of small (\SI{1}{\kibi\byte}) files to each target \gls{VCS}.

\towrite{List VCSs in a more compact way than in thesis}

For each experiment, the procedure for a single trial was as follows:
\begin{tight_enumerate}
    \item Create an empty \gls{repository} of the target \gls{VCS} in a temporary directory
    \item Generate target data to store, either a single file of the target size, or the target number of \SI{1}{\kibi\byte} files
    \item \Gls{commit} the target data to the \gls{repository}, measuring wall-clock time to \gls{commit}
    \item Verify that the first \gls{commit} exists in the \gls{repository}, and if there was any kind of error, run the \gls{repository}'s integrity check operation
    \item Measure the total \gls{repository} size
    \item Overwrite a fraction of each target file
    \item (Number-of-files experiment only) Run the \gls{VCS}'s status command that lists what files have changed, and measure the wall-clock time that it takes to complete
    \item \Gls{commit} again, measuring wall-clock time to \gls{commit}
    \item Verify that the second \gls{commit} exists in the \gls{repository}, and if there was any kind of error, run the \gls{repository}'s integrity check operation
    \item Measure the total \gls{repository} size again
    \item (File-size experiment only) Run Git's garbage collector (\lstinline{git fsck}) to pack objects, then measure total \gls{repository} size again
    \item Delete temporary directory and all trial files
\end{tight_enumerate}

We increased file sizes exponentially by powers of two from \SI{1}{\byte} up to \SI{128}{\gibi\byte}, adding an additional step at \num{1.5} times the base size at each order of magnitude.
For example, starting at \SI{1}{\mebi\byte}, we would run trails with \SI{1}{\mebi\byte}, \SI{1.5}{\mebi\byte}, \SI{2}{\mebi\byte}, \SI{3}{\mebi\byte}, \SI{4}{\mebi\byte}, \SI{6}{\mebi\byte}, \SI{8}{\mebi\byte}, \SI{12}{\mebi\byte}, and so on.

We increased numbers of files exponentially by powers of ten from one file to ten million files, adding additional steps at \num{2.5}, \num{5}, and \num{7.5} times the base number at each order of magnitude.
For example, starting at \num{100} files we would run trials with \num{100}, \num{250}, \num{500}, \num{750}, \num{1000}, \num{2500}, \num{5000}, \num{7500}, \num{10000}, and so on.

Input data files consisted of pseudorandom bytes taken from the operating system's pseudorandom number generator (\lstinline{/dev/urandom} on Linux).

When updating data files for the second \gls{commit}, we would overwrite a single contiguous section of each file with new pseudorandom bytes.
We would start one-quarter of the way into the file, and overwrite \num{1/1024}th of the file's size (or 1 byte if the file was smaller than \SI{1024}{\kibi\byte}).
So a \SI{1}{\mebi\byte} file would have \SI{1}{\kibi\byte} overwritten, a \SI{1}{\gibi\byte} file would have \SI{1}{\mebi\byte} overwritten, and so on.

%

\subsection{Experiment Platform}

We ran the trials on four dedicated computers with no other load.
Each was a typical office desktop with a \SI{3.16}{\giga\hertz} \num{64}-bit dual-core processor and \SI{8}{\gibi\byte} of RAM, running Debian version 8.6 ("Jessie").
Each computer had one normal SATA hard disk (spinning platter, not solid-state), and trials were conducted on a dedicated \SI{197}{\gibi\byte} LVM partition formatted with the ext4 filesystem.
All came from the same manufacturer with the same specifications and were, for practical purposes, identical.
%Additional details can be found in \autoref{test-machine-specs}.
\todo{Include platform table?}

We ran every trial four times, once on each of the experiment computers, and took the mean and standard deviation of each time and disk space measurement.
However, because the experiment computers are practically identical, there was little real variation.

%

\section{Results}

\towrite{We find that processing files whole will limit maximum file size to what can fit in RAM.}

\towrite{And we find that storing millions of objects loose as files with hash-based names will result in inefficient write speeds and use of disk space.}

\section{Conclusion}

\towrite{We conclude that the key to storing large files is to break them into smaller chunks, and that the key to storing many small chunks is to aggregate them into larger files.}

\towrite{We intend to incorporate these insights into future versions of DMV.}


\printbibliography[]

\listoftodos

\ifoptionfinal{}{
    \input{dmv-nik-scratchpad.tex}
}

\end{document}
