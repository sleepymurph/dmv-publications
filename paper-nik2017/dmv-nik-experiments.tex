\section{Experiments}

\written{We perform experiments with the popular version control systems Git and Mercurial, the Git-based backup tool Bup, and our DMV prototype.}

\written{We measured commit times and repository sizes when storing single files of increasing size, and when storing increasing numbers of single-kilobyte files.}

\subsection{Methodology}

We conducted two major experiments.
In order to measure the effect of file size, we committed a single file of increasing size to a each target \gls{VCS}.
And to measure the effect of numbers of files, we committed increasing number of small (\SI{1}{\kibi\byte}) files to each target \gls{VCS}.

We ran each experiment with four different \glspl{VCS}: Git, Mercurial, Bup, the DMV prototype.
We chose Git because it is the most popular \gls{DVCS} in use today~\cite{what_are_devs_talking_about}.
We chose the Mercurial and Bup because they are both related to Git but each store data differently.
Git and DMV both store objects in an \gls{objectstore} directory as a file named for its hash ID.
Git has a separate garbage collection step that takes object files and aggregates them into \glspl{packfile}~\cite[Section 10.7]{git_book}.
Mercurial stores revisions of each file as a base revision followed by a series of deltas~\cite[Chapter 4]{hgbook}, much like previous \glspl{VCS} such as RCS, CVS, and Subversion~\cite{history_of_version_control}.
Bup uses Git's exact data model and \gls{packfile} format, but Bup breaks files into chunks using a \gls{rollinghash}, reusing Git's \gls{tree} object as a \gls{chunkedblob} index\footnotemark.
Unlike Git, Bup writes to the \gls{packfile} format directly, without Git's separate commit and pack steps, and without bothering to calculate deltas~\cite{bup_design}.
As a control, we also ran the experiments with a dummy \gls{VCS} that simply copied the files to a hidden directory.

\footnotetext{Git can read a repository written by Bup, but it will see
the large file as a directory full of smaller chunk files.}

For each experiment, the procedure for a single trial was as follows:
\begin{tight_enumerate}
    \item Create an empty \gls{repository} of the target \gls{VCS} in a temporary directory
    \item Generate target data to store, either a single file of the target size, or the target number of \SI{1}{\kibi\byte} files
    \item \Gls{commit} the target data to the \gls{repository}, measuring wall-clock time to \gls{commit}
    \item Verify that the first \gls{commit} exists in the \gls{repository}, and if there was any kind of error, run the \gls{repository}'s integrity check operation
    \item Measure the total \gls{repository} size
    \item Overwrite a fraction (\num{1/1024}) of each target file
    \item (Number-of-files experiment only) Run the \gls{VCS}'s status command that lists what files have changed, and measure the wall-clock time that it takes to complete
    \item \Gls{commit} again, measuring wall-clock time to \gls{commit}
    \item Verify that the second \gls{commit} exists in the \gls{repository}, and if there was any kind of error, run the \gls{repository}'s integrity check operation
    \item Measure the total \gls{repository} size again
    \item (File-size experiment only) Run Git's garbage collector (\lstinline{git fsck}) to pack objects, then measure total \gls{repository} size again
    \item Delete temporary directory and all trial files
\end{tight_enumerate}

We increased file sizes exponentially by powers of two from \SI{1}{\byte} up to \SI{128}{\gibi\byte}, adding an additional step at \num{1.5} times the base size at each order of magnitude.
For example, on the megabyte scale, the file sizes are \SI{1}{\mebi\byte}, \SI{1.5}{\mebi\byte}, \SI{2}{\mebi\byte}, \SI{3}{\mebi\byte}, \SI{4}{\mebi\byte}, \SI{6}{\mebi\byte}, \SI{8}{\mebi\byte}, \SI{12}{\mebi\byte}, and so on.

We increased numbers of files exponentially by powers of ten from one file to ten million files, adding additional steps at \num{2.5}, \num{5}, and \num{7.5} times the base number at each order of magnitude.
For example, at the hundreds and thousands scales, the file quantities are \num{100}, \num{250}, \num{500}, \num{750}, \num{1000}, \num{2500}, \num{5000}, \num{7500}, \num{10000}, and so on.

Input data files consisted of pseudorandom bytes taken from the operating system's pseudorandom number generator (\lstinline{/dev/urandom} on Linux).

%

\subsection{Experiment Platform}

We ran the trials on four dedicated computers with no other load.
Each was a typical office desktop with a \SI{3.16}{\giga\hertz} \num{64}-bit dual-core processor and \SI{8}{\gibi\byte} of RAM, running Debian version 8.6 ("Jessie").
Each computer had one normal SATA hard disk (spinning platter, not solid-state), and trials were conducted on a dedicated \SI{197}{\gibi\byte} LVM partition formatted with the ext4 filesystem.
All came from the same manufacturer with the same specifications and were, for practical purposes, identical.
%Additional details can be found in \autoref{test-machine-specs}.
\todo{Include platform table?}

We ran every trial four times, once on each of the experiment computers, and took the mean and standard deviation of each time and disk space measurement.
However, because the experiment computers are practically identical, there was little variation.

%
