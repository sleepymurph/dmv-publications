\section{Introduction}

\written{Distributed version control is an interesting form of distributed system because it takes eventual consistency to the extreme.}

Distributed systems are ruled by the \gls{CAP-theorem}~\cite{cap_origin}, which states that a system cannot be completely consistent (C), available (A), and tolerant of network partitions (P) all at the same time.
When communication between replicas breaks down and they cannot all acknowledge an operation, the system is faced with "the \gls{partitiondecision}: block the operation and thus decrease availability, or proceed and thus risk inconsistency."~\cite{cap_years_later}

Much research is aimed at improving consistency.
Vector clocks~\cite{lamport_ordering} and consensus algorithms such as Paxos~\cite{paxos_made_simple,paxos_made_moderately_complex} make sure the same updates are applied in the same order on all replicas even, if a minority of nodes cannot respond.
There are also data types are cleverly designed to be commutative, so that the resulting data will be the same regardless of the order in which updates are applied~\cite{crdt_orig}.
But in general, when systems cannot communicate, the CAP theorem cannot be avoided~\cite{cap_proof}, and the system is still faced with the \gls{partitiondecision}.

\written{Every replica of a repository contains the full history in an append-only data structure, any replica may add new commits, and conflicting updates are reconciled later in a merge operation.}

Though maybe not designed with the CAP theorem explicitly in mind, a \gls{DVCS} is in fact a small-scale distributed system that takes the availability-first approach to the extreme.
Rather than a set of connected nodes that may occasionally lose contact in a network partition, a \gls{DVCS}'s \glspl{repository} are self-contained and offline by default.
They allow writes to local data at any time, and only connect to other \glspl{repository} intermittently by user command to exchange updates.
Concurrent updates are not only allowed but embraced as different \glspl{branch} of development.
A \gls{DVCS} can track many different \glspl{branch} at the same time, and conflicting \glspl{branch} can be combined and resolved by the user in a \gls{merge} operation.

The \glsdisp{DVCS}{distributed version control} concept may have something to
teach larger-scale systems about availability.

\written{These systems are popular, but their use is generally limited to the small text files of source code.}

\Glspl{DVCS} are designed primarily to store program source code: plain text files in the range of tens of kilobytes.
Checking in larger binary files such as images, sound, or video affects performance.
Actions that require copying data in and out of the system slow from hundredths of a second to full seconds or minutes.
And since a \gls{DVCS} keeps every version of every file in every \gls{repository}, forever, the disk space needs compound.

This has lead to a conventional wisdom that binary files should never be stored in version control, inspiring blog posts with titles such as
"Don't ever commit binary files to Git! Or what to do if you do"~\cite{dont_ever_commit_binaries_to_version_control},
even as the modern software development practice of continuous delivery was commanding teams to "keep absolutely everything in version control."~\cite[p.33]{continuousdeliverybook}

\towrite{This paper explores the challenges of using version control to store larger binary files, with the goal of building a scalable, highly-available, distributed storage system for media files such as images, audio, and video.}

\subsection{The Power of the DAG}

\todo[inline]{Reduce and compress DAG section}

Git's \gls{DAG} data structure has several interesting properties for distributed data storage.

\begin{description}

    \item[De-duplication]
        Identical objects are de-duplicated because they will have the same ID and naturally collapse into a single object in the data store.
        This results in a natural compression of redundant objects.

    \item[A record of causality]
        Copies of the DAG can be distributed and updated independently.
        Concurrent updates will result in multiple branches of history, but references from child commit to parent commit establish a happens-before relationship and give a chain of causality.
        Branches can be merged by manually reconciling the changes, and then creating a merge commit that refers to both parent commits.
        When transferring updates from one copy to another, only new objects need to be transferred.

    \item[Atomic updates]
        When a new commit is added, all objects are added the database first, then finally the reference to the current commit is updated.
        This reference is a 160-byte SHA-1 hash value, and which can be updated atomically.

    \item[Verifiability]
        Because every object is identified by its cryptographic hash, the data integrity of each object can be verified at any time by re-computing and checking its hash.
        And because objects refer to other objects by hash, the graph is a form of blockchain.
        All objects can be verified by checking hashes and following references from the most recent commits down to the initial commits and the blob leaves of the graph.

\end{description}

The efficiency of de-duplication depends on how well identical pieces of data map to identical objects.
In Git, the redundant objects are the files and directories that do not change between commits.
De-duplication of redundant data within files is accomplished by aggregating objects together into pack files and compressing them with zlib \cite[Section 10.4]{git_book}.

Calculating deltas during this \glsdisp{packfile}{packing} phase requires loading the objects into memory, and so it can cause an out-of-memory error if an object is too large to fit into available RAM.
Because Git stores files whole in \glspl{blob}, it cannot \glsdisp{packfile}{pack} objects that are larger than available RAM.

If the \gls{DAG} operated at a granularity smaller than the file, it could become even more powerful.
It could naturally de-duplicate chunks of files the way that Git already de-duplicates whole files, and it could ensure that all objects fit into RAM for \glsdisp{packfile}{packing} or other operations.

This sub-file granularity and de-duplication is the core idea behind our new data storage system, \acrlong{DMV}.

%
