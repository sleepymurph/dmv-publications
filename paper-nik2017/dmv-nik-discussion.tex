\section{Discussion}

\subsection{Reading Whole Files into RAM Limits File Size to RAM}

Both Git and Mercurial will at some point load an entire file into memory in order to compare it to another version.
This limits the maximum file size that the system can work with to what can fit into RAM.
In Mercurial's case, the error message that appears when attempting to \gls{commit} a \SI{2}{\gib} file warns that \SI{6}{\gib} will be required to manage it.
And because it has to calculate deltas in order to store a file at all, Mercurial simply cannot work with any file that it can't fit into memory three times over.
This is why Mercurial could not store files larger than \SI{1.5}{\gib} in the file-size experiments.

Because Git's delta calculation happens behind-the-scenes in a secondary phase, it can still manage to \gls{commit} files larger than available RAM, but it prints errors as the other operations fail.
The two-phase approach also requires extra disk space and processing power.
If a large file is changed, then both revisions will be written in full, taking twice the disk space.
Then a separate operation will have to reread both \glspl{blob} in full to calculate deltas and pack the objects.

These limits could be circumvented by increasing RAM, or by implementing the diff algorithms in a streaming fashion that does not load the whole file at once.

Both \gls{DMV} and Bup avoid these pitfalls by operating with a finer granularity than the file, using a \gls{rollinghash} to divide files into chunks by their content.
It is the chunks and their indexes that must fit into memory, not the entire file.
And then since chunks are only a few kilobytes and chunk indexes are hierarchical, file size becomes theoretically unlimited.
Dividing into chunks by \gls{rollinghash} also makes delta compression unnecessary, because identical chunks in different files or file revisions will naturally de-duplicate.
At this point, it is the method of object storage that becomes the bottleneck.


\subsection{Naming Files by Hash Leads to Inefficient Writes}

DMV's super-linear commit time increase when storing a large file is due to the way it breaks the large file into chunks and stores objects as individual files on the filesystem.
So its performance characteristic for storing a large file is closer to that of storing many files.
And both Git and DMV had super-linear commit time increases as the number of files increases, due to their randomized directory and file names.
Mercurial's many-files commit times were faster, with many files written sequentially, without jumping between directories.
Bup's appends to pack files where faster still, though the speed gains over Mercurial were marginal.


\subsection{Storing Many Small Files Leads to Inefficient Use of Disk Space}

Git, Mercurial, DMV, and the copy all create one file in their \glspl{objectstore} for each input file.
So they all used up the filesystem's available inodes when storing millions of files.
Bup, with its aggregated storage, did not.
Storing many small files also make less efficient use of disk space when the file sizes are smaller than the filesystem's block size.
With immutable stored objects and an append-only history, the usage pattern of version control does not require room for objects to grow.
Therefore it makes sense to aggregate objects together into larger files.

%
