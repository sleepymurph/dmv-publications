\documentclass[a4paper]{article}
% vim: set ts=2 sts=2 sw=2 :

\usepackage{graphicx}
\usepackage[utf8]{inputenc}

% Use a blank space between paragraphs instead of an indent.
\usepackage[parfill]{parskip}

% Source code listings
\usepackage{listings}
\lstset{basicstyle=\ttfamily\footnotesize,breaklines=false}

\usepackage{url}
% To actually link URLs in the PDF.
%
% The hyperref docs recommend declaring it after the other \usepackage
% declarations, because it has to redefine several commands to work properly,
% and other later redefinitions might interfere.
% Remove the hidelinks parameter to get (ugly) visual highlights around the
% links.
\usepackage[hidelinks]{hyperref}

% Create a short macro for newly-defined key terms.
\newcommand{\newterm}{\textit}


\begin{document}

\title{Version Control for Large Files\\
    Specification}
\author{Michael Murphy}
\date{August 2016}
\maketitle

\section{Idea}\label{idea}

\subsection{Main idea: Distributed version control for large data
sets}\label{main-idea-distributed-version-control-for-large-data-sets}

Distributed systems tend to focus on maintaining a consistent view of a large
data set, across many replicas, as it is updated from many sources.
This is difficult.

By contrast, distributed version control wholly accepts inconsistency.
Each replica has its own copy of the dataset, each can diverge by their own
updates, and none is more valid than any other.
There is no global concept of a most-recent version.

Version control makes the history of mutations explicit, and provides tools for
one replica to copy changes from other replicas and incorporate them into its
own history.
In terms of the CAP theorem, distributed version control systems such as Git or
Mercurial go all-in on availability and partition tolerance.
Each replica is apart from the others by default, and each replica is always
available to read and make updates on its own.
Inconsistency is made explicit, and reconciliation is done manually by the user
with help from difference comparison tools and merge algorithms.

Version control is a powerful tool for maintaining important data sets, usually
source code.
It allows easy backups, synchronization between computers, and collaboration
between users.
The main limitation of current version control systems is that they are designed
for source code, which as data sets go is relative small, tens or maybe
thousands of text files that are merely kilobytes in size.
Adding larger binary files such as media causes existing version control systems
to become sluggish and wasteful of disk space.

Our goal is to apply the distributed version control concept to larger data
sets.
These data sets might:

\begin{itemize}

  \item Contain individual files in a wide range of sizes, from single kilobyte
    text files to several gigabyte videos

  \item Contain files in large quantities, perhaps millions of files

  \item Be too large as a whole to fit on a single conventional hard drive, up
    to multiple terabytes or petabytes

\end{itemize}

\subsubsection{Accommodating large data sets in version control}

We believe this might be achieved by starting with Git's cryptographic DAG data
structure and:

\begin{itemize}

  \item Adding facilities to break large files into smaller chunks for more
    efficient storage and comparison

  \item Relaxing the requirement that every replica store the entire history of
    the entire data set, allowing replicas to focus on particular subsets of the
    data set or particular slices of its history

\end{itemize}

Allowing each replica to only store portions of the data set will compromise
availability as well as consistency.
However, replicas will always be able to record updates to the data they do
have.
And by keeping track of what data is available at neighboring nodes, the replica
can fetch and cache requested data as needed.

In much the same way that distributed version control makes consistency and
inconsistency explicit, relaxing the full-history requirement makes availability
explicit as well.

Replicas will be able to choose their own balance of how much data to make
available locally, based on available storage space and latency to neighboring
replicas.

\subsection{Expected Benefits}

We believe such a system could be flexible enough to be used at various scales.

\begin{itemize}

  \item Individual users might use it to maintain a collection of important
    documents, photos, and media, making it easier to keep up-to-date backups
    and to synchronize between computers and mobile devices.

  \item Professional users that work with files too large for traditional
    version control, such as graphic designers, audio engineers, or video
    editors, might finally be able to adopt a version-control workflow.

  \item Corporate or government users might it to maintain large archives of
    data with full history.

  \item Far-flung networks with high-latency or rare connectivity, such as
    remote wildlife sensors or Mars rovers, could use it to manage and
    synchronize data.

\end{itemize}


\subsection{As an abstraction}

In a sense, what we want to build is an abstraction for tracking a data set, its
differing versions, and its history as a cohesive whole, even though it may be
physically spread across many nodes.

Just as version control is a tool for managing snapshots of a codebase, this
will be a tool for managing those snapshots when they become too large to store
on a single disk and must be offloaded to removable drives or the cloud.

We are thinking about data across a number of dimensions:

\begin{description}

  \item[Coverage of data set] How much of the data set is available locally or
    in neighboring nodes?

  \item[Coverage of data history] How much of the data set's history is
    available locally or in neighboring nodes?

  \item[Divergence of versions] How many different branches has this data been
    forked into, and how different are they?

  \item[Number of replicas] How many times is the data replicated across
    neighboring nodes? Is any data in danger of being permanently lost?

  \item[Availability of or distance to replicas] Of the replicas available, how
    available are they? What is the bandwidth of the connection to the
    neighboring nodes? What is the latency?

\end{description}

Rather than strive for automated consensus or availability, we want to make the
trade-offs explicit.
The goal is to track and visualize the data in these dimensions for the user, so
that they can make informed decisions about how to access the data they need.

Ideally, this system will be a generalized and flexible piece on infrastructure
that others can use to build more automated systems for specific situations.


\subsection{Main principles}

\begin{itemize}

  \item Data must never be lost accidentally.

  \item However, history may be deliberately truncated to save space, and
    sensitive data may be deliberately redacted.

  \item Data integrity must be verifiable: The system must be able to detect
    errors and, if possible, repair them.

  \item Changes to the dataset should be tracked, versions should be explicitly
    labeled, and history should be kept.

  \item Like with distributed version control, updates can be made independently
    and merged later. Different sites can have different versions. Updates
    (commits) and synchronization are deliberate, explicit, and manual.

\end{itemize}


\subsection{Important assumptions}

\begin{itemize}

  \item Contact between repositories is intermittent. Repositories may be on
    removable drives or mobile devices. Updates may require physical connection
    and reconnection. It is important to track the state of other repositories,
    so that the user can know what needs to be synchronized.

  \item Assume all actors are honest for now. No malicious components.

  \item However, components can and will fail. The system must discover and
    recover from errors (checksums, replication).

\end{itemize}


\subsection{What the system should not do}

We want to focus on the problem of storing file history and synchronizing files
between replicas.
We should be careful not to expand across the wrong abstraction boundaries or to
try to do too much.
In particular:

\begin{itemize}

  \item We do not want to reinvent the filesystem. The system should place and
    update files on the filesystem (or offer a filesystem view, such as with
    FUSE) for applications to use normally. Applications such as editors should
    not have to be rewritten to use our system.

  \item We do not want to create new exotic file formats. We believe that the
    classic tree of files is our best chance for long-term storage.

  \item We hope this system could eventually be used as a piece of
    infrastructure on which to build useful applications. It should not
    incorporate functionality that would better be left to an application.

  \item We do not want to deal with media metadata and categorization. Metadata
    and categorization is best left to the applications that produce and consume
    those media formats. We will merely provide the storage.

  \item However, knowledge of media formats might be used for behind-the-scenes
    optimization such as more efficient compression. E.g. recognizing that only
    tag data has changed in an audio file.

\end{itemize}


\section{Architecture}\label{architecture}


\subsection{Key properties of distributed version control}

\begin{figure}[h!]
  \caption{Distributed Version Control}
  \label{fig:object-db-and-working-directory}
  \centering
    \includegraphics[width=0.5\textwidth]{object-db-and-working-directory}
\end{figure}


\subsubsection{Free-form network architecture}

Because replicas are autonomous and there is no global most-recent state, there
is no need for complicated network membership schemes. Replica network
topologies reflect the connections between their users and their workflows.

Many small projects use a hub-and-spoke topology, designating one replica as the
main replica, and others pull from it and push to it.

Large projects such as the Linux kernel can form hierarchies of maintainers,
each in charge of specific subsystems.


\subsubsection{Working directory and plain local file access}

Key advantage: applications access and edit files normally through the
filesystem. Applications do not need to be rewritten to use the data.

Disadvantage: double the disk space.

Possible solution: virtual working directory (e.g. via FUSE) as a
copy-on-write snapshot of objects in the database.


\subsection{Possible workflows}\label{possible-workflows}

\subsubsection{Personal Workflow}

  \begin{figure}[h]
    \caption{Personal Workflow}
    \label{fig:workflow-personal}
    \centering
      \includegraphics[width=.75\textwidth]{workflow-personal}
  \end{figure}

  \begin{itemize}
  \item
    Repository on removable drive stores current version of whole data
    set, and historical versions as far back as space will allow.
  \item
    Second removable drive repository configured the same way for
    redundancy.
  \item
    Laptop has several partial repositories + working directories:

    \begin{itemize}
    \item
      Full history of \texttt{/Documents}
    \item
      A year or so of \texttt{/Pictures}
    \item
      Recent history of \texttt{/Music}
    \item
      No history and selected \texttt{/Videos}
    \end{itemize}
  \item
    Phone has several thin repositories + working directories:

    \begin{itemize}
    \item
      Selected \texttt{/Music} checked out with no history. Just used to
      sync.
    \item
      Selected \texttt{/Pictures} are checked out with no history, to be
      displayed on the phone.
    \item
      A \texttt{/Pictures/new} directory is checked out as the phone's
      new directory in which to put photos as they're taken. It stores
      only history that has not been synced. A new state is pushed to
      the laptop. The user categorizes the photos on the laptop, commits
      the new state, and pushes it to the phone. The selected photos
      show up in the categorized areas, and the \texttt{new} directory
      is emptied.
    \end{itemize}
  \end{itemize}

\subsubsection{Corporate/Scientific Workflow}

  \begin{figure}[h]
    \caption{Corporate Workflow}
    \label{fig:workflow-corporate}
    \centering
      \includegraphics[width=.95\textwidth]{workflow-corporate}
  \end{figure}

  \begin{itemize}
  \item
    Main repository uses a DHT as a massive backing store for its object
    database, keeps all history.
  \item
    Users check out pieces of the data set as needed, work with it, and
    push their changes back.
  \end{itemize}

\subsubsection{Remote Sensors Workflow}

  \begin{figure}[h]
    \caption{Remote Sensor Workflow}
    \label{fig:workflow-sensors}
    \centering
      \includegraphics[width=.95\textwidth]{workflow-sensors}
  \end{figure}

  \begin{itemize}
  \item
    Archive on computers in office stores all data
  \item
    A directory in the hierarchy is designated for new data from each sensor
  \item
    The sensor has a thin repository + working directory for just its own new
    data directory. It commits new data.
  \item
    A courier has a thin repository on their phone, holding just the new data
    directories for all sensors. The courier visits a sensor and connects to it,
    pulling in the new data. They visit another sensor and pulls in its data
    too.
  \item
    The courier gets back to the office and syncs with the main archive.
  \item
    The archive now has a state with new data from all visited sensors. A
    process moves the new data to a permanent directory, and commits that new
    state.
  \item
    The courier syncs the phone again, this clears the space on their phone.
  \item
    When they visit the sensor again, it syncs and merges, deleting the data
    that was stored safely, and creates a new state with just new data.
  \end{itemize}


\section{Design}\label{design}


\subsection{Main inspiration: Git and its DAG}\label{main-inspiration-git}

\subsubsection{Start with data structure}

If the data structure is right, the rest should follow.

Can we take the elegance of Git's block chain and generalize and
distribute it so that it works well with large files, and so that all
the data does not have to be present in every repository?

\begin{itemize}
\item
  Data structure will be based on Git's DAG: immutable blobs, trees, and commits
  that are stored in a content-addressable object database, and referred to by
  cryptographic hash.
\item
  The immutability, cryptographic hashing, and DAG data structure make it easy
  to synchronize between repositories and check data integrity.
\item
  Like Git, current state of local branches, and known state of remote
  repositories will be pointers to commits in the DAG.
\end{itemize}

\subsubsection{Differences from git}

\begin{itemize}
\item
  Partial repositories: Individual repositories need not store entire
  history. Several repositories can work together to spread the data
  across many machines.
\item
  Partial checkouts: Working directories need not check out entire data
  set.
\item
  Support for large files: Large files can be split into chunks and
  spread across repositories.
\item
  Repositories work more closely together to form a whole:

  \begin{itemize}
  \item
    Repositories need to know not just the state of their neighbors but what
    data each one actually stores.
  \item
    Should be able to visualize how complete data set storage is and how
    well data is replicated to protect against data loss.
  \end{itemize}
\end{itemize}


\subsection{Modified Git DAG}

\begin{figure}[h!]
    \caption{Modified Git DAG}
    \label{fig:new-dag}
    \centering
        \includegraphics[width=0.5\textwidth]{new-dag}
\end{figure}

We start with the Git DAG and modify it (Figure \ref{fig:new-dag}) to
accommodate our desired features.

\begin{itemize}
    \item
        Unlike Git, the repository is not required to store all objects in the
        DAG. A repository needs only to include the bare minimum of objects to
        record the state of its references. These include a reference to the
        current commit, that commit object, and all of that commit's trees.
        Large blob indexes should also be included. These required objects are
        shaded grey in Figure \ref{fig:new-dag}. Those connections that can be
        left dangling by not storing the referenced object are shown with dashes
        in Figure \ref{fig:new-dag}.
    \item
        Large blobs in the object database can be broken into \newterm{chunks}
        to make them easier to store, sync, and transfer. We introduce a new
        \newterm{large blob index} object type to point to the chunks that make
        up the larger blob (shown light grey in Figure \ref{fig:new-dag}.
        Chunks themselves are just blobs.
\end{itemize}


\subsection{Possible variants of partial repositories}

Commits and trees by themselves carry information about the structure and
history of the data, the metadata of the repository. They are a kind of
``backbone'' that carries the actual data.

\begin{itemize}
\item
  A full archive could store all history, just as in Git.
\item
  Several partial archives could work together to store the full history.

  Can configure which repository holds which data according to storage size
  and network topography. Old infrequently-accessed versions could be kept on
  larger, slower data stores.
\item
  A shallow repository could store only a few recent versions, to be
  compared against the working directory or to be restored to correct
  mistakes. However, it would still have the full ``backbone,'' so it
  would know what blobs would be needed to checkout different states.
\item
  A ``backbone-only'' repository could store no blobs, just the
  working directory and the ``backbone.'' This would allow the working
  directory to detect changes, and it could create new commits, only
  storing new blobs until they were pushed.
\item
  A repository could also focus on a particular subtree, storing blobs
  for its entire history, but none of the blobs outside. This would
  allow detailed work on one part of the larger data set.
\item
  Which blobs to keep could be configurable by rules that work along
  dimensions of time and parts of the tree.

\begin{verbatim}
/       last 1 versions
/foo    last 5 versions
/bar    all versions
/baz    no versions
\end{verbatim}
\item
  We could also provide tools to recommend which blobs to store based
  on usage frequency, available storage space, and repository
  availability.
\item
  Perhaps it is not even necessary to store the full backbone. The
  backbone will be tiny compared to the whole data set, but for large
  (millions of files) or long-lived data sets, the full backbone could
  be a burden on small, focused repositories.
\end{itemize}


\subsection{Other deviations from the Git data model}

\begin{itemize}
\item
  Trees and commits may hold more information than in Git.

  \begin{itemize}
  \item
    Objects could include a measure of the cumulative size of all the objects
    they refer to, so that repositories could make decisions about space
    trade-offs, and choosing to drop unneeded blobs to save space.
  \end{itemize}

\item
  Remote pointers will hold more information than with Git.

  \begin{itemize}
  \item
    Because we cannot assume that every repository has all objects,
    remote pointers must also keep metadata on which blobs are available
    at which repository. So that it knows where to look if needed.
  \item
    This availability data will be used to gather health metrics about
    what parts of history and hierarchy are safely replicated over many
    stores, and which are in danger of being lost.
  \end{itemize}
\item
  All algorithms will have to be written around the idea that data might
  not be available immediately, or at all.

  \begin{itemize}
  \item
    Repositories and working directories will work with what is
    available locally, and what is available locally will be chosen by
    the user based on what they need to work on now.
  \item
    When those needs change, it will be easy to push and pull blobs to
    and from other repositories.
  \item
    However, if a blob is lost, it is lost. This should never happen by
    accident, but it may happen deliberately by dropping old history to
    save space, or to deliberately expunge sensitive blobs from the
    records. If the algorithms can deal with missing objects locally,
    then they should naturally also be able to handle objects that are
    missing permanently.
  \end{itemize}
\end{itemize}


\section{Implementation}\label{implementation}

\begin{itemize}
  \item
    Object database storage should be pluggable. Flat files by default, but
    should be able to use a DHT as a large highly-available object store.
  \item
    Should be able to sync with a phone, either with an on-phone app, or via USB
    mount of filesystem.
\end{itemize}


\section{Related Works}

\subsection{Distributed storage and synchronization systems}

\subsubsection{Camlistore}

Camlistore \cite{camlistore_homepage} is an open-source project to create a
private long-term data storage system for personal users. It allows storage of
diverse types of data and it synchronizes between multiple replicas of the data
store. However, it eschews normal filesystems and creates its own schemas to
store various media.


\subsubsection{Eyo}

Eyo \cite{Strauss:2011:EDP:2002181.2002216} is system for storing personal media
and synchronizing it between devices. It utilizes a Git-like content-addressed
object database behind the scenes, but it works more like a networked filesystem
than version control. It focuses on organizing media by metadata, which requires
agreement on metadata formats, and it requires applications to be rewritten to
access files via Eyo rather than the filesystem, both of which are thorny and
ambitious problems. We prefer to focus purely on storage and synchronization.


\subsubsection{IPFS: The Interplanetary Filesystem}

IPFS \cite{ipfs_github_main} is an open-source project to create a global
content-addressed filesystem. By its global nature, all files are stored
together, publicly, in a global network of nodes with global addressing. IPFS
should be an excellent resource for storing published information, but we wanted
to work on a smaller, more private scale with discrete data sets. We want
individuals and organizations to be able manage their own data stores privately
on their own hardware.

It should be noted that IPFS does have support for storing private objects by
way of object-level encryption. However, this seems wasteful of disk space,
since small changes in the plain text of a file would completely change the
ciphertext, leaving no way to compress the redundancy.


\subsubsection{Kademlia}

Kademlia \cite{Maymounkov2002} is an advanced distributed hash table system that
updates its network topology information as part of normal lookups. It is an
advanced piece of infrastructure, but like other distributed hash tables, it
focuses on system-wide consistency, rather than the version-control paradigm we
are trying to achieve.


% \nocite{*}  % Print all references even if they're not used
\bibliographystyle{plain}
\bibliography{research}

\end{document}
