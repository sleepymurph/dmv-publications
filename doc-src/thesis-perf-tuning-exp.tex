\chapter{Performance Tuning Experiments}

\section{Object Store Directory Layout}
\label{dir-experiment}

During initial runs of the "many files" experiment
(\autoref{num-files-exp-desc}), we would often notice the disk being reported as
full even though the total bytes used was less than the capacity of the disk
partition. This had to do with how each system stores its objects as files on
the filesystem and how it organizes them into directories. Each file and
directory on a Unix filesystem requires one \newterm{inode}, of which the
filesystem has only a finite number. A storage scheme that allocates too many
files or directories will exhaust the filesystem's available inodes before it
uses all the available disk space.

We also noticed that average write speed would slow down as the operation
progressed. The progress meter we added to \gls{DMV}'s \gls{commit} operation
would show a rate of \SIrange{30}{40}{\mib\s} at the beginning of an operation
but slow to less than \SI{300}{\kib} by the end of a long one. We added log
output to print the write times for individual objects, and we discovered that
while most objects would be written in milliseconds, occasionally a single
object write would take multiple seconds or tens of seconds, even though there
was no appreciable difference in size between the objects
(\autoref{write-times-log-output}).

\begin{figure}
    \caption{DMV log output showing varying object write times}
    \label{write-times-log-output}
    \centering

    Not shown: many objects written in under \SI{10}{\ms}, which are logged at
    \lstinline{TRACE} level.

    \lstinputlisting[nolol,basicstyle=\scriptsize]{lst-storing-long-write-times.txt}
\end{figure}

\gls{DMV} stores its objects as individual files in an \gls{objectstore}
directory, in the same manner as Git. The object's SHA-1 hash is used as its
file name, except that the first two hex digits are removed and used as a
subdirectory (also described in \autoref{dir-impl}). Our prototype originally
took the first four hex digits to create two levels of subdirectories, under the
assumption that we would store more objects than Git and need to spread them out
with more subdirectories. That original prototype was showing this odd behavior,
and it stored files much more slowly than Git. We suspected that the number of
subdirectories could be at fault, so we experimented with different subdirectory
schemes to see their effects.

%


\paragraph{Procedure}

To measure the effects of different object storage schemes, we created a new
\SI{100}{\mebi\byte} partition on one of the dedicated experiment computers, and then generated a series
of pseudo-random files of \SI{4}{\kibi\byte} each until the disk was reported
full. For each file, we would give it a pseudo-random name that resembled an
SHA-1 hash, and store it according to the object storage scheme under test. We
increased a counter each time we created a file, and another each time we
created a new directory. We also checked the number of files already in the
target directory before writing, timed the write, and used the Unix
\lstinline{df} utility to measure free disk space in bytes and the number of
free inodes.

The directory schemes we tested were all variations of the basic scheme of
taking leading hex digits of the SHA-1 hash to form directories. We varied the
number of directories taken (depth) and the number of hex digits per directory
(see \autoref{sample-directory-scheme-variations} for examples). We tried depths
from \num{0} to \num{6} and digits per directory from \num{0} to \num{16},
discarding combinations that did not make sense, such as combinations involving
\num{0} and another number (which would all simply be undivided), or those that
required more than the \num{40} hex digits of a \num{160}-byte SHA-1 hash.

\begin{table}[]
    \caption{Sample object store directory variations}
    \label{sample-directory-scheme-variations}
    \centering
    \begin{tabular}{l l l}
        Hex digits & Depth & Example \\
        \midrule
        0 & 0 & \lstinline{03d37679d1fab86e5286decd6cd2a94efcfe083f} \\
        1 & 1 & \lstinline{7/9332ca7ce9163f78e3c115a2173bd8fd853d286} \\
        1 & 3 & \lstinline{6/8/c/40e64f3e74e6ebefdcf2f5f30fb8da004792c} \\
        2 & 1 & \lstinline{9f/4ec22c3e0289b29eefefe4728dece14e67e6ac} \\
        2 & 2 & \lstinline{dd/52/bcccff156a179cdac0793ef049039372d8a1} \\
        3 & 1 & \lstinline{cc5/199084d70f7c5ba325a240e1927579ee24bb1} \\
        3 & 4 & \lstinline{472/e98/e88/0b1/c5905065c70cbe806361d32f6429} \\
        4 & 3 & \lstinline{1ed2/bd51/01fe/5b23763e8c76852739f59201280f} \\
    \end{tabular}
\end{table}

\paragraph{Environment}

Like the "many files" experiment, this was automated as a Python script and run
on one of the dedicated computers used for that experiment (specs shown in
\autoref{test-machine-specs}). However, rather than spending hours to fill the
\SI{197}{\gibi\byte} partition used for the other experiments, this experiment
used a new \SI{100}{\mebi\byte} LVM partition.

\paragraph{Results}
\label{seek-times-results}

\begin{figure}[]
    \caption{Number of Files vs. number of directories filling a disk}
    \label{fig:plot-filesystem-limits--directory-takeover}
    \centering

    The number of files and directories present when the disk reported that it
    was full under the given directory scheme, shown by number of hex digits per
    directory (the different plots) and levels of depth (x axis)

    \includegraphics[]{plot-filesystem-limits--directory-takeover}
\end{figure}

\perottoinline{Error flags? "comment valid for other figs too"}
\askottoinline{Error flags for figures? I'm not following}

\autoref{fig:plot-filesystem-limits--directory-takeover} shows how quickly
directories overtake files as subdirectory nesting goes deeper. Presented
visually, the connection between files and directories becomes obvious. The
maximum number of files plus directories is constant: the number of inodes on
the filesystem. However, the number of directories created increases
exponentially with both the number of hex digits per directory and then again by
directory depth. This can be expressed mathematically.

Let $h$ denote the number of hex digits per subdirectory and let $n$ denote the
subdirectory depth. Then the total number of directories created by the scheme,
$d$ is given by

\begin{equation}
    d = \sum_{i=1}^n \left( 16^h \right)^i \quad.
\end{equation}

The directories are not created all at once, only when a file that should be
placed in that directory is stored. But because files are named according to a
uniformly distributed hash function, no particular directory will be favored and
the number of directories will trend towards $d$.

Let $o$ denote the number of inodes available on the filesystem, and let $f$
denote the number of files that can be stored on the filesystem when the
directory scheme creates $d$ directories. Then,

\begin{equation}
    f = o-d \quad,
\end{equation}

And therefore,

\begin{equation}
    f = o - \sum_{i=1}^n \left( 16^h \right)^i \quad.
\end{equation}

So we can see that \gls{DMV}'s original scheme, with two hex digits per
directory and a depth of two, would yield \SI{65792} subdirectories, which by
itself is more than \num{2.5} times the total number of inodes available on the
\SI{100}{\mib} test partition. So of course it ran out of inodes long before
running out of disk space.

From there, we turn our attention to the mysterious, intermittent long write
times. In the experiment, across all directory schemes, there were \num{315601}
total writes. Of those, \num{312813} (\SI{99.1}{\percent}) completed in less
than \SI{1}{\ms}. The others are plotted in \autoref{plot-seek-times}, and the
top ten longest writes are listed in \autoref{longest-writes}. The spikes in the
graph are arranged in curves radiating out from zero files and zero directories,
each curve representing the run of one of the different directory schemes. No
single directory scheme stands out as worse than the others, though having more
directories does seem to lead to more frequent and longer long writes. The
scheme with the fewest and shortest long writes is the one that has no
subdirectories at all. So we conclude that there is no penalty for storing many
thousands of files in one directory.

\perottoinline{Note interesting peak in write times graph, with x and y}

\begin{figure}[]
    \caption{Unusually high write times}
    \label{plot-seek-times}
    \centering

    \SI{4}{\kib} files that took \SI{1}{\ms} or longer to write, plotted
    according to the number of files and directories on the disk already, and
    colored by the time it took to write the file. \\
    Not shown: The \SI{99.1}{\percent} of writes that were faster than
    \SI{1}{\ms}.

    \includegraphics[]{plot-seek-times}
    \perrolfinline{label z-axis and color axis. Kind of weird to have both
    height and color represent same value, but ok.}
\end{figure}

\begin{table}
    \caption{Top-ten longest writes}
    \label{longest-writes}
    % To generate:
    % awk -e '$7=="ok" && $8 >= 0.5 {print $0}' \
    %   exp--filesystem-limits--micro/*murphytest04.txt \
    %   | sort -r -k 8 \
    %   | awk -e '{ printf "%05.3f & %d & %d & %5d & %5d & %2d & %04.1f \\\\ \n", \
    %                       $8, $2, $3, $4, $5, $6, $13*100/$12 }' \
    %   | head -n10
    \begin{tabular}{c c r r c r r}
        Time (\si{\s}) & Digits & Depth & Files & Dirs & Files in Dir & \% inodes used \\
        \midrule
2.306 & 1 & 4 & 10699 & 13997 &  1 & 96.2 \\
2.180 & 1 & 4 & 11025 & 14289 &  1 & 98.6 \\
1.775 & 3 & 1 & 16321 &  4008 &  5 & 79.2 \\
1.654 & 1 & 5 &  5834 & 14755 &  1 & 80.2 \\
1.646 & 3 & 2 & 10831 & 14635 &  1 & 99.2 \\
1.466 & 1 & 5 &  5389 & 13790 &  1 & 74.7 \\
1.456 & 2 & 3 &  8393 & 16550 &  1 & 97.1 \\
1.443 & 4 & 2 &  7823 & 15225 &  1 & 89.8 \\
1.434 & 1 & 5 &  5922 & 14949 &  1 & 81.3 \\
1.379 & 1 & 6 &  5302 & 18885 &  1 & 94.2 \\
    \end{tabular}
\end{table}

The single longest write took \SI{2.306}{\s}, but it is only one of two writes
that took more than \SI{2}{\s}. Both of the two-second writes occurred under the
scheme with one hex digit per directory at a depth of four. Only five took
longer than \SI{1.5}{\s}. Sixteen are longer than \SI{1}{\s}, \num{155} are
longer than \SI{0.5}{\s}, and \num{340} are longer than \SI{0.1}{\s}.

There is no clear pattern to the occurrence of long writes in terms of number of
files or directories, though they tend to get longer as both increase. The long
writes are spaced apart somewhat regularly. That suggests that they are caused
by upkeep that the filesystem has to do periodically, and that there is no
obvious way to avoid them, at least not while storing many small files.
Aggregating objects into pack files as discussed in
\autoref{chunk-then-recombine} might be a better strategy.

%



\section{Effect of Different Linux I/O Schedulers}

Since the anticipatory I/O scheduler was removed in version 2.6.33
\cite{as_removed_linux_release_notes}, the Linux kernel has included three
different I/O schedulers to choose from\cite{ioschedulers}:

\begin{description}

    \item[Completely Fair Queueing] The \lstinline{cfq} scheduler is the default
        I/O scheduler as of Linux 2.6.18 \cite{cfq_default_linux_release_notes}.
        It creates a separate queue for each process and handles requests in a
        round, preventing any one process from dominating I/O.

    \item[Deadline] The \lstinline{deadline} scheduler tries to set hard limits
        on wait time for scheduled I/O operations.

    \item[No-op] The \lstinline{noop} scheduler does as little as possible,
        passing requests directly to the device for it to manage.

\end{description}

We were curious if the choice of scheduler would have any effect on performance.
In particular, we aimed to document if it might alleviate the high seek times we
were seeing. So we ran extra trials of the VCS scaling experiments using the
\gls{DMV} prototype and different I/O schedulers.

\begin{figure}[]
    \caption{Time for DMV prototype to commit an increasing number of 1KiB files
    to a fresh repository, by I/O scheduler}
    \label{fig:plot-iosched-num-files--c1-time}
    \centering

    The top left graph shows the whole data set on a logarithmic scale. The
    other two show an increasingly zoomed-in linear scale, to show how small the
    difference is.

    \includegraphics[]{plot-iosched-num-files--c1-time}
\end{figure}

The results of running the "many files" experiment with different schedulers are
shown in \autoref{fig:plot-iosched-num-files--c1-time}. The I/O scheduler used
made little difference. At \num{100000} files, the average initial \gls{commit}
times were \SI{19.666}{\s} for CFQ, \SI{19.708}{\s} for deadline, and
\SI{19.598}{\s} for noop. The difference between each pair is less than any of
the standard deviations at that number of files: \SI{0.674}{\s},
\SI{0.3153}{\s}, and \SI{0.447}{\s}, respectively.

In retrospect, these results are not surprising. The I/O scheduler deals mainly
with juggling I/O access between different processes on the system, but the
\gls{DMV} prototype is a single process\perotto{It could have been more than
one}. Perhaps a multi-threaded or multi-process version of the prototype could
give the scheduler something to work with, but a better approach would be to
combine objects into pack files that can be written sequentially\perotto{ok, but
you could do both}, as discussed in \autoref{chunk-then-recombine}.

%



\section{Tuning Chunk Size}\label{rolling-hash-exp}

The algorithm used to divide files into chunks (described in
\autoref{chunking-algoritm}) involves moving a window across the data, and
setting a chunk boundary where the sum of the bytes in that window is evenly
divisible by a given number. We ran an experiment to determine the effects of
these two parameters on chunk size.

\paragraph{Procedure}

For each combination of window size and divisor, we would run the rolling hash
algorithm on a stream of pseudo-random bytes until it had identified \num{100}
chunks. Then we would compute the mean and standard deviation of the chunk
sizes.

We used window sizes in powers of two from \SI{128}{\byte} ($2^7$) to
\SI{256}{\kibi\byte} ($2^{18}$), and divisors in powers of two from \num{256}
($2^8$) to \SI{128}{\kibi\relax} ($2^{17}$).

The pseudo-random number generator used was an xorshift RNG\cite{xorshift_rng}.
The experiment itself was automated as a unit test in the \gls{DMV} prototype's
Rust code.

\paragraph{Environment}

Because this experiment measures only the output of calculations, the
environment in which it is run should make no difference in the outcome. In
fact, if the xorshift RNG is given the same initial seed value, the resulting
random byte stream will be identical, which will lead to an identical sequence
of chunks, which will lead to an identical average chunk size. This experiment
is deterministic.


\paragraph{Results}

\begin{figure}[]
  \caption{Mean chunk size}
  \label{fig:plot-rolling-hash}
  \centering
    \includegraphics[]{plot-rolling-hash}
\end{figure}

\perottoinline{What can be concluded from these results?}

\autoref{fig:plot-rolling-hash} shows the mean chunk sizes generated by the
experiment. Both mean and deviation increase with divisor and window size,
though divisor has a greater effect. We did not invest time in a mathematical
analysis, but for the \gls{DMV} prototype we set a window of \SI{32}{\kib} and a
divisor of \SI{16}{\kib}, which yields a mean chunk size of \SI{18.7}{\kib} with
a standard deviation of \SI{22.0}{\kib}.

\towrite{Earlier, was clearing window after chunk, shifted whole thing up by
window size}

\towrite{After running all tests, will affect chunk size-- smaller, would mean
longer for file size experiment. Need to pack.}

\towrite{Rolling hash improvements:
    - more thought to window size. Larger than avg chunk size and you get
    subsequent chunks affecting each other.
    - thought to what happens at chunk boundaries. Run of zeros at chunk
    boundary will create a run of 1-byte chunks. How likely is that?
    - What about long runs of zeros anyway?
}

%
