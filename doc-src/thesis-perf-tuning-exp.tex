\chapter{Performance Tuning Experiments}
\label{perf-tuning-exp-chapter}

After noticing \gls{DMV}'s long commit times, we tried tuning certain \gls{DMV}
parameters in an attempt to speed it up. We re-ran the file-size and many-files
experiments on \gls{DMV} several times, varying first the object store directory
layout, then Linux I/O scheduler, and finally chunk size. We also ran new, more
targeted experiments to investigate the effects of directory layout and chunk
size.

%



\begin{figure}[p]
    \caption{DMV output showing varying object write times}
    \label{write-times-log-output}
    \centering

    Not shown: many objects written in under \SI{10}{\ms}, which are logged at
    \lstinline{TRACE} level.

    \programoutput{lst-storing-long-write-times.txt}
\end{figure}

\begin{table}[p]
    \caption{Sample object store directory variations}
    \label{sample-directory-scheme-variations}
    \centering
    \begin{tabular}{c c l}
        Hex digits & Depth & Example \\
        \midrule
        0 & 0 & \lstinline{03d37679d1fab86e5286decd6cd2a94efcfe083f} \\
        1 & 1 & \lstinline{7/9332ca7ce9163f78e3c115a2173bd8fd853d286} \\
        1 & 3 & \lstinline{6/8/c/40e64f3e74e6ebefdcf2f5f30fb8da004792c} \\
        2 & 1 & \lstinline{9f/4ec22c3e0289b29eefefe4728dece14e67e6ac} \\
        2 & 2 & \lstinline{dd/52/bcccff156a179cdac0793ef049039372d8a1} \\
        3 & 1 & \lstinline{cc5/199084d70f7c5ba325a240e1927579ee24bb1} \\
        3 & 4 & \lstinline{472/e98/e88/0b1/c5905065c70cbe806361d32f6429} \\
        4 & 3 & \lstinline{1ed2/bd51/01fe/5b23763e8c76852739f59201280f} \\
    \end{tabular}
\end{table}

\section{Object Store Directory Layout}
\label{dir-experiment}

During initial runs of the "many files" experiment
(\autoref{num-files-exp-desc}), we would often notice the disk being reported as
full even though the total bytes used was less than the capacity of the disk
partition. This had to do with how each system stores its objects as files on
the filesystem and how it organizes them into directories. Each file and
directory on a Unix filesystem requires one \gls{inode}, of which the filesystem
has only a finite number. A storage scheme that allocates too many files or
directories will exhaust the filesystem's available \glspl{inode} before it uses
all the available disk space.

We also noticed that average write speed would slow down as the operation
progressed. The progress meter we added to \gls{DMV}'s \gls{commit} operation
would show a rate of \SIrange{30}{40}{\mib\s} at the beginning of an operation
but slow to less than \SI{300}{\kib} by the end of a long one. We added log
output to print the write times for individual objects, and we discovered that
while most objects would be written in milliseconds, occasionally a single
object write would take multiple seconds or tens of seconds, even though there
was no appreciable difference in size between the objects
(\autoref{write-times-log-output}).

\gls{DMV} stores its objects as individual files in an \gls{objectstore}
directory, in the same manner as Git. The object's SHA-1 hash is used as its
file name, except that the first two hex digits are removed and used as a
subdirectory (also described in \autoref{dir-impl}). Our prototype originally
took the first four hex digits to create two levels of subdirectories, under the
assumption that we would store more objects than Git and need to spread them out
with more subdirectories. That original prototype was showing this odd behavior,
and it stored files much more slowly than Git. We suspected that the number of
subdirectories could be at fault, so we experimented with different subdirectory
schemes to see their effects.

%


\subsection{Procedure}

To measure the effects of different object storage schemes, we performed a new
experiment wherein we created a new \SI{100}{\mebi\byte} partition on one of the
dedicated experiment computers, and then generated a series of pseudo-random
files of \SI{4}{\kibi\byte} each until the disk was reported full.

For each file, we would give it a pseudo-random name that resembled an SHA-1
hash, and store it according to the object storage scheme under test. We
increased a counter each time we created a file, and another each time we
created a new directory. If storing the file required creating several
subdirectories to get to the proper depth, we would count each subdirectory. We
also checked the number of files already in the target directory before writing,
timed the write, and used the Unix \lstinline{df} utility to measure free disk
space in bytes and the number of free \glspl{inode}.

The directory schemes we evaluated were all variations of the basic scheme of
taking leading hex digits of the SHA-1 hash to form directories. We varied the
number of directories taken (depth) and the number of hex digits per directory
(see \autoref{sample-directory-scheme-variations} for examples). We tried depths
from \num{0} to \num{6} and digits per directory from \num{0} to \num{16},
discarding combinations that did not make sense, such as combinations involving
\num{0} and another number (which would all simply be undivided), or those that
required more than the \num{40} hex digits of a \num{160}-byte SHA-1 hash.

\subsection{Environment}

Like the "many files" experiment, this was automated as a Python script and run
on one of the dedicated computers used for that experiment (specs shown in
\autoref{test-machine-specs}). However, rather than spending hours to fill the
\SI{197}{\gibi\byte} partition used for the other experiments, this experiment
used a new \SI{100}{\mebi\byte} LVM partition.


\begin{figure}[b!]
    %\begin{leftfullpage}
        \caption{Number of Files vs. number of directories filling a disk}
        \label{fig:plot-filesystem-limits--directory-takeover}
        \centering

        The number of files and directories present when the disk reported that it
        was full under the given directory scheme, shown by number of hex digits per
        directory (the different plots) and levels of depth (x axis)

        \includegraphics[]{plot-filesystem-limits--directory-takeover}
    %\end{leftfullpage}
\end{figure}

\cleardoublepage
\subsection{Results}
\label{object-dir-layout-results}

\subsubsection{Out of Inodes}

\autoref{fig:plot-filesystem-limits--directory-takeover} shows how quickly
directories overtake files as subdirectory nesting goes deeper. Presented
visually, the connection between files and directories becomes obvious. The
maximum number of files plus directories is constant and limited by the number
of \glspl{inode} on the filesystem, which on the \SI{100}{\mib} test partition
is \num{25688}. However, the number of directories created increases
exponentially with both the number of hex digits per directory and then again by
directory depth. This can be expressed mathematically.

Let $h$ denote the number of hex digits per subdirectory and let $n$ denote the
subdirectory depth. Then the total number of directories created by the scheme,
$d$ is given by

\begin{equation}
    d = \sum_{i=1}^n \left( 16^h \right)^i \quad.
\end{equation}

The directories are not created all at once, only when a file that should be
placed in that directory is stored. But because files are named according to a
uniformly distributed hash function, no particular directory will be favored and
the number of directories will trend towards $d$.

Let $o$ denote the number of \glspl{inode} available on the filesystem, and let
$f$ denote the number of files that can be stored on the filesystem when the
directory scheme creates $d$ directories. Then,

\begin{equation}
    f = o-d \quad,
\end{equation}

And therefore,

\begin{equation}
    f = o - \sum_{i=1}^n \left( 16^h \right)^i \quad.
\end{equation}

So we can see that \gls{DMV}'s original scheme, with two hex digits per
directory and a depth of two, would yield \num{65792} subdirectories, which by
itself is more than \num{2.5} times the total number of \glspl{inode} available
on the \SI{100}{\mib} test partition. So of course it ran out of \glspl{inode}
long before running out of disk space (in terms of blocks).


\begin{figure}[p]
    \caption{Unusually high write times}
    \label{plot-seek-times}
    \centering

    \SI{4}{\kib} files that took \SI{1}{\ms} or longer to write, plotted
    according to the number of files and directories on the disk already, and
    colored by subdirectory depth. \\
    Not shown: The \SI{99.1}{\percent} of writes that were faster than
    \SI{1}{\ms}.

    \includegraphics[]{plot-seek-times}
\end{figure}

\begin{table}[p]
    \caption{Top-ten longest writes}
    \label{longest-writes}
    % To generate:
    % awk -e '$7=="ok" && $8 >= 0.5 {print $0}' \
    %   exp--filesystem-limits--micro/*murphytest04.txt \
    %   | sort -r -k 8 \
    %   | awk -e '{ printf "%05.3f & %d & %d & %5d & %5d & %2d & %04.1f \\\\ \n", \
    %                       $8, $2, $3, $4, $5, $6, $13*100/$12 }' \
    %   | head -n10
    \begin{tabular}{r r r r r r r}
        Time (\si{\s}) & Digits & Depth & Files & Dirs & Files in dir & \% inodes used \\
        \midrule
2.306 & 1 & 4 & 10699 & 13997 &  1 & 96.2 \\
2.180 & 1 & 4 & 11025 & 14289 &  1 & 98.6 \\
1.775 & 3 & 1 & 16321 &  4008 &  5 & 79.2 \\
1.654 & 1 & 5 &  5834 & 14755 &  1 & 80.2 \\
1.646 & 3 & 2 & 10831 & 14635 &  1 & 99.2 \\
1.466 & 1 & 5 &  5389 & 13790 &  1 & 74.7 \\
1.456 & 2 & 3 &  8393 & 16550 &  1 & 97.1 \\
1.443 & 4 & 2 &  7823 & 15225 &  1 & 89.8 \\
1.434 & 1 & 5 &  5922 & 14949 &  1 & 81.3 \\
1.379 & 1 & 6 &  5302 & 18885 &  1 & 94.2 \\
    \end{tabular}
\end{table}

%


\subsubsection{Long Write Times}

From there, we turn our attention to the mysterious, intermittent long write
times. In the experiment, across all directory schemes, there were \num{315601}
total writes. Of those, \num{312813} (\SI{99.1}{\percent}) completed \SI{1}{\ms}
or less. The others are plotted in \autoref{plot-seek-times}, and data about the
top ten longest writes is listed in \autoref{longest-writes}. The spikes in the
graph appear as curves radiating out from zero files and zero directories. Each
curve represents a cluster of directory schemes that filled up the disk in the
same pattern, corresponding more to subdirectory depth than to number of hex
digits per subdirectory.

No single directory scheme stands out as worse than the others, though longer
writes seem correlated with having more directories and having more
\glspl{inode} already used (shown by distance from origin). The scheme with the
fewest and shortest long writes is the one that has no subdirectories at all
(shown by the short green spikes along the files axis). So we conclude that
there is no penalty for storing many thousands of files in one directory.

The two longest writes are clustered together near the center of the plot, near
\num{11000} files and \num{14000} directories. Both occur in the directory
scheme with \num{1} hex digit and a depth of \num{4}. The longest was
\SI{2.306}{\second} at \num{10699} files and \num{13997} directories
(\SI{96.2}{\percent} of inodes used), and the second was \SI{2.180}{\second} at
\num{11025} files and \num{14289} directories (\SI{98.6}{\percent} of inodes
used).

% Analyze:
%
% awk -e '$7=="ok" && $8 > 0.01 {print $0}' \
%       ../data/exp--filesystem-limits--micro/*murphytest04.txt \
%   | wc -l

Write times seem to follow a power law distribution. The two peaks are the only
two writes out of \num{315601} total that took longer than \SI{2}{\second}. Five
took longer than \SI{1.5}{\s} (including the two over \SI{2}{\s}). Sixteen are
longer than \SI{1}{\s}, \num{155} are longer than \SI{0.5}{\s}, \num{340} are
longer than \SI{0.1}{\s}, and \num{1381} are longer than \SI{0.01}{\s}. The vast
majority (\num{312813}, \SI{99.1}{\percent}) completed \SI{1}{\ms} or less.

The long writes appear to be spaced apart somewhat regularly. This suggests that
they are caused by upkeep that the filesystem has to do periodically, and that
there is no obvious way to avoid them, at least not while storing many small
files. Aggregating objects into \glspl{packfile} would be a better strategy, as
we saw when Bup was consistently the fastest \gls{VCS} in the file-size and
number-of-files experiments.

%

\begin{figure}[b!]
    \caption{Time to commit one large file, with different object directory
    schemes}
    \label{plot-dir-schemes-file-size--c1-time}
    \centering

    \explainlogsubfig

    \includegraphics{plot-dir-schemes-file-size--c1-time}
\end{figure}

\subsubsection{Directory Schemes in Action}

We tuned the DMV prototype and re-ran the full file-size experiments
(\autoref{file-size-exp-desc}) with two different directory schemes. First, with
an early DMV version (fb2f43d) that used \num{2} hex digits per directory and a
depth of \num{2}, and then also with the reference DMV prototype (c9baf3a, as
noted in \autoref{vcs-versions}) that used \num{2} hex digits per directory and
a depth of \num{1}. \autoref{plot-dir-schemes-file-size--c1-time} shows the
initial commit times for both prototype versions, plus Bup for comparison.

As with the other runs of this experiment, the commit time for files under about
\SI{6}{\mib} is dominated by overhead. The depth-\num{2} version of DMV has less
overhead than the depth-\num{1} version, and especially so at sizes up to
\SI{8}{\kib}, where the depth-\num{2} version completes its commit in under
\SI{5}{\ms}, rivaling the sub-millisecond write times for the \SI{4}{\kib} files
in the targeted experiment.

At up to \SI{6}{\mib}, the depth-\num{2} version has a consistent commit time of
\SIrange{103}{104}{\ms} while the depth-\num{1} version has a consistent commit
time of \SIrange{201}{202}{\ms}. This difference might be caused by additional
work that we did on DMV between running the experiments on the depth-\num{2}
version and switching to depth-\num{1}, including refactoring and adding some
statistics collecting code to the verify (\lstinline{fsck}) procedure. None of
this should have impacted commit times directly, but it may have caused changes
to the DMV executable's size or layout that made it take longer to load from
disk and start up.

From \SIrange{8}{384}{\mib} there is no noticeable difference between the two
versions of DMV, but at \SI{512}{\mib} and above, the commit results in enough
chunk files that the directory layout starts to make a difference. The slight
trend we noticed in \autoref{plot-seek-times} for more directories to result in
more long write times seems to have a more pronounced effect, and the
depth-\num{2} version starts to lag behind depth-\num{1}. At \SI{768}{\mib}, the
depth-\num{1} version of DMV finally starts to lag behind Bup.

At \SI{768}{\mib} and above, the commit times for both versions of DMV increase
linearly with file size. They appear to have the same slope, with the
depth-\num{1} version shifted down. Bup, though also increasing linearly, does
so with a flatter slope. This is further evidence to suggest that aggregating
objects into \glspl{packfile} is not only less wasteful of disk space but also
faster as the number of objects grows into the millions.

%
\tweak{\clearpage}


\section{Linux I/O Scheduler}

Since the anticipatory I/O scheduler was removed in version
2.6.33~\cite{as_removed_linux_release_notes}, the Linux kernel has included
three different I/O schedulers to choose from~\cite{ioschedulers}:

\begin{description}

    \item[Completely Fair Queueing] The \lstinline{cfq} scheduler is the default
        I/O scheduler as of Linux 2.6.18~\cite{cfq_default_linux_release_notes}.
        It creates a separate queue for each process and handles requests in a
        round, preventing any one process from dominating I/O.

    \item[Deadline] The \lstinline{deadline} scheduler tries to set hard limits
        on wait time for scheduled I/O operations.

    \item[No-op] The \lstinline{noop} scheduler does as little as possible,
        passing requests directly to the device for it to manage. So this is the
        null benchmark for this experiment.

\end{description}

We were curious if the choice of scheduler would have any effect on performance.
In particular, we aimed to document if it might alleviate the high seek times we
were seeing. So we ran extra trials of the VCS scaling experiments using the
\gls{DMV} prototype and different I/O schedulers.

\begin{figure}[]
    \caption{Time for DMV prototype to commit an increasing number of 1KiB files
    to a fresh repository, by I/O scheduler}
    \label{fig:plot-iosched-num-files--c1-time}
    \centering

    \explainlogsubfig

    \includegraphics[]{plot-iosched-num-files--c1-time}
\end{figure}

The results of running the "many files" experiment with different schedulers are
shown in \autoref{fig:plot-iosched-num-files--c1-time}. The I/O scheduler used
made little difference. At \num{100000} files, the average initial \gls{commit}
times were \SI{19.666}{\s} for CFQ, \SI{19.708}{\s} for deadline, and
\SI{19.598}{\s} for noop. The difference between each pair is less than any of
the standard deviations at that number of files: \SI{0.674}{\s},
\SI{0.3153}{\s}, and \SI{0.447}{\s}, respectively.

In retrospect, these results are not surprising. The I/O scheduler deals mainly
with juggling I/O access between different processes on the system, but the
current \gls{DMV} prototype is a single process. A multi-threaded or
multi-process version of the prototype could give the scheduler something to
work with.

%
