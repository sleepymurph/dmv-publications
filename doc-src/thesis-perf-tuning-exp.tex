\chapter{Performance Tuning Experiments}

\section{Object Store Directory Layout}
\label{dir-experiment}

During initial runs of the "many files" experiment
(\autoref{num-files-exp-desc}), we would often notice the disk being reported as
full even though the total bytes used was less than the capacity of the disk
partition. This had to do with how each system stores its objects as files on
the filesystem and who it organizes them into directories. Each file and
directory on a Unix filesystem requires one \newterm{inode}, of which the
filesystem has only a finite number. A storage scheme that allocates too many
files or directories will exhaust the filesystem's available inodes before it
uses all the available disk space.

We also noticed that total write speed would slow down as the operation
progressed. The progress meter we added to DMV's commit operation would show a
rate of \SIrange{30}{40}{\mib\s} at the beginning of an operation but slow to
less than \SI{300}{\kib} by the end of a long one. We added log output to print
the write times for individual objects, and we discovered that while most
objects would be written in milliseconds, occasionally a single object write
would take multiple seconds or tens of seconds, even though there was no
appreciable difference in size between the objects
(\autoref{write-times-log-output}).

\begin{figure}
    \caption{DMV log output showing varying object write times}
    \label{write-times-log-output}
    \centering

    Not shown: many objects written in under \SI{10}{\ms}, which are logged at
    \lstinline{TRACE} level.

    \lstinputlisting[nolol,basicstyle=\scriptsize]{lst-storing-long-write-times.txt}
\end{figure}

DMV stores its objects as individual files in an object-store directory, in the
same manner as Git. The object's SHA-1 hash is used as its file name, except
that the first two hex digits are removed and used as a subdirectory (also
described in \autoref{dir-impl}). Our prototype originally took the first four
hex digits to create two levels of subdirectories, under the assumption that we
would store more objects than Git and need to spread them out with more
subdirectories. That original prototype was showing this odd behavior, and it
stored files much more slowly than Git. We suspected that the number of
subdirectories could be at fault, so we experimented with different subdirectory
schemes to see their effects.

%


\paragraph{Procedure}

To test the effects of different object storage schemes, we created a new
\SI{100}{\mebi\byte} partition on a test computer, and then generated a series
of pseudo-random files of \SI{4}{\kibi\byte} each until the disk was reported
full. For each file, we would give it a pseudo-random name that resembled an
SHA-1 hash, and store it according to the object storage scheme under test. We
increased a counter each time we created a file, and another each time we
created a new directory. We also checked the number of files already in the
target directory before writing, timed the write, and used the Unix
\lstinline{df} utility to measure free disk space in bytes and the number of
free inodes.

The directory schemes we tested were all variations of the basic scheme of
taking leading hex digits of the SHA-1 hash to form directories. We varied the
number of directories taken (depth) and the number of hex digits per directory
(see \autoref{sample-directory-scheme-variations} for examples). We tried depths
from \num{0} to \num{6} and digits per directory from \num{0} to \num{16},
discarding combinations that did not make sense, such as combinations involving
\num{0} and another number (which would all simply be undivided), or those that
required more than the \num{40} hex digits of a \num{160}-byte SHA-1 hash.

\begin{table}[]
    \caption{Sample object store directory variations}
    \label{sample-directory-scheme-variations}
    \centering
    \begin{tabular}{l l l}
        Hex digits & Depth & Example \\
        \midrule
        0 & 0 & \lstinline{03d37679d1fab86e5286decd6cd2a94efcfe083f} \\
        1 & 1 & \lstinline{7/9332ca7ce9163f78e3c115a2173bd8fd853d286} \\
        1 & 3 & \lstinline{6/8/c/40e64f3e74e6ebefdcf2f5f30fb8da004792c} \\
        2 & 1 & \lstinline{9f/4ec22c3e0289b29eefefe4728dece14e67e6ac} \\
        2 & 2 & \lstinline{dd/52/bcccff156a179cdac0793ef049039372d8a1} \\
        3 & 1 & \lstinline{cc5/199084d70f7c5ba325a240e1927579ee24bb1} \\
        3 & 4 & \lstinline{472/e98/e88/0b1/c5905065c70cbe806361d32f6429} \\
        4 & 3 & \lstinline{1ed2/bd51/01fe/5b23763e8c76852739f59201280f} \\
    \end{tabular}
\end{table}

\paragraph{Environment}

Like the "many files" experiment, this was automated as a Python script and run
on one of the dedicated test computers used for that experiment (specs shown in
\autoref{test-machine-specs}). However, rather than spending hours to fill the
\SI{197}{\gibi\byte} partition used for the other experiments, this experiment
used a new \SI{100}{\mebi\byte} LVM partition.

\paragraph{Results}
\label{seek-times-results}

\begin{figure}[]
    \caption{Files vs. directories filling a disk}
    \label{fig:plot-filesystem-limits--directory-takeover}
    \centering

    The number of files and directories present when the disk reported that it
    was full under the given directory scheme, shown by number of hex digits per
    directory (the different plots) and levels of depth (x axis)

    \includegraphics[]{plot-filesystem-limits--directory-takeover}
\end{figure}

\autoref{fig:plot-filesystem-limits--directory-takeover} shows how quickly
directories overtake files as subdirectory nesting goes deeper. Presented
visually, the connection between files and directories becomes obvious. The
maximum number of files plus directories is constant: the number of inodes on
the filesystem. However, the number of directories created increases
exponentially with both the number of hex digits per directory and then again by
directory depth. This can be expressed mathematically. Let $f$, $d$, and $i$ be
the number of files, directories, and inodes on the filesystem, and let $h$ and
$n$ be the number of hex digits per subdirectory and the subdirectory depth.
Then the following equations govern the number of files that can fit on the
filesystem:

\begin{equation*}
    \begin{aligned}
        f+d = i, \quad
        d = \left( 16^h \right)^n, \quad
        f = i - \left( 16^h \right)^n
    \end{aligned}
\end{equation*}



\begin{figure}[]
    \caption{Unusually high write times}
    \label{plot-seek-times}
    \centering

    \SI{4}{\kib} files that took \SI{1}{\ms} or longer to write, plotted
    according to the number of files and directories on the disk already, and
    colored by the time it took to write the file. \\
    Not shown: \num{312813} writes that were faster than \SI{1}{\ms}.

    \includegraphics[]{plot-seek-times}
\end{figure}

\autoref{plot-seek-times} shows unusual write times by number of files and
directories already present on the filesystem.


\subsection{Observations}

\begin{itemize}

  \item In hindsight, it should have been easy to see that the number of
    directories would grow exponentially.

\end{itemize}

%



\section{Effect of Different Linux I/O Schedulers}

Since the anticipatory I/O scheduler was removed in version 2.6.33
\cite{as_removed_linux_release_notes}, the Linux kernel has included three
different I/O schedulers to choose from\cite{ioschedulers}:

\begin{description}

    \item[Completely Fair Queueing] The \lstinline{cfq} scheduler is the default
        I/O scheduler as of Linux 2.6.18 \cite{cfq_default_linux_release_notes}.
        It creates a separate queue for each process and handles requests in a
        round, preventing any one process from dominating I/O.

    \item[Deadline] The \lstinline{deadline} scheduler tries to set hard limits
        on wait time for scheduled I/O operations.

    \item[No-op] The \lstinline{noop} scheduler does as little as possible,
        passing requests directly to the device for it to manage.

\end{description}

We were curious if the choice of scheduler would have any effect on performance.
In particular, we hoped it might alleviate the high seek times we were seeing.
So we ran extra trials of our VCS scaling experiments using the DMV prototype
and different I/O schedulers.

\begin{figure}[]
    \caption{Time for DMV prototype to commit many 1KiB files to a fresh
    repository, by I/O scheduler}
    \label{fig:plot-iosched-num-files--c1-time}
    \centering

    The top left graph shows the whole data set on a logarithmic scale. The
    other two show an increasingly zoomed-in linear scale, to show how small the
    difference is.

    \includegraphics[]{plot-iosched-num-files--c1-time}
\end{figure}

The results of running the "many files" experiment with different schedulers are
shown in \autoref{fig:plot-iosched-num-files--c1-time}. In our case, the I/O
scheduler used made little difference. At \num{100000} files, the average
initial commit times were \SI{19.666}{\s} for CFQ, \SI{19.708}{\s} for deadline,
and \SI{19.598}{\s} for noop. The difference between each pair is less than any
of the standard deviations at that number of files: \SI{0.674}{\s},
\SI{0.3153}{\s}, and \SI{0.447}{\s}, respectively.

In retrospect, these results are not surprising. The I/O scheduler deals mainly
with juggling I/O access between different processes on the system, but the DMV
prototype is a single process. Perhaps a multi-threaded or multi-process version
of the prototype could give the scheduler something to work with, but a better
approach would be to combine objects into pack files that can be written
sequentially, as discussed in \autoref{chunk-then-recombine}.

%



\section{Tuning Chunk Size}\label{rolling-hash-exp}

The algorithm used to divide files into chunks (described in
\autoref{chunking-algoritm}) involves moving a window across the data, and
setting a chunk boundary where the sum of the bytes in that window is evenly
divisible by a given number. We ran an experiment to determine the effects of
these two parameters on chunk size.

\paragraph{Procedure}

For each combination of window size and divisor, we would run the rolling hash
algorithm on a stream of pseudo-random bytes until it had identified \num{100}
chunks. Then we would compute the mean and standard deviation of the chunk
sizes.

We used window sizes in powers of two from \SI{128}{\byte} ($2^7$) to
\SI{256}{\kibi\byte} ($2^{18}$), and divisors in powers of two from \num{256}
($2^8$) to \SI{128}{\kibi\relax} ($2^{17}$).

The pseudo-random number generator used was an xorshift RNG\cite{xorshift_rng}.
The experiment itself was automated as a unit test in the DMV prototype's Rust
code.

\paragraph{Environment}

Because this experiment measures only the output of calculations, the
environment in which it is run should make no difference in the outcome. In
fact, if the xorshift RNG is given the same initial seed value, the resulting
random byte stream will be identical, which will lead to an identical sequence
of chunks, which will lead to an identical average chunk size. This experiment
is deterministic.


\subsection{Results}

\begin{figure}[]
  \caption{Mean chunk size}
  \label{fig:plot-rolling-hash}
  \centering
    \includegraphics[]{plot-rolling-hash}
\end{figure}

\autoref{fig:plot-rolling-hash} shows the mean chunk size.

\subsection{Observations}

\begin{itemize}

  \item The mean chunk size is approximately the sum of the window size and
    match parameter.

  \item When the match parameter is less than the window size, the standard
    deviation is approximately the match parameter.

\end{itemize}


%
