\chapter{Discussion}


\section{Limits}

\towrite{Mercurial's design requires 3x RAM as file size}

Mercurial's commit algorithm requires that the entire file fit into RAM three
time over. On our trial computers with 8GiB of ram, the largest file Mercurial
could commit successfully was 2GiB. At larger sizes, the commit command would
abort, roll back the commit in progress, and exit with an error status.

This reflects the fact that Mercurial stores only changes to files. It must
examine every file with a diff algorithm during the commit.


\towrite{Rewrite and de-itemize the following:}

\begin{itemize}

    \item Git initially stores a full copy of every revision and then has a
        separate garbage collection phase to compact stored data in the
        repository. So for a single commit, the operation uses 2x the disk space
        as the size of the file: one for the file itself, and one for the copy
        in the repository. And for a single commit and update, the operation
        requires 3x the disk space: one for the file itself, one for the initial
        copy in the repository, and one for the updated copy.

        Disk space usage reduces after garbage collection (in this case to
        2.001x), because git compacts and de-duplicates the data in its
        repository. However, it cannot be reliably used unless there is enough
        space to store the uncompressed version first. Therefore Git starts to
        fail the test with files about 1/3 of the size of the available disk
        space.

    \item When the file is too large to fit into RAM, Git's commit operation
        prints an error message and exits with an error code, but the operation
        still completes successfully.

    \item Starting at just under 1GiB, the Git garbage collection seems to fail
        silently. No errors are reported, but on the graph one can see that the
        garbage-collected disk space usage jumps from 2x to 3x, indicating that
        the garbage collection phase is not doing any compacting. This might
        have to do with RAM requirements, similar to Mercurial's commit
        operation.

    \item Mercurial stores only deltas of files. In a sense, it is doing Git's
        garbage collection phase during each commit. This means more efficient
        disk usage. But it places a strong limitation on file size: Mercurial
        commits fail unless it has 3x as much RAM available as the size of the
        file. We suspect this has to do with the way Mercurial calculates. On
        the 8GiB test machines, Mercurial could only store files up to 2GiB in
        size.

\end{itemize}



\section{Prototype development}


\section{Prototype performance tuning}

\begin{figure}[]
  \caption{Increasing file size: prototype improvements}
  \label{fig:plot-file-size--c1-time--prototype-improvements}
  \centering
    \includegraphics[]{plot-file-size--c1-time--detail-high-end--prototype-improvements}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to commit many 1KiB files to an empty repository
  (detail)}
  \label{fig:plot-num-files--prototype-improvements--c1-time-detail}
  \centering
    \includegraphics[]{plot-num-files--prototype-improvements--c1-time-detail}
\end{figure}



\section{Possible Applications}

\begin{itemize}

  \item Individual users might use it to maintain a collection of important
    documents, photos, and media, making it easier to keep up-to-date backups
    and to synchronize between computers, mobile devices, and removable drives.

  \item Professional users that work with files too large for traditional
    version control, such as graphic designers, audio engineers, or maybe even
    video editors, might finally be able to adopt a version-control workflow.

  \item Corporate or government users might use it to maintain large archives of
    data with full history.

  \item Far-flung networks with high-latency or rare connectivity, such as
    remote wildlife sensors or Mars rovers, could use it to manage and
    synchronize data.

\end{itemize}



\section{As an Abstraction of Data Space and Time}

We are thinking about data across a number of dimensions:

\begin{description}

  \item[Coverage of data set] How much of the data set is available locally or
    in neighboring nodes?

  \item[Coverage of data history] How much of the data set's history is
    available locally or in neighboring nodes?

  \item[Divergence of versions] How many different branches has this data been
    forked into, and how different are they?

  \item[Number of replicas] How many times is the data replicated across
    neighboring nodes? Is any data in danger of being permanently lost?

  \item[Availability of or distance to replicas] Of the replicas available, how
    available are they? What is the bandwidth of the connection to the
    neighboring nodes? What is the latency?

\end{description}



\section{What the system should not do}

\todo{Update tense}
We want to focus on the problem of storing file history and synchronizing files
between replicas.
We should be careful not to expand across the wrong abstraction boundaries or to
try to do too much.
In particular:

\begin{itemize}

  \item We do not want to reinvent the filesystem. The system should place and
    update files on the filesystem (or offer a filesystem view, such as with
    FUSE) for applications to use normally. Applications such as editors should
    not have to be rewritten to use our system.

  \item We do not want to create new exotic file formats. We believe that the
    classic tree of files is our best chance for long-term storage.

  \item We hope this system could eventually be used as a piece of
    infrastructure on which to build useful applications. It should not
    incorporate functionality that would better be left to an application.

  \item We do not want to deal with media metadata and categorization. Metadata
    and categorization is best left to the applications that produce and consume
    those media formats. We will merely provide the storage.

  \item However, knowledge of media formats might be used for behind-the-scenes
    optimization such as more efficient compression. E.g. recognizing that only
    tag data has changed in an audio file.

\end{itemize}



\section{Limitations}

\towrite{non-programmers (and even some programmers) cannot handle Git
    - Key to usability would be to make as linear a history as possible with
        auto-updates, but that is the job of a separate app
    - Cite redesign of Git\cite{redesign_of_git}
}
