\chapter{Discussion}


\section{Source of File Size Limits in Tested Systems}
\label{chunk-then-recombine}

\towrite{All limited by granularity of objects they work with. Git and Mercurial
by file, DMV and Bup by chunk, Bup also packs so better performance.}

Mercurial and Git seem to be limited by available RAM. Mercurial is especially
curious, because it seems to require three times the file size. When trying to
commit a \SI{2}{\gibi\byte} file, it prints a warning that "up to 6442 MB of RAM
may be required to manage this file." On a computer with \SI{8}{\gibi\byte} of
RAM, that operation failed. The reason might have to do with Mercurial's method
of storing files. Each file is stored as a base revision and then a series of
deltas \cite[Chapter 4]{hgbook}, so every commit requires a diff operation. It
is likely that Mercurial loads multiple copies of the file in RAM in order to
calculate this diff, and files cannot be stored at all without sufficient
available RAM.

Git also calculates deltas to pack similar files together, but this is a
secondary operation. It first stores the file as a separate whole object, and
then packs similar objects together in a separate phase. This might explain why
Git reports errors on large files but then still completes the commit operation:
it can do the essential part of the commit in a streaming fashion, but then it
needs to allocate more RAM for comparisons on the secondary packing phase, and
it is this secondary packing phase that fails. Git's errors begin appearing at
\SI{12}{\gibi\byte}, the first size tested that cannot possibly fit in the test
machine's \SI{8}{\gibi\byte} of RAM, and its error message says that
\lstinline{malloc} failed to allocate \SI{12}{\gibi\byte}. So whatever the
purpose, Git is attempting to allocate enough RAM to hold a complete copy of the
file.

This need to operate on whole files is a limiting factor for both Git and
Mercurial. DMV and Bup get around this by using a rolling hash to break files
into chunks. Interestingly, Bup does this even while using Git's own DAG and
repository format\cite{bup_design}. Git has no notion of file chunks, but Bup
reuses Git's tree objects to index them. A large file in Bup thus becomes a
directory of chunk files in the Git repository format.

Bup also writes directly to Git's pack file format, splitting into chunks but
then writing many chunks back to one pack file. It does this in one pass, rather
than following Git's two phases of storing then packing. This proves to be the
most efficient design for dealing with both large files and numerous files. So
far, our DMV prototype naively stores chunks (indeed, all objects) as individual
files on the file system. This uses disk space inefficiently and leads to seek
time slowdown as inodes become scarce. It seems that the key to storing large
files efficiently is to break them into chunks first, then recombine those
chunks into larger packed files. Future versions of DMV should incorporate this
insight.


\towrite{Git store then pack with changed file: 3x then 2x}

%



\section{Prototype development}


\section{Prototype performance tuning}

\begin{figure}[]
  \caption{Increasing file size: prototype improvements}
  \label{fig:plot-file-size--c1-time--prototype-improvements}
  \centering
    \includegraphics[]{plot-file-size--c1-time--detail-high-end--prototype-improvements}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to commit many 1KiB files to an empty repository
  (detail)}
  \label{fig:plot-num-files--prototype-improvements--c1-time-detail}
  \centering
    \includegraphics[]{plot-num-files--prototype-improvements--c1-time-detail}
\end{figure}



\section{Possible Applications}

\begin{itemize}

  \item Individual users might use it to maintain a collection of important
    documents, photos, and media, making it easier to keep up-to-date backups
    and to synchronize between computers, mobile devices, and removable drives.

  \item Professional users that work with files too large for traditional
    version control, such as graphic designers, audio engineers, or maybe even
    video editors, might finally be able to adopt a version-control workflow.

  \item Corporate or government users might use it to maintain large archives of
    data with full history.

  \item Far-flung networks with high-latency or rare connectivity, such as
    remote wildlife sensors or Mars rovers, could use it to manage and
    synchronize data.

\end{itemize}



\section{As an Abstraction of Data Space and Time}

We are thinking about data across a number of dimensions:

\begin{description}

  \item[Coverage of data set] How much of the data set is available locally or
    in neighboring nodes?

  \item[Coverage of data history] How much of the data set's history is
    available locally or in neighboring nodes?

  \item[Divergence of versions] How many different branches has this data been
    forked into, and how different are they?

  \item[Number of replicas] How many times is the data replicated across
    neighboring nodes? Is any data in danger of being permanently lost?

  \item[Availability of or distance to replicas] Of the replicas available, how
    available are they? What is the bandwidth of the connection to the
    neighboring nodes? What is the latency?

\end{description}



\section{What the system should not do}

\todo{Update tense}
We want to focus on the problem of storing file history and synchronizing files
between replicas.
We should be careful not to expand across the wrong abstraction boundaries or to
try to do too much.
In particular:

\begin{itemize}

  \item We do not want to reinvent the filesystem. The system should place and
    update files on the filesystem (or offer a filesystem view, such as with
    FUSE) for applications to use normally. Applications such as editors should
    not have to be rewritten to use our system.

  \item We do not want to create new exotic file formats. We believe that the
    classic tree of files is our best chance for long-term storage.

  \item We hope this system could eventually be used as a piece of
    infrastructure on which to build useful applications. It should not
    incorporate functionality that would better be left to an application.

  \item We do not want to deal with media metadata and categorization. Metadata
    and categorization is best left to the applications that produce and consume
    those media formats. We will merely provide the storage.

  \item However, knowledge of media formats might be used for behind-the-scenes
    optimization such as more efficient compression. E.g. recognizing that only
    tag data has changed in an audio file.

\end{itemize}



\section{Limitations}

\towrite{non-programmers (and even some programmers) cannot handle Git
    - Key to usability would be to make as linear a history as possible with
        auto-updates, but that is the job of a separate app
    - Cite redesign of Git\cite{redesign_of_git}
}
