\chapter{Discussion}

\perottoinline{Discuss somewhere "what if" iowait could be reduced
significantly. What would happen with memory, disk, remote}

\section{Data Granularity and Storage Schemes}
\label{chunk-then-recombine}

All four of the systems we examined in detail-- Git, Mercurial, Bup, and
\gls{DMV}-- model data and its history with a similar directed acyclic graph.
The major difference is the granularity at which they work with data, and how
they store it.

Both Git and Mercurial take the file as the basic unit of data granularity,
though they approach storage differently. Git stores files whole as \glspl{blob}
during \gls{commit}, storing them and other objects as files in object
directories (as we experimented with in \autoref{dir-experiment}). Later, an
optional packing phase will compact objects together into \glspl{packfile},
where similar objects are stored as deltas against a base revision~\cite[Section
10.4]{git_book}. Mercurial stores each file's different revisions as deltas
   against a base revision in a \gls{filelog} structure~\cite[Chapter
   4]{hgbook}. This is Mercurial's primary storage format, and it is constructed
     during \gls{commit}.

By using files as the basic unit of storage, and storing files as deltas against
a base revision, both Git and Mercurial will at some point load an entire file
into memory in order to compare it to another version. This ties the maximum
file size that the system can work with to the amount of available RAM. In
Mercurial's case, the error message that appears when attempting to \gls{commit}
a \SI{2}{\gib} file warns that \SI{6}{\gib} will be required to manage it. And
because it has to calculate deltas in order to store a file at all, Mercurial
simply cannot work with any file that it can't fit into memory three times over.
This is why Mercurial could not store files larger than \SI{1.5}{\gib} in the
file-size experiments (\autoref{file-size-limits-results}).

Because Git's delta calculation happens behind-the-scenes in a secondary phase,
it can still manage to \gls{commit} files larger than available RAM, but it
prints errors as the other operations fail. The two-phase approach also requires
extra disk space and processing power. If a large file is changed, the at first
both revisions will be written in full, taking twice the disk space. Then a
separate operation will have to reread both \glspl{blob} in full to calculate
deltas and pack the objects.

Both \gls{DMV} and Bup avoid these pitfalls by operating with a finer
granularity, using a \gls{rollinghash} to divide files into chunks by their
content. It is the chunks and their indexes that must fit into memory, not the
entire file. And then since chunks are only a few kilobytes and chunk indexes
are hierarchical, file size becomes theoretically unlimited. Dividing into
chunks by \gls{rollinghash} also makes delta compression unnecessary, because
identical chunks in different files or file revisions will naturally
de-duplicate. At this point, it is the method of object storage that becomes the
bottleneck.

The current \gls{DMV} prototype stores objects loose as files on the filesystem.
This proves to be wasteful of disk space, taking up whole filesystem blocks with
tiny objects and taking up \glspl{inode} with subdirectories. Also, naming the
object files by their SHA-1 hash effectively makes the filenames random, which
causes dramatic write slowdowns compared to writing sequentially, as we saw in
the number-of-files and object-directory-layout experiments
(\cref{results-num-files--c1-time,object-dir-layout-results}). Dividing into
chunks solves the problem of storing large files by turning it into the problem
of storing many files.

Bup's storage strategy is the best of both worlds. It first divides files into
chunks, but then re-packs objects together into \glspl{packfile}. In fact, Bup
uses Git's \gls{packfile} format\footnotemark, but it writes it directly without
the separate compacting phase, and without bothering to calculate
deltas~\cite{bup_design}. This makes efficient use of disk space, and allows the
\glspl{packfile} to be written sequentially, minimizing disk seeks. This is why
Bup was clearly the fastest of the systems evaluated in both the file-size and
number-of-files experiments
(\cref{results-file-size--c1-time,results-num-files--c1-time}).

\footnotetext{Git has no notion of chunks, but Bup reuses Git's \gls{tree}
objects as chunk indices. Git can read a repository written by Bup, but it will
see a large file as a directory full of small chunk files.}

So we see that the key to handling large files is to break them into many
smaller files, and the key to storing many small files is to combine them into
larger files. The magic is in the combination, where files and revisions of
files are broken into chunks by content, so that identical chunks are naturally
de-duplicated in storage. That is what gives significant disk space savings over
simply zipping up snapshots of the data.

So the next step for \gls{DMV} will be to start aggregating objects.

%


\section{Subtleties of the Rolling Hash}

The \gls{rollinghash} is the key to providing smaller granularity, because it is
what identifies common byte strings within files. We saw in our chunk-size
experiment (\autoref{results-rolling-hash}), that smaller chunks lead to slower
writes, but the effect was due to the number of files. Aggregating objects into
\glspl{packfile} will alleviate that concern, so we can examine the subtler
effects of chunk size.

First is the overhead of indexing the chunks. In DMV, we have the
\gls{chunkedblob} objects, which keep a \num{168}-byte record for every chunk:
\num{160} bytes for the chunk's SHA-1 object ID, \num{4} for the chunk's offset
within the file, and \num{4} for the chunk's size\footnotemark. With an average
chunk size of \SI{4}{\kib}, a \SI{1}{\gib} file would be broken into
\num{262144} chunks, which would require \SI{42}{\mib} of chunk records. The
blob header for each chunk would contribute some overhead as well. DMV's object
header is currently only \num{8} bytes, but that still makes \SI{2}{\mib} of
additional overhead. Then there is metadata overhead in whatever scheme we use
to store or \glsdisp{packfile}{pack} the objects. Some overhead is inevitable,
and \SI{44}{\mib} per \si{\gib} of data is probably perfectly acceptable, but it
is something to consider.

\footnotetext{

    Chunk size is actually redundant in the chunk index record, since it can be
    computed by comparing the chunk offset to the offset of the following chunk,
    or, in the case of the last chunk in the file, the total file size. We could
    eliminate it and save \SI{1}{\mib} of overhead per \si{\gib} of data.

}

The less direct, but ultimately greater effect of chunk size is that it is the
granularity of de-duplication. Smaller chunk sizes should lead to more
de-duplication: small changes will lead to smaller changed chunks and less new
data to store. However, the utility of this de-duplication depends on how much
redundancy is in the data, including:

\begin{itemize}

    \item The degree of similarity between files in the data set

    \item How often the files are updated

    \item How much of the file is changed with each update

\end{itemize}

We hypothesize that most common media formats used by individual users (images,
music, video, etc.) would have very little redundancy, and rarely be edited.
Though some may update the internal metadata of their music files. The project
files of popular professional software (Photoshop, ProTools, Avid, etc.) might
be edited more frequently and benefit more from the de-duplication. We will need
to perform additional experiments using real-world data to measure actual space
savings.

%


\section{DMV Prototype development}

\begin{figure}
    \caption{Actual output of DMV status command during merge}
    \label{lst_dmv_merge_in_progress}
    \centering
    \programoutput{lst-dmv-merge-in-progress.txt}
\end{figure}

\begin{figure}
    \caption{Actual output of DMV log command after merge}
    \label{lst_dmv_log_output}
    \centering
    \programoutput{lst-dmv-log-output.txt}
\end{figure}

Unfortunately, the current \gls{DMV} prototype does not yet include any of the
planned networking features. However, basic local version control functionality
is working, and the system can be used to store data, keep history, and retrieve
old versions. Branching and rudimentary merging functionality are implemented as
well (\cref{lst_dmv_merge_in_progress,lst_dmv_log_output}).

Also, as a proof-of-concept for sharding the DAG, the \gls{DMV} prototype is
able to check out a subdirectory of the dataset to the working directory and
commit changes to it. This demonstrates the ability to keep a full history of
part of the dataset, as described in \autoref{working-with-incomplete-dag} and
illustrated in \autoref{dia_dmv_dag_slice_history_of_subset}.

\towrite{Mistakes along the way: storing whole status tree in memory}

\towrite{Graph walk abstraction}

%


\iffalse
\section{Limitations}

\towrite{non-programmers (and even some programmers) cannot handle Git
    - Key to usability would be to make as linear a history as possible with
        auto-updates, but that is the job of a separate app
    - Cite redesign of Git~\cite{redesign_of_git}
}
\fi

%


\section{Aggregating Data about a Sharded DAG}

Though not implemented in the \gls{DMV} prototype, we would like a \gls{DMV}
node to be able to aggregate data about what \gls{DAG} objects are available at
its neighbors and throughout the network. The data could be analyzed to give
metrics about the data set in several dimensions:

\begin{description}

  \item[Coverage of data set] How much of the data set is available locally or
    in neighboring nodes?

  \item[Coverage of data history] How much of the data set's history is
    available locally or in neighboring nodes?

  \item[Divergence of versions] How many different branches has this data been
    forked into, and how different are they?

  \item[Number of replicas] How many times is the data replicated across
    neighboring nodes? Is any data in danger of being permanently lost?

  \item[Availability of or distance to replicas] Of the replicas available, how
    available are they? What is the bandwidth of the connection to the
    neighboring nodes? What is the latency?

\end{description}

Such data could be useful for monitoring the health of the data set, alerting
the user to shards of data that risk being lost without further replication.

%


\section{Potential Applications of DMV}

As a general distributed storage platform, \gls{DMV} could have a wide range of
potential applications:

\begin{description}

    \item[Private data storage] Individual users might use \gls{DMV} to maintain
        a collection of important documents, photos, and media on their own
        devices, making it easier to keep up-to-date backups and to synchronize
        between computers, mobile devices, and removable drives without giving
        their data to a third-party cloud service.

    \item[Large-file version control] Professional users that work with files
        too large for traditional version control, such as graphic designers,
        audio engineers, or video editors, might finally be able to adopt a
        version-control workflow.

    \item[Long-term data archiving] Corporate or government users might use it
        to maintain large archives of data with full revision history.

    \item[Low-connectivity networks] Far-flung networks with high-latency or
        rare connectivity, such as remote wildlife sensors or Mars rovers, could
        use it to manage and synchronize data.

\end{description}

%


\section{What DMV should not do}

We want to focus on the problem of storing file history and synchronizing files
between replicas.
We should be careful not to expand across the wrong abstraction boundaries or to
try to do too much.
In particular:

\begin{description}

    \item[We do not want to reinvent the filesystem] DMV should place and update
        files on the filesystem (or offer a filesystem view, such as with FUSE)
        for applications to use normally. Applications such as editors should
        not have to be rewritten to use our system.

  \item[We do not want to create new exotic file formats] We believe that the
      classic file hierarchy is our best chance for long-term storage.

  \item[We want to keep it simple] We hope that DMV could eventually be used as
      a piece of infrastructure on which to build useful applications. It should
      not incorporate functionality that would better be left to an application.

  \item[We do not want to deal with media metadata and categorization] Metadata
      and categorization is best left to the applications that produce and
      consume those media formats. We will merely provide the storage.

      However, knowledge of media formats might be used for behind-the-scenes
      optimization, such as setting chunk boundaries at natural boundaries in
      the file.

\end{description}
