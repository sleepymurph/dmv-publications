\documentclass[12pt,a4paper,two-side]{book}
% vim: set ts=2 sts=2 sw=2 :

\usepackage{graphicx}
\usepackage[utf8]{inputenc}

% Use a blank space between paragraphs instead of an indent.
\usepackage[parfill]{parskip}

% Source code listings
\usepackage{listings}
\lstset{basicstyle=\ttfamily\footnotesize,breaklines=false}

\usepackage{url}
% To actually link URLs in the PDF.
%
% The hyperref docs recommend declaring it after the other \usepackage
% declarations, because it has to redefine several commands to work properly,
% and other later redefinitions might interfere.
% Remove the hidelinks parameter to get (ugly) visual highlights around the
% links.
\usepackage[hidelinks]{hyperref}

% Create a short macro for newly-defined key terms.
\newcommand{\newterm}{\textit}


\begin{document}

\title{Master's Thesis}
\author{Michael Murphy}
\date{May 2017}
\maketitle

\frontmatter

\chapter{Abstract}
  Nowadays...

\tableofcontents

\phantomsection
\addcontentsline{toc}{chapter}{Lists}

\listoffigures
\addcontentsline{toc}{section}{List of Figures}

%\listoftables
%\addcontentsline{toc}{section}{List of Tables}

\lstlistoflistings
\addcontentsline{toc}{section}{Listings}

\mainmatter

\chapter{Introduction}

\section{Idea}\label{idea}

A low-level distributed data storage platform that makes data location and
availability explicit, rather than trying to hide or abstract them away.

The target use for this system is to spread a data set across many compute
nodes, and to allow the sharding and replication of the data set to be
customizable according to the location and availability of the nodes, and what
data is needed where. The ultimate goal is to track the scattered data as a
coherent data whole, and to allow the end user or client application to
visualize and manage what data is stored on what nodes.

The system aims to be a low-level service, providing information about the data
and tools to manage it, but ultimately leaving decisions to the end user or
client application.

We assume that the data set and all nodes in the system are controlled by the
user.

The target data sets for this system include collections of media files (binary
files ranging in size from hundreds of kilobytes to several gigabytes) numbering
in the hundreds to millions. The system will not attempt to understand the
internals of different media formats and will treat all files as opaque blobs.
Such media data suggests a usage pattern where files are more often created or
moved than they are updated in-place. Also, we assume writes will be infrequent
compared to traditional online transactional databases (seconds to minutes, or
longer, between updates).

The system will be designed for high availability in the face of connections
that are intermittent, high-latency, expensive, or otherwise restricted. At each
node, the portion of data that is stored on that node will be available for
reading and writing by local applications. Metadata about the non-local portions
of the data set will be available as well, along with hints about the location
of that data. Clients can use that information to schedule transfers and
updates. Consistency will be resolved during updates. Conflicting versions will
be presented to the client application to resolve.


\chapter{Architecture and Design}

\section{Architecture}\label{architecture}

Data will be distributed across several \newterm{stores}, which will each hold a
portion of the data set. Stores can be diverse, from mobile devices to storage
clusters. Distribution of the data will be determined by the end user or client
application according to available resources and the needs of the user.

The data store network will be decentralized and unstructured (Figure
\ref{fig:dia_architecture}). Stores can be connected to other stores in an ad-hoc
manner, with a user- or application-defined topography. The number of
connections will not be strictly limited, but extreme scalability of individual
connections is not a priority. We are aiming for one, to tens, to perhaps
hundreds of connections.

\begin{figure}[h]
  \caption{Stores in an ad-hoc network}
  \label{fig:dia_architecture}
  \centering
    \includegraphics[width=0.95\textwidth]{dia_architecture}
\end{figure}

Each store will be autonomous, and local applications will be able to access and
perform processing on any data that is stored locally. Data not currently
available can be listed and requested. The listing will give hints about the
estimated availability of each piece of data, with metrics including connection
uptime, latency, and bandwidth to stores that have copies of the data. This
metadata will naturally fall out of date between syncs with connected nodes. The
age of that data can be part of the hint.

We are envisioning a filesystem with an \lstinline{ls} command that lists these
hints as part of its output (in this example presented as a single estimated
time to retrieve):

\begin{lstlisting}[caption=Example ls output]
    -rw-r--r-- 1 user user  121306 Oct 21 18:28 local   filex
    -rw-r--r-- 1 user user   25475 Oct 21 17:52 100ms   filey
    -rw-r--r-- 1 user user   32031 Oct 21 17:52 20min   filez
    -rw-r--r-- 1 user user   74968 Oct 18 17:12 missing filexx
    -rw-r--r-- 1 user user   83977 Sep 22 21:23 unknown fileyy
\end{lstlisting}

A primary goal for the system is to detect storage errors and never lose data
inadvertently. Local data stores will detect corruption, and data loss will be
prevented by replication between stores. However, data can be explicitly removed
from the data set. Data can be removed from the local store, relying on remote
stores to keep replicas, though it will be up the end user/application to ensure
that there are enough copies of the data to ensure its durability. Number of
known replicas will be one of the metrics tracked by the system.


\section{Design}\label{design}

The data set will be a traditional hierarchical file structure, and it will be
presented as a normal filesystem. We do not want to reinvent the filesystem, and
we do not want to have to modify local applications to work with the data,
though applications can be modified to be aware of and to work with the system.

The storage will be modeled on version control systems, specifically Git and
it's directed acyclic graph data structure (DAG). There will be a
content-addressable blob store to de-duplicate and store data in a DAG
structure, and a working tree where the files can be read and written by normal
applications.

Like in a version control system, previous versions of the data will be kept,
along with metadata about the history of changes. Snapshots of the data will be
explicitly \newterm{committed} to the system by the user/application. The chain
of commit history can diverge into branches to be merged later, and each
individual store is also naturally a branch. Syncs and merges with connected
stores will be explicitly initiated.

Individual stores are not required to store all blobs in the DAG. Some blobs may
not be available on a given store, and history may be deliberately pruned to
save space. Algorithms and client applications must work with the blobs they
have locally and the hints about the availability of remote blobs.


\chapter{Implementation}

\section{Implementation}\label{implementation}

% TODO: DAG diagram

Files over a certain threshold will be broken into smaller chunks in the blob
store, using a content-based splitting technique such as Rabin fingerprinting.


\section{Possible extensions}

The goal of the system is to be a low-level platform for tracking a single data
set that is distributed across many nodes. Following the version-control
paradigm, most operations will be explicit. However, it will be possible to
write applications on top of this system that automate more operations.

\begin{itemize}

  \item A gossip protocol could spread information about availability of remote
    stores that are not directly connected.

  \item Daemons could auto-commit and auto-sync with other stores.

  \item To save storage space, the working directory can be a virtual filesystem
    that is a view into the object database.

\end{itemize}



\chapter{Experiments and Results}

\section{Version Control System Behavior Experiments}

\subsection{Committing a single file of increasing size}

\iffalse

\subsubsection{Methodology}

The experiment was written as a script in Python. For each version control
system, and each file size, this script would do the following:

\begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item Initialize an empty repository
    \item Generate a test file of the given size from random binary data
    \item Commit the file
    \item Overwrite a small part of the file (1/1024th of the data)
    \item Commit the file again
    \item Run the garbage collection / compaction algorithm (Git only)
\end{enumerate}

Times were measured by the Python script, checking the time of the real-time
clock before and after the relevant command was run.

Total repository size was measured via the Unix \lstinline{du} command, and it
includes the original file in the working directory, as well as the repository.

Trials were performed four times each, on four identical computers. Because the
computers are identical, the performance across trials was nearly identical.
These graphs display the mean values and have error bars for standard deviation,
but the error bars are difficult to see.

\subsubsection{Platform and Configuration}

Each identical experiment computer had a 3GHz Intel processor, 8GiB of RAM, and
about 200GiB of free hard disk space.

\fi

\subsubsection{Results}

\begin{figure}[p]
  \caption{Real time required to commit one file to an empty repository}
  \label{fig:plot-file-size--c1-time}
  \centering
    \includegraphics[]{plot-file-size--c1-time}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to commit one file to an empty repository (detail)}
  \label{fig:plot-file-size--c1-time--detail-high-end}
  \centering
    \includegraphics[]{plot-file-size--c1-time--detail-low-end}
    \includegraphics[]{plot-file-size--c1-time--detail-high-end}
\end{figure}

\begin{figure}[p]
  \caption{Increasing file size: prototype improvements}
  \label{fig:plot-file-size--c1-time--prototype-improvements}
  \centering
    \includegraphics[]{plot-file-size--c1-time--detail-high-end--prototype-improvements}
\end{figure}

\begin{figure}[p]
  \caption{CPU utilization while committing one file to an empty repository}
  \label{fig:plot-file-size--c1-cpu}
  \centering
    \includegraphics[]{plot-file-size--c1-cpu-a}
    \includegraphics[]{plot-file-size--c1-cpu-b}
\end{figure}

\begin{figure}[p]
  \caption{Total repository size after committing, editing, and committing again}
  \label{fig:plot-file-size--repo-size}
  \centering
    \includegraphics[]{plot-file-size--repo-size}
\end{figure}

Figure \ref{fig:plot-file-size--c1-time} shows time required for
the initial commit, copying the file into a fresh empty repository.

Figure \ref{fig:plot-file-size--c1-time--detail-high-end} shows
detailed views of figure \ref{fig:plot-file-size--c1-time}, zooming
in and using a linear scale.

Figure \ref{fig:plot-file-size--c1-cpu} shows the
CPU usage during the initial commit.

Figure \ref{fig:plot-file-size--repo-size} shows the total
repository size, including the original file, after committing once, editing
1/1024th of the file, and committing again.

\iffalse

\subsubsection{Observations}

\begin{itemize}

  \item Mercurial's commit algorithm requires that the entire file fit into RAM
    three time over. On our trial computers with 8GiB of ram, the largest file
    Mercurial could commit successfully was 2GiB. At larger sizes, the commit
    command would abort, roll back the commit in progress, and exit with an
    error status.

    This reflects the fact that Mercurial stores only changes to files. It must
    examine every file with a diff algorithm during the commit.

\end{itemize}

% -------------------------------------------------
% TODO: rewrite observations below and place above

\begin{itemize}

    \item Git initially stores a full copy of every revision and then has a
        separate garbage collection phase to compact stored data in the
        repository. So for a single commit, the operation uses 2x the disk space
        as the size of the file: one for the file itself, and one for the copy
        in the repository. And for a single commit and update, the operation
        requires 3x the disk space: one for the file itself, one for the initial
        copy in the repository, and one for the updated copy.

        Disk space usage reduces after garbage collection (in this case to
        2.001x), because git compacts and de-duplicates the data in its
        repository. However, it cannot be reliably used unless there is enough
        space to store the uncompressed version first. Therefore Git starts to
        fail the test with files about 1/3 of the size of the available disk
        space.

    \item When the file is too large to fit into RAM, Git's commit operation
        prints an error message and exits with an error code, but the operation
        still completes successfully.

    \item Starting at just under 1GiB, the Git garbage collection seems to fail
        silently. No errors are reported, but on the graph one can see that the
        garbage-collected disk space usage jumps from 2x to 3x, indicating that
        the garbage collection phase is not doing any compacting. This might
        have to do with RAM requirements, similar to Mercurial's commit
        operation.

    \item Mercurial stores only deltas of files. In a sense, it is doing Git's
        garbage collection phase during each commit. This means more efficient
        disk usage. But it places a strong limitation on file size: Mercurial
        commits fail unless it has 3x as much RAM available as the size of the
        file. We suspect this has to do with the way Mercurial calculates. On
        the 8GiB test machines, Mercurial could only store files up to 2GiB in
        size.

\end{itemize}

Performance observations:

\begin{itemize}

    \item After some initial overhead, performance increases linearly with size.
        This is to be expected, since the operations are IO bound, copying all
        data to the repository.

    \item Times for the Mercurial update are faster than for Git's update,
        because Mercurial's archive format only saves deltas to the file.

    \item Mercurial has more initial time overhead, this is probably due to the
        fact that it is written in Python and requires starting the Python
        interpreter each time. This overhead is only 50 or 100ms, and quickly
        becomes insignificant compared to the IO operation time.

    \item Mercurial has less initial space overhead than Git.

        \begin{itemize}
            \setlength{\itemsep}{0pt}
            \setlength{\parskip}{0pt}
            \setlength{\parsep}{0pt}
            \item Minimum Mercurial repository size: 80KiB.
            \item Minimum Git repository size: 16KiB.
        \end{itemize}

        This can be seen in the disk space graph in the way the Mercurial usage
        converges towards 2x faster than Git usage does. But again, this quickly
        becomes insignificant compared to the size of the file.

\end{itemize}

\fi

\subsection{Committing an increasing number of small files}

\subsubsection{Results}

\begin{figure}[p]
  \caption{Real time required to commit many 1KiB files to an empty repository}
  \label{fig:plot-num-files--c1-time}
  \centering
    \includegraphics[]{plot-num-files--c1-time}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to commit many 1KiB files to an empty repository
  (detail)}
  \label{fig:plot-num-files--c1-time-detail}
  \centering
    \includegraphics[]{plot-num-files--c1-time-detail}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to commit many 1KiB files to an empty repository
  (detail)}
  \label{fig:plot-num-files--prototype-improvements--c1-time-detail}
  \centering
    \includegraphics[]{plot-num-files--prototype-improvements--c1-time-detail}
\end{figure}

\begin{figure}[p]
  \caption{CPU utilization while committing many 1KiB files to an empty
  repository}
  \label{fig:plot-num-files--c1-cpu}
  \centering
    \includegraphics[]{plot-num-files--c1-cpu-a}
    \includegraphics[]{plot-num-files--c1-cpu-b}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to check the status of many 1KiB files after
  initial commit}
  \label{fig:plot-num-files--stat1-time}
  \centering
    \includegraphics[]{plot-num-files--stat1-time}
\end{figure}

\begin{figure}[p]
  \caption{Total repository size after committing, editing, and committing again}
  \label{fig:plot-num-files--repo-size}
  \centering
    \includegraphics[]{plot-num-files--repo-size}
\end{figure}

Figure \ref{fig:plot-num-files--c1-time} shows the time
required for the initial commit, copying all files into a fresh empty
repository.

Figure \ref{fig:plot-num-files--c1-cpu} shows CPU utilization
during the commit.

Figure \ref{fig:plot-num-files--stat1-time} shows the time
required to check the changed status of all files just after committing.

Figure \ref{fig:plot-num-files--repo-size} shows the total
repository size, including the original files, after committing once, editing
1/1024th of every sixteenth file, and committing again.


\iffalse

We performed a test where increasingly large sets of files were committed to the
different version control repositories. The procedure was as follows:

\begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item Initialize an empty repository
    \item Generate a test file set of the given size. Each file is 1KiB of
        random binary data
    \item Commit the file set
    \item Check the status of the files
    \item Overwrite a small part of some of the files (1/1024th of the data in
        1/16 files)
    \item Check the status of the files again
    \item Commit the file set again
\end{enumerate}

Unlike the test with a single large file, the numerous small files did not
quickly hit error-causing disk space or RAM limitations. The version control
systems happily crunched the data as test times grew into hours.

Observations:

\begin{itemize}

    \item Again, after some initial overhead, commit and times increase
        linearly. However, Git's initial commit times actually decrease at
        certain points (128Ki, 1.5Mi, and 2Mi files). We are not sure how to
        explain this.

    \item Git in general is faster then Mercurial up to about half a million
        files. At 512Ki files is Mercurial and Git are about neck and neck. At
        768Ki and over, Mercurial is faster.

    \item Status check times are more erratic, though still increasing linearly
        overall. The variations may have to do with the output of the status
        commands and whether our terminal multiplexer was focused on the
        execution at the time. The status commands print one status line per
        file changed, which can be significant when hundreds of thousands of
        files involved. This output is significantly slower when the terminal
        multiplexer we used to monitor the experiments is connected, because it
        sends every line over the network to the monitoring machine.

    \item Mercurial update status is consistently slower than initial status,
        often by about 2-3x.

    \item Both Git and Mercurial converge to a little over 8x the space
        required. This probably has more to do with the filesystem block size
        than anything else. The underlying file system uses a 4KiB block size,
        so each 1KiB file will still use 4KiB of disk space. And since there are
        two copies of each file, that's 8KiB total for each 1KiB file, 8x the
        disk space.

    \item Mercurial converges towards the 8x limit faster though. We guess this
        is because of lower repo overhead, and also because Git is creating tree
        objects for each of the subdirectories in the file set. These files will
        be small, but each will take up another 4KiB block on the disk.

    \item Mercurial commits began to abort with disk space errors at 8Mi files,
        8GiB of data. This was surprising. Even at 8x disk usage, that should
        only be 64GiB of disk usage, well below the 192GiB free on the test
        disk.

\end{itemize}

\fi


\section{Performance Tuning}


\subsection{Rolling Hash Chunk Size}

\subsubsection{Methodology}

This experiment is written as a unit test in the prototype Rust code. The test
is found in the \texttt{rolling\_hash} module, under the name
\texttt{chunk\_size\_experiment}.

\begin{itemize}

  \item Generate random data and pass it through the rolling hash algorithm with
    the given window size and match parameters.

  \item Continue generating data until the rolling hash has flagged 100 chunks.

  \item Calculate the mean chunk size and standard deviation.

\end{itemize}

\subsubsection{Results}

\begin{figure}[p]
  \caption{Mean chunk size}
  \label{fig:plot-rolling-hash}
  \centering
    \includegraphics[]{plot-rolling-hash}
\end{figure}

Figure \ref{fig:plot-rolling-hash} shows the mean chunk size.

\subsubsection{Observations}

\begin{itemize}

  \item The mean chunk size is approximately the sum of the window size and
    match parameter.

  \item When the match parameter is less than the window size, the standard
    deviation is approximately the match parameter.

\end{itemize}


\subsection{Directory Structure}

\subsubsection{Methodology}

\begin{itemize}

  \item Create an empty 100MiB partition.

  \item Write 4KiB objects according to the given object directory scheme, until
    the disk is reported as full.

  \item Track the total number of files written and total number of directories
    created.

\end{itemize}

\subsubsection{Results}

\begin{figure}[p]
  \caption{Overwhelmed by subdirectories}
  \label{fig:plot-filesystem-limits--directory-takeover}
  \centering
    \includegraphics[]{plot-filesystem-limits--directory-takeover}
\end{figure}

Figure \ref{fig:plot-filesystem-limits--directory-takeover} shows directories
overtaking files as nesting goes deeper.

\subsubsection{Observations}

\begin{itemize}

  \item In hindsight, it should have been easy to see that the number of
    directories would grow exponentially.

\end{itemize}



\chapter{Related Works}

\section{Distributed storage and synchronization systems}

\subsection{Camlistore}

Camlistore \cite{camlistore_homepage} is an open-source project to create a
private long-term data storage system for personal users. It allows storage of
diverse types of data and it synchronizes between multiple replicas of the data
store. However, it eschews normal filesystems and creates its own schemas to
store various media.


\subsection{Dat Data}

Dat \cite{dat_homepage} is an open-source project for publishing and sharing
scientific data sets for research. This project has a lot of overlap with ours,
and several of the core ideas are similar, including breaking files into smaller
chunks, and tracking changes via a Git-like DAG. However, their focus is
different. The Dat team is concentrating on publishing research data, and making
that specific task as simple as possible for non-technical researchers who might
not be familiar with version control. By contrast, our project operates at a
lower level of abstraction, offering the full power of version control in a very
general way, exposing and illuminating the complexities rather than trying to
hide them or automate them away.

Where Dat focuses on publishing on the open internet, we focus on ad-hoc
networks and data that may be private. Where Dat has components for automating
peer discovery and consensus, we work at a lower level, trying to perfect and
generalize the storage aspect first. Dat seems to assume that data sets will be
small enough to fit on a typical disk on a workstation, while we want to scale
even larger.

We hope that our system could be used as a base to build something like Dat, but
we intend to create something even more general than the Dat core.


\subsection{Eyo}

Eyo \cite{Strauss:2011:EDP:2002181.2002216} is system for storing personal media
and synchronizing it between devices. It utilizes a Git-like content-addressed
object database behind the scenes, but it works more like a networked filesystem
than version control. It focuses on organizing media by metadata, which requires
agreement on metadata formats, and it requires applications to be rewritten to
access files via Eyo rather than the filesystem, both of which are thorny and
ambitious problems. We prefer to focus purely on storage and synchronization.


\subsection{git-annex and git-media}

Git-annex \cite{git_annex_homepage} and git-media \cite{git_media_github} are
open-source projects that extend Git with special handling for larger files.
Both store the metadata of larger files in the normal Git repository and then
store the files themselves in a separate location. Git-media stores all the
larger files in a separate data store which may be remote. Git-annex is more
flexible. Annex files may be spread across several different remote repository
clones or data stores, and git-annex has features for tracking the locations of
annex files in different remote repositories and moving them from one repository
to another. These tracking and distribution features are very similar to our
goals. However, git-annex is not quite as flexible as we aim for in our system.
It considers the large files atomic units, and it does not break them into
smaller chunks for de-duplication. Also, because metadata is processed by Git,
it has the same limitations that Git does. All repositories must have all
metadata, and performance suffers when metadata is too large to fit into RAM.


\subsection{IPFS: The Interplanetary Filesystem}

IPFS \cite{ipfs_github_main} is an open-source project to create a global
content-addressed filesystem. By its global nature, all files are stored
together, publicly, in a global network of nodes with global addressing. IPFS
should be an excellent resource for storing published information, but we wanted
to work on a smaller, more private scale with discrete data sets. We want
individuals and organizations to be able manage their own data stores privately
on their own hardware.

It should be noted that IPFS does have support for storing private objects by
way of object-level encryption. However, this seems wasteful of disk space,
since small changes in the plain text of a file would completely change the
ciphertext, leaving no way to compress the redundancy.


\subsection{Kademlia}

Kademlia \cite{Maymounkov2002} is an advanced distributed hash table system that
updates its network topology information as part of normal lookups. It is an
advanced piece of infrastructure, but like other distributed hash tables, it
focuses on system-wide consistency, rather than the version-control paradigm we
are trying to achieve.


\section{Content-Addressed Storage and Backup}

\subsection{Boar}

Boar \cite{boar_homepage} is an open-source project to create a version control
system for large binary files. It is one of the main inspirations for our
project. It stores file versions in a content-addressed way, and provides
de-duplication for large files that only change in small pieces, and it can
truncate history to reclaim disk space. However, Boar retreats to a centralized
version control paradigm, with a central repository that working directories
must connect to to check files in or out. We want to provide the advantages of
Boar in a flexible distributed version control model. Boar also has practical
limitations on repository size and number of files. Repositories are assumed to
fit on one disk volume, and file metadata is assumed to fit into Ram. We aim to
overcome both of those limitations.


\subsection{Bup}

Bup \cite{bup_homepage} is an open-source file backup system that is based on
Git's repository format. A Bup backup is a valid Git repository and it can be
read by Git, but Bup is a separate program written from scratch to read and
write files to Git's pack file format directly, skipping Git's separate store
and pack steps that use double the disk space. It has many features that we want
for our low-level storage of the object database. It breaks files into chunks by
rolling checksum, and it has considerations for metadata that is larger than
RAM. However, it is locked into a backup-based workflow. History is linear and
based on clock time of backup. And it assumes that the whole data set and the
whole repository can fit onto one filesystem.



\backmatter


% Bibliography

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}

%\nocite{*}  % Print all references even if they're not used
\bibliographystyle{plain}
\bibliography{research}

\end{document}
