\chapter{Experiments}




\section{Version Control System Scaling Experiments}
\label{num-files-exp-desc}
\label{file-size-exp-desc}

We performed experiments to probe the limits of existing version control
systems, to see how they would cope with file sizes and file numbers in ranges
beyond what would be expected in a source code tree. We wanted to see how long
it would take for each VCS to store that amount of data, how much disk space it
used, and what CPU utilization was like during storage. And since version
controls systems track changes, so we also wanted to see what would happen when
a small subset of that data was modified and then committed again.

We conducted two major experiments. To measure the effect of file size, we would
commit a single file of increasing size to each target VCS. And to measure
the effect of file numbers, we would commit increasing number of small
(\SI{1}{\kibi\byte}) files to each target VCS.



\subsection{Version Control Systems Tested}

We ran each experiment using four different version control systems: Git,
Mercurial (also known as \newterm{hg}), Bup, and our DMV prototype (specific
versions listed in \autoref{vcs-versions}). As a control we also ran the
experiments using a dummy VCS that simple made a copy of the files. We chose Git
and Mercurial because they are the two most widely used distributed version
control systems available. We also included Bup because it uses Git's storage
format but, like DMV, it also breaks files into chunks. For discussion on the
similarities and differences between DMV and Bup, see \autoref{related_bup}.

\towrite{Include more background about Git, Mercurial, and Bup: Git and Hg
conceptually use same DAG but implemented differently. Bup uses Git's file
format directly but is rewritten.}

\begin{table}
    \caption{Version control systems tested and their versions}
    \label{vcs-versions}
    \centering
    \begin{tabular}{ l l }
        Git & 2.1.4 \\
        Mercurial (hg) & 3.1.2 \\
        Bup & debian/0.25-1 \\
        DMV prototype & exp\_prototype2x1mem (c9baf3a) \\
    \end{tabular}
\end{table}


\subsection{Procedure}

For each experiment, the procedure for a single trial was as follows:

\begin{tight_enumerate}

    \item Create an empty repository of the target VCS in a temporary directory

    \item Generate target data to store, either a single file of the target
        size, or the target number of \SI{1}{\kibi\byte} files

    \item Commit the target data to the repository, measuring wall clock time to
        commit

    \item Verify that the first commit exists in the repository, and if there
        was any kind of error, run the repository's integrity check operation

    \item Measure the total repository size

    \item Overwrite a fraction of each target file

    \item Commit again, measuring wall clock time to commit

    \item Verify that the second commit exists in the repository, and if there
        was any kind of error, run the repository's integrity check operation

    \item Measure the total repository size again

    \item Delete temporary directory and all trial files

\end{tight_enumerate}

We increased file sizes exponentially by powers of two from \SI{1}{\byte} up to
\SI{128}{\gibi\byte}, adding an additional step at \num{1.5} times the base size
at each order of magnitude. For example, starting at \SI{1}{\mebi\byte}, we
would run trails with \SI{1}{\mebi\byte}, \SI{1.5}{\mebi\byte},
\SI{2}{\mebi\byte}, \SI{3}{\mebi\byte}, \SI{4}{\mebi\byte}, \SI{6}{\mebi\byte},
\SI{8}{\mebi\byte}, \SI{12}{\mebi\byte}, and so on.

We increased numbers of files exponentially by powers of ten from one file to
ten million files, adding additional steps at \num{2.5}, \num{5}, and \num{7.5}
times the base number at each order of magnitude. For example, starting at
\num{100} files we would run trials with \num{100}, \num{250}, \num{500},
\num{750}, \num{1000}, \num{2500}, \num{5000}, \num{7500}, \num{10000}, and so
on.

Test data files consisted simply of pseudo-random bytes taken from the operating
system's pseudo-random number generator (\lstinline{/dev/urandom} on Linux).

When updating data files for the second commit, we would overwrite a single
contiguous section of each file with new pseudo-random bytes. We would start
one-quarter of the way into the file, and overwrite \num{1/1024}th of the file's
size (or 1 byte if the file was smaller than \SI{1024}{\kibi\byte}). So a
\SI{1}{\mebi\byte} file would have \SI{1}{\kibi\byte} overwritten, a
\SI{1}{\gibi\byte} file would have \SI{1}{\mebi\byte} overwritten, and so on.


\subsection{Automation and Measurement}

The trials were run via a Python script that would set up, run, and clean up
each trial in a loop, covering the full range of sizes or numbers for a given
VCS. The script would measure the wall-clock time duration taken by each commit
command and collect CPU utilization metrics. It would also terminate any
individual VCS operation that ran longer than five hours. After commit and
verification, the script would also measure repository size.

The script measured the wall-clock time duration for each commit by checking the
system time (\lstinline{time.time()}) just before and just after using Python's
\lstinline{subprocess} module to execute the necessary VCS command. CPU
utilization was measured by sampling the CPU status lines provided in Linux's
\lstinline{/proc/stat} information. The status lines show a cumulative count of
time slices that the CPU has spent in user mode, system mode, and waiting for
I/O. Like with the time measurements, the script samples CPU utilization before
and after executing a VCS command, and then subtracts to get the number of time
slices spent in each state during execution. We then compare the relative number
of time slices in each state to get an idea of whether the operation is
CPU-bound or I/O-bound.

The script measures repository size using the standard Unix disk usage command
(\lstinline{du}) and measures the size of the trial's entire temporary
directory, which includes the generated test data itself along with the VCS's
storage.


\subsection{Test Environment}

We ran the trials on four identical, dedicated test computers with no other
load. Each was a typical office desktop with a \SI{3.16}{\giga\hertz}
\num{64}-bit dual-core processor and \SI{8}{\gibi\byte} of RAM, running Debian
version 8.6 ("Jessie"). Each computer had one normal SATA hard disk (spinning
platter, not solid-state), and trials were conducted on a dedicated
\SI{197}{\gibi\byte} LVM partition formatted with the ext4 filesystem.
Additional details can be found in \autoref{test-machine-specs}.

\begin{table}
    \caption{Test computer specifications}
    \label{test-machine-specs}
    \begin{tabular}{ l l }
        Vendor & Hewlett Packard \\
        CPU & Intel(R) Core(TM)2 Duo CPU     E8500  @ 3.16GHz \\
        RAM & \SI{8}{\gibi\byte} \\
        Hard disk & ATA model ST3250318AS \\
        \midrule
        Operating system & Debian 8.6 ("Jessie") \\
        Kernel & Linux 3.16.0 \\
        \midrule
        Test partition & \SI{197}{\gibi\byte} LVM partition \\
        Filesystem & ext4 \\
        I/O scheduler & cfq (unless otherwise noted) \\
    \end{tabular}
\end{table}

We ran every trial four times, once on each of the test computers, and took the
mean and standard deviation of each time and disk space measurement. However,
because the test machines are identical, there was little real variation.





\section{Tuning Chunk Size}

The algorithm used to divide files into chunks (described in
\autoref{chunking-algoritm}) involves moving a window across the data, and
setting a chunk boundary where the sum of the bytes in that window is evenly
divisible by a given number. We ran an experiment to determine the effects of
these two parameters on chunk size.

\paragraph{Procedure}

For each combination of window size and divisor, we would run the rolling hash
algorithm on a stream of pseudo-random bytes until it had identified \num{100}
chunks. Then we would compute the mean and standard deviation of the chunk
sizes.

We used window sizes in powers of two from \SI{128}{\byte} ($2^7$) to
\SI{256}{\kibi\byte} ($2^{18}$), and divisors in powers of two from \num{256}
($2^8$) to \SI{128}{\kibi\relax} ($2^{17}$).

The pseudo-random number generator used was an xorshift RNG\cite{xorshift_rng}.
The experiment itself was automated as a unit test in the DMV prototype's Rust
code.

\paragraph{Environment}

Because this experiment measures only the output of calculations, the
environment in which it is run should make no difference in the outcome. In
fact, if the xorshift RNG is given the same initial seed value, the resulting
random byte stream will be identical, which will lead to an identical sequence
of chunks, which will lead to an identical average chunk size. This experiment
is deterministic.

%


\section{Tuning Object Store Directory Layout}\label{dir-experiment}

During initial runs of the "many-files" experiment
(\autoref{num-files-exp-desc}), we would often notice the disk being reported as
full even though the total bytes used was less than the capacity of the disk
partition. This had to do with the way we divided the test files into
directories, and how the version control systems stored their objects (see
\autoref{dir-impl} for DMV's object storage scheme). Since every directory
requires an inode on an ext4 filesystem, a storage scheme that creates too many
subdirectories will use up all the available inodes with directories rather than
data.

\todo{Note mysterious seek times}

\paragraph{Procedure}

To test the effects of different object storage schemes, we created a new
\SI{100}{\mebi\byte} partition on a test computer, and then generated a series
of pseudo-random files of \SI{4}{\kibi\byte} each until the disk was reported
full. For each file, we would give it a pseudo-random name that resembled an
SHA-1 hash, and store it according to the object storage scheme under test. We
increased a counter each time we created a file, and another each time we
created a new directory.

We followed the basic scheme of taking leading hex digits of the SHA-1 hash to
form directories. We varied the number of directories taken (depth) and the
number of hex digits per directory (see
\autoref{sample-directory-scheme-variations} for examples). We tried depths from
\num{0} to \num{6} and digits per directory from \num{0} to \num{16}, discarding
combinations that did not make sense, such as combinations involving \num{0} and
another number (which would all simply be undivided), or those that required
more than the \num{40} hex digits of a \num{160}-byte SHA-1 hash.

\begin{table}[h]
    \caption{Sample object store directory variations}
    \label{sample-directory-scheme-variations}
    \centering
    \begin{tabular}{l l l}
        Hex digits & Depth & Example \\
        \midrule
        0 & 0 & \lstinline{03d37679d1fab86e5286decd6cd2a94efcfe083f} \\
        1 & 1 & \lstinline{7/9332ca7ce9163f78e3c115a2173bd8fd853d286} \\
        1 & 3 & \lstinline{6/8/c/40e64f3e74e6ebefdcf2f5f30fb8da004792c} \\
        2 & 1 & \lstinline{9f/4ec22c3e0289b29eefefe4728dece14e67e6ac} \\
        2 & 2 & \lstinline{dd/52/bcccff156a179cdac0793ef049039372d8a1} \\
        3 & 1 & \lstinline{cc5/199084d70f7c5ba325a240e1927579ee24bb1} \\
        3 & 4 & \lstinline{472/e98/e88/0b1/c5905065c70cbe806361d32f6429} \\
        4 & 3 & \lstinline{1ed2/bd51/01fe/5b23763e8c76852739f59201280f} \\
    \end{tabular}
\end{table}

\paragraph{Environment}

Like the "many files" experiment, this was automated as a Python script and run
on one of the dedicated test computers used for that experiment (specs shown in
\autoref{test-machine-specs}). However, rather than spending hours to fill the
\SI{197}{\gibi\byte} partition used for the other experiments, this experiment
used a new \SI{100}{\mebi\byte} LVM partition.
