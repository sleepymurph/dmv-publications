\chapter{Experiments and Results}

\section{Version Control System Behavior Experiments}

\subsection{Committing a single file of increasing size}

\iffalse

\subsubsection{Methodology}

The experiment was written as a script in Python. For each version control
system, and each file size, this script would do the following:

\begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item Initialize an empty repository
    \item Generate a test file of the given size from random binary data
    \item Commit the file
    \item Overwrite a small part of the file (1/1024th of the data)
    \item Commit the file again
    \item Run the garbage collection / compaction algorithm (Git only)
\end{enumerate}

Times were measured by the Python script, checking the time of the real-time
clock before and after the relevant command was run.

Total repository size was measured via the Unix \lstinline{du} command, and it
includes the original file in the working directory, as well as the repository.

Trials were performed four times each, on four identical computers. Because the
computers are identical, the performance across trials was nearly identical.
These graphs display the mean values and have error bars for standard deviation,
but the error bars are difficult to see.

\subsubsection{Platform and Configuration}

Each identical experiment computer had a 3GHz Intel processor, 8GiB of RAM, and
about 200GiB of free hard disk space.

\fi

\subsubsection{Results}

\begin{figure}[p]
  \caption{Real time required to commit one file to an empty repository}
  \label{fig:plot-file-size--c1-time}
  \centering
    \includegraphics[]{plot-file-size--c1-time}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to commit one file to an empty repository (detail)}
  \label{fig:plot-file-size--c1-time--detail-high-end}
  \centering
    \includegraphics[]{plot-file-size--c1-time--detail-low-end}
    \includegraphics[]{plot-file-size--c1-time--detail-high-end}
\end{figure}

\begin{figure}[p]
  \caption{Increasing file size: prototype improvements}
  \label{fig:plot-file-size--c1-time--prototype-improvements}
  \centering
    \includegraphics[]{plot-file-size--c1-time--detail-high-end--prototype-improvements}
\end{figure}

\begin{figure}[p]
  \caption{CPU utilization while committing one file to an empty repository}
  \label{fig:plot-file-size--c1-cpu}
  \centering
    \includegraphics[]{plot-file-size--c1-cpu-a}
    \includegraphics[]{plot-file-size--c1-cpu-b}
\end{figure}

\begin{figure}[p]
  \caption{Total repository size after committing, editing, and committing again}
  \label{fig:plot-file-size--repo-size}
  \centering
    \includegraphics[]{plot-file-size--repo-size}
\end{figure}

\autoref{fig:plot-file-size--c1-time} shows time required for
the initial commit, copying the file into a fresh empty repository.

\autoref{fig:plot-file-size--c1-time--detail-high-end} shows
detailed views of \autoref{fig:plot-file-size--c1-time}, zooming
in and using a linear scale.

\autoref{fig:plot-file-size--c1-cpu} shows the
CPU usage during the initial commit.

\autoref{fig:plot-file-size--repo-size} shows the total
repository size, including the original file, after committing once, editing
1/1024th of the file, and committing again.

\iffalse

\subsubsection{Observations}

\begin{itemize}

  \item Mercurial's commit algorithm requires that the entire file fit into RAM
    three time over. On our trial computers with 8GiB of ram, the largest file
    Mercurial could commit successfully was 2GiB. At larger sizes, the commit
    command would abort, roll back the commit in progress, and exit with an
    error status.

    This reflects the fact that Mercurial stores only changes to files. It must
    examine every file with a diff algorithm during the commit.

\end{itemize}

\todo{rewrite observations below and place above}

\begin{itemize}

    \item Git initially stores a full copy of every revision and then has a
        separate garbage collection phase to compact stored data in the
        repository. So for a single commit, the operation uses 2x the disk space
        as the size of the file: one for the file itself, and one for the copy
        in the repository. And for a single commit and update, the operation
        requires 3x the disk space: one for the file itself, one for the initial
        copy in the repository, and one for the updated copy.

        Disk space usage reduces after garbage collection (in this case to
        2.001x), because git compacts and de-duplicates the data in its
        repository. However, it cannot be reliably used unless there is enough
        space to store the uncompressed version first. Therefore Git starts to
        fail the test with files about 1/3 of the size of the available disk
        space.

    \item When the file is too large to fit into RAM, Git's commit operation
        prints an error message and exits with an error code, but the operation
        still completes successfully.

    \item Starting at just under 1GiB, the Git garbage collection seems to fail
        silently. No errors are reported, but on the graph one can see that the
        garbage-collected disk space usage jumps from 2x to 3x, indicating that
        the garbage collection phase is not doing any compacting. This might
        have to do with RAM requirements, similar to Mercurial's commit
        operation.

    \item Mercurial stores only deltas of files. In a sense, it is doing Git's
        garbage collection phase during each commit. This means more efficient
        disk usage. But it places a strong limitation on file size: Mercurial
        commits fail unless it has 3x as much RAM available as the size of the
        file. We suspect this has to do with the way Mercurial calculates. On
        the 8GiB test machines, Mercurial could only store files up to 2GiB in
        size.

\end{itemize}

Performance observations:

\begin{itemize}

    \item After some initial overhead, performance increases linearly with size.
        This is to be expected, since the operations are IO bound, copying all
        data to the repository.

    \item Times for the Mercurial update are faster than for Git's update,
        because Mercurial's archive format only saves deltas to the file.

    \item Mercurial has more initial time overhead, this is probably due to the
        fact that it is written in Python and requires starting the Python
        interpreter each time. This overhead is only 50 or 100ms, and quickly
        becomes insignificant compared to the IO operation time.

    \item Mercurial has less initial space overhead than Git.

        \begin{itemize}
            \setlength{\itemsep}{0pt}
            \setlength{\parskip}{0pt}
            \setlength{\parsep}{0pt}
            \item Minimum Mercurial repository size: 80KiB.
            \item Minimum Git repository size: 16KiB.
        \end{itemize}

        This can be seen in the disk space graph in the way the Mercurial usage
        converges towards 2x faster than Git usage does. But again, this quickly
        becomes insignificant compared to the size of the file.

\end{itemize}

\fi

\subsection{Committing an increasing number of small files}

\subsubsection{Results}

\begin{figure}[p]
  \caption{Real time required to commit many 1KiB files to an empty repository}
  \label{fig:plot-num-files--c1-time}
  \centering
    \includegraphics[]{plot-num-files--c1-time}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to commit many 1KiB files to an empty repository
  (detail)}
  \label{fig:plot-num-files--c1-time-detail}
  \centering
    \includegraphics[]{plot-num-files--c1-time-detail}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to commit many 1KiB files to an empty repository
  (detail)}
  \label{fig:plot-num-files--prototype-improvements--c1-time-detail}
  \centering
    \includegraphics[]{plot-num-files--prototype-improvements--c1-time-detail}
\end{figure}

\begin{figure}[p]
  \caption{CPU utilization while committing many 1KiB files to an empty
  repository}
  \label{fig:plot-num-files--c1-cpu}
  \centering
    \includegraphics[]{plot-num-files--c1-cpu-a}
    \includegraphics[]{plot-num-files--c1-cpu-b}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to check the status of many 1KiB files after
  initial commit}
  \label{fig:plot-num-files--stat1-time}
  \centering
    \includegraphics[]{plot-num-files--stat1-time}
\end{figure}

\begin{figure}[p]
  \caption{Total repository size after committing, editing, and committing again}
  \label{fig:plot-num-files--repo-size}
  \centering
    \includegraphics[]{plot-num-files--repo-size}
\end{figure}

\autoref{fig:plot-num-files--c1-time} shows the time
required for the initial commit, copying all files into a fresh empty
repository.

\autoref{fig:plot-num-files--c1-cpu} shows CPU utilization
during the commit.

\autoref{fig:plot-num-files--stat1-time} shows the time
required to check the changed status of all files just after committing.

\autoref{fig:plot-num-files--repo-size} shows the total
repository size, including the original files, after committing once, editing
1/1024th of every sixteenth file, and committing again.


\iffalse

We performed a test where increasingly large sets of files were committed to the
different version control repositories. The procedure was as follows:

\begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item Initialize an empty repository
    \item Generate a test file set of the given size. Each file is 1KiB of
        random binary data
    \item Commit the file set
    \item Check the status of the files
    \item Overwrite a small part of some of the files (1/1024th of the data in
        1/16 files)
    \item Check the status of the files again
    \item Commit the file set again
\end{enumerate}

Unlike the test with a single large file, the numerous small files did not
quickly hit error-causing disk space or RAM limitations. The version control
systems happily crunched the data as test times grew into hours.

Observations:

\begin{itemize}

    \item Again, after some initial overhead, commit and times increase
        linearly. However, Git's initial commit times actually decrease at
        certain points (128Ki, 1.5Mi, and 2Mi files). We are not sure how to
        explain this.

    \item Git in general is faster then Mercurial up to about half a million
        files. At 512Ki files is Mercurial and Git are about neck and neck. At
        768Ki and over, Mercurial is faster.

    \item Status check times are more erratic, though still increasing linearly
        overall. The variations may have to do with the output of the status
        commands and whether our terminal multiplexer was focused on the
        execution at the time. The status commands print one status line per
        file changed, which can be significant when hundreds of thousands of
        files involved. This output is significantly slower when the terminal
        multiplexer we used to monitor the experiments is connected, because it
        sends every line over the network to the monitoring machine.

    \item Mercurial update status is consistently slower than initial status,
        often by about 2-3x.

    \item Both Git and Mercurial converge to a little over 8x the space
        required. This probably has more to do with the filesystem block size
        than anything else. The underlying file system uses a 4KiB block size,
        so each 1KiB file will still use 4KiB of disk space. And since there are
        two copies of each file, that's 8KiB total for each 1KiB file, 8x the
        disk space.

    \item Mercurial converges towards the 8x limit faster though. We guess this
        is because of lower repo overhead, and also because Git is creating tree
        objects for each of the subdirectories in the file set. These files will
        be small, but each will take up another 4KiB block on the disk.

    \item Mercurial commits began to abort with disk space errors at 8Mi files,
        8GiB of data. This was surprising. Even at 8x disk usage, that should
        only be 64GiB of disk usage, well below the 192GiB free on the test
        disk.

\end{itemize}

\fi


\section{Performance Tuning}


\subsection{Rolling Hash Chunk Size}

\subsubsection{Methodology}

This experiment is written as a unit test in the prototype Rust code. The test
is found in the \texttt{rolling\_hash} module, under the name
\texttt{chunk\_size\_experiment}.

\begin{itemize}

  \item Generate random data and pass it through the rolling hash algorithm with
    the given window size and match parameters.

  \item Continue generating data until the rolling hash has flagged 100 chunks.

  \item Calculate the mean chunk size and standard deviation.

\end{itemize}

\subsubsection{Results}

\begin{figure}[p]
  \caption{Mean chunk size}
  \label{fig:plot-rolling-hash}
  \centering
    \includegraphics[]{plot-rolling-hash}
\end{figure}

\autoref{fig:plot-rolling-hash} shows the mean chunk size.

\subsubsection{Observations}

\begin{itemize}

  \item The mean chunk size is approximately the sum of the window size and
    match parameter.

  \item When the match parameter is less than the window size, the standard
    deviation is approximately the match parameter.

\end{itemize}


\subsection{Directory Structure}

\subsubsection{Methodology}

\begin{itemize}

  \item Create an empty 100MiB partition.

  \item Write 4KiB objects according to the given object directory scheme, until
    the disk is reported as full.

  \item Track the total number of files written and total number of directories
    created.

\end{itemize}

\subsubsection{Results}

\begin{figure}[p]
  \caption{Overwhelmed by subdirectories}
  \label{fig:plot-filesystem-limits--directory-takeover}
  \centering
    \includegraphics[]{plot-filesystem-limits--directory-takeover}
\end{figure}

\autoref{fig:plot-filesystem-limits--directory-takeover} shows directories
overtaking files as nesting goes deeper.

\subsubsection{Observations}

\begin{itemize}

  \item In hindsight, it should have been easy to see that the number of
    directories would grow exponentially.

\end{itemize}
