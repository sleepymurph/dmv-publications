\chapter{Experiments}




\section{Version Control System Scaling Experiments}
\label{num-files-exp-desc}
\label{file-size-exp-desc}

We performed experiments to probe the limits of existing version control
systems, to see how they would cope with file sizes and file numbers in ranges
beyond what would be expected in a source code tree. We wanted to see how long
it would take for each VCS to store that amount of data, how much disk space it
used, and what CPU utilization was like during storage. And since version
controls systems track changes, so we also wanted to see what would happen when
a small subset of that data was modified and then committed again.

We conducted two major experiments. To measure the effect of file size, we would
commit a single file of increasing size to each target VCS. And to measure
the effect of file numbers, we would commit increasing number of small
(\SI{1}{\kibi\byte}) files to each target VCS.



\subsection{Version Control Systems Tested}

We ran each experiment using four different version control systems: Git,
Mercurial (also known as \newterm{hg}), Bup, and our DMV prototype (specific
versions listed in \autoref{vcs-versions}). As a control we also ran the
experiments using a dummy VCS that simple made a copy of the files. We chose Git
and Mercurial because they are the two most widely used distributed version
control systems available. We also included Bup because it uses Git's storage
format but, like DMV, it also breaks files into chunks. For discussion on the
similarities and differences between DMV and Bup, see \autoref{related_bup}.

\begin{table}
    \caption{Version control systems tested and their versions}
    \label{vcs-versions}
    \centering
    \begin{tabular}{ l l }
        Git & 2.1.4 \\
        Mercurial (hg) & 3.1.2 \\
        Bup & debian/0.25-1 \\
        DMV prototype & exp\_prototype2x1mem (c9baf3a) \\
    \end{tabular}
\end{table}


\subsection{Procedure}

For each experiment, the procedure for a single trial was as follows:

\begin{tight_enumerate}

    \item Create an empty repository of the target VCS in a temporary directory

    \item Generate target data to store, either a single file of the target
        size, or the target number of \SI{1}{\kibi\byte} files

    \item Commit the target data to the repository

    \item Verify that the commit succeeded

    \item Overwrite a fraction of each target file

    \item Commit again

    \item Verify that the commit succeeded

    \item Delete temporary directory and all trial files

\end{tight_enumerate}

We increased file sizes exponentially by powers of two from \SI{1}{\byte} up to
\SI{128}{\gibi\byte}, adding an additional step at \num{1.5} times the base size
at each order of magnitude. For example, starting at \SI{1}{\mebi\byte}, we
would run trails with \SI{1}{\mebi\byte}, \SI{1.5}{\mebi\byte},
\SI{2}{\mebi\byte}, \SI{3}{\mebi\byte}, \SI{4}{\mebi\byte}, \SI{6}{\mebi\byte},
\SI{8}{\mebi\byte}, \SI{12}{\mebi\byte}, and so on.

We increased numbers of files exponentially by powers of ten from one file to
ten million files, adding additional steps at \num{2.5}, \num{5}, and \num{7.5}
times the base number at each order of magnitude. For example, starting at
\num{100} files we would run trials with \num{100}, \num{250}, \num{500},
\num{750}, \num{1000}, \num{2500}, \num{5000}, \num{7500}, \num{10000}, and so
on.

Test data files consisted simply of pseudo-random bytes taken from the operating
system's pseudo-random number generator (\lstinline{/dev/urandom} on Linux).

When updating data files for the second commit, we would overwrite a single
contiguous section of each file with new pseudo-random bytes. We would start
one-quarter of the way into the file, and overwrite \num{1/1024}th of the file's
size (or 1 byte if the file was smaller than \SI{1024}{\kibi\byte}). So a
\SI{1}{\mebi\byte} file would have \SI{1}{\kibi\byte} overwritten, a
\SI{1}{\gibi\byte} file would have \SI{1}{\mebi\byte} overwritten, and so on.


\subsection{Automation and Measurement}

The trials were run via a Python script that would set up, run, and clean up
each trial in a loop, covering the full range of sizes or numbers for a given
VCS. The script would measure the wall-clock time duration taken by each commit
command and collect CPU utilization metrics. It would also terminate any
individual VCS operation that ran longer than five hours. After commit and
verification, the script would also measure repository size.

The script measured the wall-clock time duration for each commit by checking the
system time (\lstinline{time.time()}) just before and just after using Python's
\lstinline{subprocess} module to execute the necessary VCS command. CPU
utilization was measured by sampling the CPU status lines provided in Linux's
\lstinline{/proc/stat} information. The status lines show a cumulative count of
time slices that the CPU has spent in user mode, system mode, and waiting for
I/O. Like with the time measurements, the script samples CPU utilization before
and after executing a VCS command, and then subtracts to get the number of time
slices spent in each state during execution. We then compare the relative number
of time slices in each state to get an idea of whether the operation is
CPU-bound or I/O-bound.

The script measures repository size using the standard Unix disk usage command
(\lstinline{du}) and measures the size of the trial's entire temporary
directory, which includes the generated test data itself along with the VCS's
storage.


\subsection{Test Environment}

We ran the trials on four identical, dedicated test computers with no other
load. Each was a typical office desktop with a \SI{3.16}{\giga\hertz}
\num{64}-bit dual-core processor and \SI{8}{\gibi\byte} of RAM, running Debian
version 8.6 ("Jessie"). Each computer had one normal SATA hard disk (spinning
platter, not solid-state), and trials were conducted on a dedicated
\SI{197}{\gibi\byte} LVM partition formatted with the ext4 filesystem.
Additional details can be found in \autoref{test-machine-specs}.

\begin{table}
    \caption{Test computer specifications}
    \label{test-machine-specs}
    \begin{tabular}{ l l }
        Vendor & Hewlett Packard \\
        CPU & Intel(R) Core(TM)2 Duo CPU     E8500  @ 3.16GHz \\
        RAM & \SI{8}{\gibi\byte} \\
        Hard disk & ATA model ST3250318AS \\
        \midrule
        Operating system & Debian 8.6 ("Jessie") \\
        Kernel & Linux 3.16.0 \\
        \midrule
        Test partition & \SI{197}{\gibi\byte} LVM partition \\
        Filesystem & ext4 \\
        I/O scheduler & cfq (unless otherwise noted) \\
    \end{tabular}
\end{table}

We ran every trial four times, once on each of the test computers, and took the
mean and standard deviation of each time and disk space measurement. However,
because the test machines are identical, there was little real variation.





\section{Tuning Chunk Size}

The algorithm used to divide files into chunks (described in
\autoref{chunking-algoritm}) involves moving a window across the data, and
setting a chunk boundary where the sum of the bytes in that window is evenly
divisible by a given number. We ran an experiment to determine the effects of
these two parameters on chunk size.

\paragraph{Procedure}

For each combination of window size and divisor, we would run the rolling hash
algorithm on a stream of pseudo-random bytes until it had identified \num{100}
chunks. Then we would compute the mean and standard deviation of the chunk
sizes.

We used window sizes in powers of two from \SI{128}{\byte} ($2^7$) to
\SI{256}{\kibi\byte} ($2^{18}$), and divisors in powers of two from \num{256}
($2^8$) to \SI{128}{\kibi\relax} ($2^{17}$).

The pseudo-random number generator used was an xorshift RNG\cite{xorshift_rng}.
The experiment itself was automated as a unit test in the DMV prototype's Rust
code.

\paragraph{Environment}

Because this experiment measures only the output of calculations, the
environment in which it is run should make no difference in the outcome. In
fact, if the xorshift RNG is given the same initial seed value, the resulting
random byte stream will be identical, which will lead to an identical sequence
of chunks, which will lead to an identical average chunk size. This experiment
is deterministic.

%


\section{Tuning Object Store Directory Layout}\label{dir-experiment}

During initial runs of the "many-files" experiment
(\autoref{num-files-exp-desc}), we would often notice the disk being reported as
full even though the total bytes used was less than the capacity of the disk
partition. This had to do with the way we divided the test files into
directories, and how the version control systems stored their objects (see
\autoref{dir-impl} for DMV's object storage scheme). Since every directory
requires an inode on an ext4 filesystem, a storage scheme that creates too many
subdirectories will use up all the available inodes with directories rather than
data.

\paragraph{Procedure}

To test the effects of different object storage schemes, we created a new
\SI{100}{\mebi\byte} partition on a test computer, and then generated a series
of pseudo-random files of \SI{4}{\kibi\byte} each until the disk was reported
full. For each file, we would give it a pseudo-random name that resembled an
SHA-1 hash, and store it according to the object storage scheme under test. We
increased a counter each time we created a file, and another each time we
created a new directory.

We followed the basic scheme of taking leading hex digits of the SHA-1 hash to
form directories. We varied the number of directories taken (depth) and the
number of hex digits per directory (see
\autoref{sample-directory-scheme-variations} for examples). We tried depths from
\num{0} to \num{6} and digits per directory from \num{0} to \num{16}, discarding
combinations that did not make sense, such as combinations involving \num{0} and
another number (which would all simply be undivided), or those that required
more than the \num{40} hex digits of a \num{160}-byte SHA-1 hash.

\begin{table}[h]
    \caption{Sample object store directory variations}
    \label{sample-directory-scheme-variations}
    \centering
    \begin{tabular}{l l l}
        Hex digits & Depth & Example \\
        \midrule
        0 & 0 & \lstinline{03d37679d1fab86e5286decd6cd2a94efcfe083f} \\
        1 & 1 & \lstinline{7/9332ca7ce9163f78e3c115a2173bd8fd853d286} \\
        1 & 3 & \lstinline{6/8/c/40e64f3e74e6ebefdcf2f5f30fb8da004792c} \\
        2 & 1 & \lstinline{9f/4ec22c3e0289b29eefefe4728dece14e67e6ac} \\
        2 & 2 & \lstinline{dd/52/bcccff156a179cdac0793ef049039372d8a1} \\
        3 & 1 & \lstinline{cc5/199084d70f7c5ba325a240e1927579ee24bb1} \\
        3 & 4 & \lstinline{472/e98/e88/0b1/c5905065c70cbe806361d32f6429} \\
        4 & 3 & \lstinline{1ed2/bd51/01fe/5b23763e8c76852739f59201280f} \\
    \end{tabular}
\end{table}

\paragraph{Environment}

Like the "many files" experiment, this was automated as a Python script and run
on one of the dedicated test computers used for that experiment (specs shown in
\autoref{test-machine-specs}). However, rather than spending hours to fill the
\SI{197}{\gibi\byte} partition used for the other experiments, this experiment
used a new \SI{100}{\mebi\byte} LVM partition.

%




\chapter{Results}

\section{VCS Scaling: File Size}

\begin{figure}[]
  \caption{Real time required to commit one file to an empty repository}
  \label{fig:plot-file-size--c1-time}
  \centering
    \includegraphics[]{plot-file-size--c1-time}
\end{figure}

\begin{figure}[]
  \caption{Commit 2}
  \label{fig:plot-file-size--c2-time}
  \centering
    \includegraphics[]{plot-file-size--c2-time}
\end{figure}

\begin{figure}[]
  \caption{CPU utilization while committing one file to an empty repository}
  \label{fig:plot-file-size--c1-cpu}
  \centering
    \includegraphics[]{plot-file-size--c1-cpu-a}
    \includegraphics[]{plot-file-size--c1-cpu-b}
\end{figure}

\begin{figure}[]
  \caption{Total repository size after committing, editing, and committing again}
  \label{fig:plot-file-size--repo-size}
  \centering
    \includegraphics[]{plot-file-size--repo-size}
\end{figure}

\autoref{fig:plot-file-size--c1-time} shows time required for
the initial commit, copying the file into a fresh empty repository.

\autoref{fig:plot-file-size--c1-time--detail-high-end} shows
detailed views of \autoref{fig:plot-file-size--c1-time}, zooming
in and using a linear scale.

\autoref{fig:plot-file-size--c1-cpu} shows the
CPU usage during the initial commit.

\autoref{fig:plot-file-size--repo-size} shows the total
repository size, including the original file, after committing once, editing
1/1024th of the file, and committing again.

\iffalse

\subsubsection{Observations}

\begin{itemize}

  \item Mercurial's commit algorithm requires that the entire file fit into RAM
    three time over. On our trial computers with 8GiB of ram, the largest file
    Mercurial could commit successfully was 2GiB. At larger sizes, the commit
    command would abort, roll back the commit in progress, and exit with an
    error status.

    This reflects the fact that Mercurial stores only changes to files. It must
    examine every file with a diff algorithm during the commit.

\end{itemize}

\towrite{rewrite observations below and place above}

\begin{itemize}

    \item Git initially stores a full copy of every revision and then has a
        separate garbage collection phase to compact stored data in the
        repository. So for a single commit, the operation uses 2x the disk space
        as the size of the file: one for the file itself, and one for the copy
        in the repository. And for a single commit and update, the operation
        requires 3x the disk space: one for the file itself, one for the initial
        copy in the repository, and one for the updated copy.

        Disk space usage reduces after garbage collection (in this case to
        2.001x), because git compacts and de-duplicates the data in its
        repository. However, it cannot be reliably used unless there is enough
        space to store the uncompressed version first. Therefore Git starts to
        fail the test with files about 1/3 of the size of the available disk
        space.

    \item When the file is too large to fit into RAM, Git's commit operation
        prints an error message and exits with an error code, but the operation
        still completes successfully.

    \item Starting at just under 1GiB, the Git garbage collection seems to fail
        silently. No errors are reported, but on the graph one can see that the
        garbage-collected disk space usage jumps from 2x to 3x, indicating that
        the garbage collection phase is not doing any compacting. This might
        have to do with RAM requirements, similar to Mercurial's commit
        operation.

    \item Mercurial stores only deltas of files. In a sense, it is doing Git's
        garbage collection phase during each commit. This means more efficient
        disk usage. But it places a strong limitation on file size: Mercurial
        commits fail unless it has 3x as much RAM available as the size of the
        file. We suspect this has to do with the way Mercurial calculates. On
        the 8GiB test machines, Mercurial could only store files up to 2GiB in
        size.

\end{itemize}

Performance observations:

\begin{itemize}

    \item After some initial overhead, performance increases linearly with size.
        This is to be expected, since the operations are IO bound, copying all
        data to the repository.

    \item Times for the Mercurial update are faster than for Git's update,
        because Mercurial's archive format only saves deltas to the file.

    \item Mercurial has more initial time overhead, this is probably due to the
        fact that it is written in Python and requires starting the Python
        interpreter each time. This overhead is only 50 or 100ms, and quickly
        becomes insignificant compared to the IO operation time.

    \item Mercurial has less initial space overhead than Git.

        \begin{itemize}
            \setlength{\itemsep}{0pt}
            \setlength{\parskip}{0pt}
            \setlength{\parsep}{0pt}
            \item Minimum Mercurial repository size: 80KiB.
            \item Minimum Git repository size: 16KiB.
        \end{itemize}

        This can be seen in the disk space graph in the way the Mercurial usage
        converges towards 2x faster than Git usage does. But again, this quickly
        becomes insignificant compared to the size of the file.

\end{itemize}

\fi

\section{VCS Scaling: Number of Files}

\subsubsection{Results}

\begin{figure}[p]
  \caption{Real time required to commit many 1KiB files to an empty repository}
  \label{fig:plot-num-files--c1-time}
  \centering
    \includegraphics[]{plot-num-files--c1-time}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to commit many 1KiB files to an empty repository
  (detail)}
  \label{fig:plot-num-files--c1-time-detail}
  \centering
    \includegraphics[]{plot-num-files--c1-time-detail}
\end{figure}

\begin{figure}[p]
  \caption{CPU utilization while committing many 1KiB files to an empty
  repository}
  \label{fig:plot-num-files--c1-cpu}
  \centering
    \includegraphics[]{plot-num-files--c1-cpu-a}
    \includegraphics[]{plot-num-files--c1-cpu-b}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to check the status of many 1KiB files after
  initial commit}
  \label{fig:plot-num-files--stat1-time}
  \centering
    \includegraphics[]{plot-num-files--stat1-time}
\end{figure}

\begin{figure}[p]
  \caption{Total repository size after committing, editing, and committing again}
  \label{fig:plot-num-files--repo-size}
  \centering
    \includegraphics[]{plot-num-files--repo-size}
\end{figure}

\autoref{fig:plot-num-files--c1-time} shows the time
required for the initial commit, copying all files into a fresh empty
repository.

\autoref{fig:plot-num-files--c1-cpu} shows CPU utilization
during the commit.

\autoref{fig:plot-num-files--stat1-time} shows the time
required to check the changed status of all files just after committing.

\autoref{fig:plot-num-files--repo-size} shows the total
repository size, including the original files, after committing once, editing
1/1024th of every sixteenth file, and committing again.


\iffalse

We performed a test where increasingly large sets of files were committed to the
different version control repositories. The procedure was as follows:

\begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item Initialize an empty repository
    \item Generate a test file set of the given size. Each file is 1KiB of
        random binary data
    \item Commit the file set
    \item Check the status of the files
    \item Overwrite a small part of some of the files (1/1024th of the data in
        1/16 files)
    \item Check the status of the files again
    \item Commit the file set again
\end{enumerate}

Unlike the test with a single large file, the numerous small files did not
quickly hit error-causing disk space or RAM limitations. The version control
systems happily crunched the data as test times grew into hours.

Observations:

\begin{itemize}

    \item Again, after some initial overhead, commit and times increase
        linearly. However, Git's initial commit times actually decrease at
        certain points (128Ki, 1.5Mi, and 2Mi files). We are not sure how to
        explain this.

    \item Git in general is faster then Mercurial up to about half a million
        files. At 512Ki files is Mercurial and Git are about neck and neck. At
        768Ki and over, Mercurial is faster.

    \item Status check times are more erratic, though still increasing linearly
        overall. The variations may have to do with the output of the status
        commands and whether our terminal multiplexer was focused on the
        execution at the time. The status commands print one status line per
        file changed, which can be significant when hundreds of thousands of
        files involved. This output is significantly slower when the terminal
        multiplexer we used to monitor the experiments is connected, because it
        sends every line over the network to the monitoring machine.

    \item Mercurial update status is consistently slower than initial status,
        often by about 2-3x.

    \item Both Git and Mercurial converge to a little over 8x the space
        required. This probably has more to do with the filesystem block size
        than anything else. The underlying file system uses a 4KiB block size,
        so each 1KiB file will still use 4KiB of disk space. And since there are
        two copies of each file, that's 8KiB total for each 1KiB file, 8x the
        disk space.

    \item Mercurial converges towards the 8x limit faster though. We guess this
        is because of lower repo overhead, and also because Git is creating tree
        objects for each of the subdirectories in the file set. These files will
        be small, but each will take up another 4KiB block on the disk.

    \item Mercurial commits began to abort with disk space errors at 8Mi files,
        8GiB of data. This was surprising. Even at 8x disk usage, that should
        only be 64GiB of disk usage, well below the 192GiB free on the test
        disk.

\end{itemize}

\fi


\section{Performance Tuning}


\subsection{Rolling Hash Chunk Size}\label{rolling-hash-exp}

\subsubsection{Methodology}

This experiment is written as a unit test in the prototype Rust code. The test
is found in the \texttt{rolling\_hash} module, under the name
\texttt{chunk\_size\_experiment}.

\begin{itemize}

  \item Generate random data and pass it through the rolling hash algorithm with
    the given window size and match parameters.

  \item Continue generating data until the rolling hash has flagged 100 chunks.

  \item Calculate the mean chunk size and standard deviation.

\end{itemize}

\subsubsection{Results}

\begin{figure}[p]
  \caption{Mean chunk size}
  \label{fig:plot-rolling-hash}
  \centering
    \includegraphics[]{plot-rolling-hash}
\end{figure}

\autoref{fig:plot-rolling-hash} shows the mean chunk size.

\subsubsection{Observations}

\begin{itemize}

  \item The mean chunk size is approximately the sum of the window size and
    match parameter.

  \item When the match parameter is less than the window size, the standard
    deviation is approximately the match parameter.

\end{itemize}


\subsection{Directory Structure}

\subsubsection{Methodology}

\begin{itemize}

  \item Create an empty 100MiB partition.

  \item Write 4KiB objects according to the given object directory scheme, until
    the disk is reported as full.

  \item Track the total number of files written and total number of directories
    created.

\end{itemize}

\subsubsection{Results}

\begin{figure}[p]
  \caption{Overwhelmed by subdirectories}
  \label{fig:plot-filesystem-limits--directory-takeover}
  \centering
    \includegraphics[]{plot-filesystem-limits--directory-takeover}
\end{figure}

\autoref{fig:plot-filesystem-limits--directory-takeover} shows directories
overtaking files as nesting goes deeper.

\subsubsection{Observations}

\begin{itemize}

  \item In hindsight, it should have been easy to see that the number of
    directories would grow exponentially.

\end{itemize}
