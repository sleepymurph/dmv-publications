\chapter{Introduction}

It is still strangely difficult to keep backups and synchronize data. Many of us
have several computers, perhaps a laptop, a phone, and a work computer, and we
would like to synchronize data between them. We want to keep a Word document
synchronized between home and work. We want to put new music on our phones, and
pull photos off of camera SD cards. We have backups on removable drives, but we
don't remember when it was that we last did a backup, or what is new since then.
We have these sets of files that tend to fragment themselves across our devices,
and we lose track of what is where.

Cloud computing promises to centralize and safeguard our data, keeping it all in
one place and taking care of the backups for us. Google Drive gives us a shared
document that many people can edit in real time. Spotify offers endless music
streams. Instagram lets us save and share photos. Dropbox gives us a centralized
cloud filesystem. But many of these solutions are specialized applications for
specific media, which can limit their general usefulness. Most require constant
network connectivity, making them ill-suited for intermittent or high-latency
connections. And all require entrusting your data to a third-party service,
which raises concerns about privacy and storage longevity.

Why can't we simply track the files we have across the devices we have?

\tweak{\glsunset{VCS}} % Don't emphasize regular VCS just after introducing DVCS

Software developers have an excellent system for backup and sync right at their
fingertips: \glspl{DVCS}, such as Git and Mercurial. \Acrfullpl{VCS} track all
changes to a collection of files, allowing collaborators to work independently
and then synchronize and share their work. Additionally, in a \gls{DVCS}, every
collaborator has a full copy of the project's history. That redundancy not only
allows collaborators to work offline, but it also functions as a backup. Linus
Torvalds, the creator of Linux and Git, once famously joked that he doesn't keep
backups, he simply publishes his work on the internet and lets others copy
it~\cite{linus_no_backups}.

\glspl{DVCS} are designed primarily to store program source code: plain text
files in the range of tens of kilobytes. Checking in larger binary files such as
images, sound, or video affects performance. Actions that require copying data
in and out of the system slow from hundredths of a second to full seconds or
minutes. And since a \gls{DVCS} keeps every version of every file in every
\gls{repository}, forever, the disk space needs compound.

This has lead to a conventional wisdom that binary files should never be stored
in version control, inspiring blog posts with titles such as "Don't ever commit
binary files to Git! Or what to do if you
do"~\cite{dont_ever_commit_binaries_to_version_control}, even as the modern
software development practice of continuous delivery was commanding teams to
"keep absolutely everything in version
control."~\cite[p.33]{continuousdeliverybook}

\blockcquote[p.33]{continuousdeliverybook}{
    Every single artifact related to the creation of your software should be
    under version control. Developers should use it for source code, of course,
    but also for tests, database scripts, build and deployment scripts,
    documentation, libraries and configuration files for your application, your
    compiler and collection of tools, and so on --- so that a new member of your
    team can start working from scratch.
}

What if we could generalize the \glsdisp{DVCS}{distributed version control}
concept to store a wider variety of file sizes, from kilobyte text files to
multi-gigabyte videos? In addition, what if we relaxed the assumption that every
replica contain the complete history, and allowed each replica to choose what
subsets of the files and the history to store, according to the replica's
capacity and need? The answer could be a new abstraction for tracking a data set
and its history as a cohesive whole, even when it is physically spread over many
different nodes.

%



\section{CAP Theorem and the Importance of Availability}

\glsreset{ACID} % ACID spelled out here

Traditional databases that operated on one powerful server could focus on data
integrity and the \acrshort{ACID} guarantees: \acrlong{ACID}~\cite[Chapter
1]{nosqldistilled}. As demand increased, the server would be scaled up, beefing
  up the server hardware with more disk space, more RAM, and more and faster
  CPUs~\cite[Chapter 4]{nosqldistilled}. But there is a limit to scaling up the
  hardware of a single machine, and as data kept growing, a new wave of systems
  appeared that scaled \emph{out} to many commodity servers, distributed
  systems~\cite[Chapter 4]{nosqldistilled}.

\glsunset{ACID} % Used acrshort and acrlong, which don't unset the 'first' bit

Spreading data across different computers creates new problems of
synchronization. How does one ensure that replicated data is updated correctly
on all replicas? How can separate machines agree on the order of updates?

Distributed systems are ruled by the \gls{CAP-theorem}~\cite{cap_origin}, which
states that a system cannot be completely consistent (C), available (A), and
tolerant of network partitions (P) all at the same time. When communication
between replicas breaks down and they cannot all acknowledge an operation, the
system is faced with "the \gls{partitiondecision}: block the operation and thus
decrease availability, or proceed and thus risk
inconsistency."~\cite{cap_years_later}

Much research is aimed at improving consistency. Vector
clocks~\cite{lamport_ordering} and consensus algorithms such as
Paxos~\cite{paxos_made_simple,paxos_made_moderately_complex} make sure the same
updates are applied in the same order on all replicas even, if a minority of
nodes cannot respond. There are also data types are cleverly designed to be
commutative, so that the resulting data will be the same regardless of the order
in which updates are applied~\cite{crdt_orig}. But in general, when systems
cannot communicate, the CAP theorem cannot be avoided~\cite{cap_proof}, and the
system is still faced with the \gls{partitiondecision}.

Amazon's Dynamo~\cite{dynamo} was a pioneer in relaxing consistency guarantees
to ensure availability. Dynamo is a key-value store that can accept updates to a
value even if not all replicas respond. This can lead to inconsistencies, but
for a global e-commerce website like Amazon, any outage represents lost revenue
and so availability is paramount. Dynamo's answer to the \gls{partitiondecision}
is to always proceed.

When multiple Dynamo replicas receive updates to the same value and the order of
those updates cannot be determined, Dynamo keeps the different versions of the
value and presents them together during a read. That way, the higher-level
application that is using Dynamo as a data store can resolve the conflict and
write a new, reconciled value. Dynamo recognizes the
\gls{endtoendargument}~\cite{endtoendargument} that conflicting updates cannot
be resolved generally by a storage platform or network protocol. Resolution is
dependent on the structure of the data and on the needs of the application using
it.

Though maybe not designed with the CAP theorem explicitly in mind, a \gls{DVCS}
is in fact a small-scale distributed system that takes the availability-first
approach to the extreme. Rather than a set of connected nodes that may
occasionally lose contact in a network partition, a \gls{DVCS}'s
\glspl{repository} are self-contained and offline by default. They allow writes
to local data at any time, and only connect to other \glspl{repository}
intermittently by user command to exchange updates. Concurrent updates are not
only allowed but embraced as different \glspl{branch} of development. A
\gls{DVCS} can track many different \glspl{branch} at the same time, and
conflicting \glspl{branch} can be combined and resolved by the user in a
\gls{merge} operation.

The \glsdisp{DVCS}{distributed version control} concept may have something to
teach larger-scale systems about availability.

%



\section{Version Control, Git, and the DAG}

The first \acrlongpl{VCS} were \glspl{SCM} created as a way to efficiently store
different versions of a source code files by encoding them as a series of
deltas~\cite{history_of_version_control}. CVS introduced a collaborative
client-server model~\cite{history_of_version_control,cvs_book}. Subversion kept
the client-server model but began focusing on the versions of the whole
collection of files together, rather than on individual
files~\cite{history_of_version_control,svnbook}. Doing so made
\glsdisp{branch}{branching} easier, and \glsdisp{branch}{branching} quickly
became a key feature for collaborative work.

BitKeeper, a commercial \gls{SCM}, pioneered the \glsdisp{DVCS}{distributed
version control} concept by giving each developer a local copy of the whole
\gls{repository}, allowing local writes, and then making it easy to push the
local changes to a central
server~\cite{history_of_version_control,git_10_years_interview}. BitKeeper was
for a time the main \acrlong{SCM} for the Linux kernel, but licensing issues
caused a rift between Linux developers and BitMover, the company behind
BitKeeper. Unsatisfied with the other \glspl{SCM} available at the time, Linus
Torvalds, the creator of Linux, wrote his own,
Git~\cite{history_of_version_control,git_10_years_interview}.

The ability to always write locally separates concerns such as handling security
from the underlying problem of storing the data. \glspl{repository} are private
by default unless they are specifically hosted on a public server, any user can
write to their own \glspl{repository}, and any ad-hoc group of users can
exchange updates. Each developer can decide what updates they want to
incorporate into their particular \gls{repository}, and the group of developers
can decide which \glspl{repository} and which versions are official and what is
to be included into official releases. These are human questions that groups of
collaborators can solve by arranging their networks and policies how they see
fit, enabled by the tool rather than constrained by it. As Torvalds put it:

\blockcquote{git_10_years_interview}{The big thing about distributed source
    control is that it makes one of the main issues with \acrshortpl{SCM} go
    away - the politics around \enquote{who can make changes.} BK
    \textins{BitKeeper} showed that you can avoid that by just giving everybody
their own source \gls{repository}. }

Since Git's release, it has quickly become the dominant \gls{VCS} in
use~\cite{what_are_devs_talking_about}, making \glsdisp{DVCS}{distributed
version control} the dominant paradigm for \glsdisp{SCM}{source code
management}.

%


\subsection{How Data is Stored in Git}
\label{how-data-stored-in-git}

One of the key aspects of Git is that all data --- files, directories, and
history-- is stored according to its content; it is
\gls{contentaddressablestorage}.

When Git stores a file, it creates a \gls{blob} by copying the file's content
and prepending a short header. Git then calculates the SHA-1 hash of the
\gls{blob}, and stores the \gls{blob} in an \gls{objectstore}, using the SHA-1
hash as the \gls{blob}'s ID. To store a directory, Git creates a \gls{tree}
object that maps filenames to SHA-1 \gls{blob} IDs for each file in the
directory. This \gls{tree} object is also stored in the \gls{objectstore} with
it's SHA-1 hash as its ID. \Gls{tree} objects can refer not only to
\glspl{blob}, but to other \glspl{tree}, representing subdirectories.

Thus, a file hierarchy in a given state is represented by a hash tree, with
\gls{tree} objects as nodes and \glspl{blob} as leaves, and the entire state can
be referred to by a single hash ID, that of the top-level \gls{tree} object. A
simple example is shown in \autoref{dia_git_dag_example_simple_tree}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{dia_git_dag_example_simple_tree}
    \caption{A simple file hierarchy represented by Git tree and blob objects
    and their SHA-1 hash IDs}
    \label{dia_git_dag_example_simple_tree}
\end{figure}

Git then links different file hierarchy states with \gls{commit} objects. A
\gls{commit} object includes a hash ID for a \gls{tree}, representing the state
of the file hierarchy, and one or more hash IDs of parent \glspl{commit},
representing the previous states that this one was built from. Like with
\glspl{blob} and \glspl{tree}, the \gls{commit} object is also hashed and stored
with the hash as its ID.

The resulting data structure is a directed graph, with new \glspl{commit}
pointing to previous \glspl{commit}, and with each \gls{commit} pointing to a
\gls{tree} that represents the state of the file set at the time. This graph is
append-only: objects are immutable and referenced by cryptographic hash of their
content, which includes the hashes of all objects that they depend on. New
objects can only refer to existing objects, which makes the graph acyclic.
Storing history in this way will naturally de-duplicate unchanged files and
directories, because the resulting \glspl{blob} and \glspl{tree} will be
identical in content and thus have the same hash ID. This
\glsdisp{DAG}{directed, acyclic graph} structure is referred to as the
\glsreset{DAG}\acrshort{DAG}. It can be uniquely identified by the hash IDs of
those \glspl{commit} which do not yet have child \glspl{commit} to refer to
them, the \glspl{head} of each current \gls{branch} of development. A simple
example is shown in \autoref{dia_git_dag_example_three_commits}.

\begin{figure}[p]
    \centering
    \includegraphics[width=0.9\textwidth]{dia_git_dag_example_three_commits}
    \caption{A simple Git DAG with three commits}
    \label{dia_git_dag_example_three_commits}
\end{figure}

\begin{figure}[p]
    \centering
        \includegraphics[width=0.4\textwidth]{dia_git_dag_types}
    \caption{Git DAG Object Types}
    \label{dia_git_dag_types}
\end{figure}

A Git \gls{repository}, then, is a collection of the \gls{blob}, \gls{tree}, and
\gls{commit} objects that make up the file set's history, stored by hash ID in
the \gls{objectstore}, with \glspl{ref} to the \glspl{commit} at the \gls{head}
of each \gls{branch}~\cite{git_initial_readme}. These object types and their
relationships are shown in \autoref{dia_git_dag_types}.

%

\subsection{The Power of the DAG}

Such a \gls{DAG} has many properties that make it useful for distributed
collaborative work and for long-term data storage.

\begin{description}

    \item[De-duplication] As noted above, unchanged and duplicate files are
        naturally de-duplicated by the content addressing, if two files are
        identical, they will have the same hash and thus be the same \gls{blob}.

    \item[Distributability and Availability] Because the \gls{DAG} is immutable
        and append-only, it can be replicated simply by copying all of its
        objects. Any replica can make its own updates by appending to the
        \gls{DAG}. Rather than have a centralized notion of a "current" version,
        development in Git naturally diverges into different \glspl{branch}, as
        different users with their own replica of the \gls{DAG} make changes.
        \Glspl{branch} created on one replica can be synchronized to another
        simply by comparing sets of objects and transferring new objects that
        that the other does not have. \Glspl{branch} are reconciled with a
        \gls{merge} operation, creating a new \gls{commit} that incorporates
        changes from both \glspl{branch}.

    \item[Atomicity] The \gls{DAG}'s append-only nature makes \glspl{commit}
        atomic. Objects are added to the \gls{objectstore} frst, and then once
        all necessary objects are stored, the \gls{ref} can be updated to point
        to the new \gls{commit} object. The \gls{ref} is the size of the hash
        digest (in Git's case, \num{160} bytes for an SHA-1 hash), so it can be
        updated atomically. If the \gls{ref} is updated, the \gls{commit} was
        successful. The objects themselves do not have to be added to the
        \gls{objectstore} atomically because their presence does not change the
        existing \gls{DAG}. An interrupted object transfer may leave orphaned
        objects in the \gls{objectstore}, but it cannot corrupt
        previously-written data, nor can it leave the \gls{repository} in an
        inconsistent state. Orphaned objects can be swept up during a
        garbage-collection phase, walking the \gls{DAG} and marking all objects
        that are reachable from the current \glspl{ref}.

    \item[Verifiability] Perhaps most importantly, since objects are identified
        by a cryptographic hash, data integrity can be verified at any time by
        re-computing an object's hash and comparing it to its ID. Corrupt
        objects can be replaced with an intact copy from another replica.

\end{description}

The main weakness of Git's \gls{DAG} is that \glspl{blob} and files are one and
the same. This makes the file the unit of de-duplication, which can lead to
inefficient storage of larger files. Git gets around this by
\glsdisp{packfile}{packing} objects during its garbage-collection phase, storing
similar objects as bases and deltas behind the scenes. But this is an
optimization.

Calculating deltas for during this \glsdisp{packfile}{packing} phase requires
loading the objects into memory, and so it can cause an out-of-memory error if
an object is too large to fit into available RAM. Because Git stores files whole
in \glspl{blob}, it cannot \glsdisp{packfile}{pack} objects that are larger than
available RAM.

If the \gls{DAG} operated at a granularity smaller than the file, it could
become even more powerful. It could naturally de-duplicate chunks of files the
way that Git already de-duplicates whole files, and it could ensure that all
objects fit into RAM for \glsdisp{packfile}{packing} or other operations.

This sub-file granularity and de-duplication is the core idea behind our new
data storage system, \acrlong{DMV}.
