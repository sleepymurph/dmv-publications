\chapter{Introduction}

It is still strangely difficult to keep backups and synchronize data. Many of us
have several computers, perhaps a laptop, a phone, and a work computer, and we
would like to synchronize data between them. We want to keep a Word document
synchronized between home and work. We want to put new music on our phones, and
pull photos off of camera SD cards. We have backups on removable drives, but we
don't remember when it was that we last did a backup, or what is new since then.
We have these sets of files that tend to fragment themselves across our devices,
and we lose track of what is where.

Cloud computing offers to centralize and safeguard our data, keeping it all in
one place and taking care of the backups for us. Google Drive gives us a shared
document that many people can edit in real time. Spotify offers endless music
streams. Instagram lets us save and share photos. DropBox gives us a centralized
cloud filesystem. But many of these solutions are specialized applications for
specific media, which can limit their general usefulness; most require constant
network connectivity, making them ill-suited for intermittent or high-latency
connections; and all require entrusting your data to a third-party service,
which raises concerns about privacy and storage longevity.

Why can't we simply track the files we have across the devices we have?

Software developers have an excellent system for backup and sync right at their
fingertips: \newterm{distributed version control systems (DVCSs)}, such as Git
and Mercurial. Version control systems track all changes to a collection of
files, allowing collaborators to work independently and then synchronize and
share their work. Additionally, in a distributed version control system, every
collaborator has a full copy of the project's history. That redundancy not only
allows collaborators to work offline, but it also functions as a backup. Linus
Torvalds, the creator of Linux and Git, once famously joked that he doesn't keep
backups, he simply publishes his work on the internet and lets others copy it
\cite{linus_no_backups}.

The major limitation of distributed version control systems is that they are
designed to store program source code: plain text files in the range of tens of
kilobytes. With larger media files, they often become sluggish or wasteful of
disk space \perotto{How sluggish, how wasteful? "Many" is too vague}. Many a
web-design team has come to regret checking their graphical assets into version
control\perotto{citation}. In addition, many will have trouble scaling up
\perotto{with regards to what} as the number of files increases or the history
grows increasingly long.

What if we could generalize the distributed version control concept to store a
wide variety of file sizes, from kilobyte text files to multi-gigabyte videos?
In addition, what if we relaxed the assumption that every replica contain the
complete history, and allowed each replica to choose what subsets of the files
and the history to store, according to the replica's capacity and need? The
answer could be a new abstraction for tracking a data set and its history as a
cohesive whole, even when it is physically spread over many different nodes.

%



\section{CAP Theorem and the Importance of Availability}

Traditional databases that operated on one powerful server could focus on data
integrity and the \newterm{ACID} guarantees: Atomicity, Consistency, Isolation,
and Durability. As demand increased, the server would be scaled up, beefing up
the server hardware with more disk space, more RAM, and more and faster CPUs.
But there is a limit to scaling up the hardware of a single machine, and as data
kept growing, a new wave of systems appeared that scaled \emph{out} to many
commodity servers, distributed systems.
\perotto{refs for this para}

Spreading data across different computers creates new problems of
synchronization. How does one ensure that replicated data is updated correctly
on all replicas? How can separate machines agree on the order of updates?

Distributed systems are ruled by the \newterm{CAP-theorem} \cite{cap_origin},
which states that a system cannot be completely consistent (\newterm{C}),
available (\newterm{A}), and tolerant of network partitions (\newterm{P}) all at
the same time. When communication between replicas breaks down and they cannot
all acknowledge an operation, the system is faced with \newterm{the partition
decision}: block the operation and decrease availability, or proceed and risk
inconsistency\cite{cap_years_later}.

Much research is aimed at improving consistency. Vector
clocks\cite{lamport_ordering} and consensus algorithms such as
Paxos\cite{paxos_made_simple,paxos_made_moderately_complex} make sure the same
updates are applied in the same order on all replicas even, if a minority of
nodes cannot respond. There are also data types are cleverly designed to be
commutative, so that the resulting data will be the same regardless of the order
in which updates are applied\cite{crdt_orig}. But in general, when systems
cannot communicate, the CAP theorem cannot be avoided\cite{cap_proof}, and the
system is still faced with the partition decision.

Amazon's Dynamo\cite{dynamo} was a pioneer in relaxing consistency guarantees to
ensure availability. Dynamo is a key-value store that can accept updates to a
value even if not all replicas respond. This can lead to inconsistencies, but
for a global shopping website like Amazon, any outage represents lost revenue
and so availability is more important. Dynamo's answer to the partition decision
is to always proceed.

When multiple Dynamo replicas receive updates to the same value and the order of
those updates cannot be determined, Dynamo keeps the different versions of the
value and presents them together during a read. That way, the higher-level
application that is using Dynamo as a data store can resolve the conflict and
write a new, reconciled value. Dynamo recognizes the end-to-end
argument\cite{endtoendargument}, that conflicting updates cannot be resolved
generally by a storage platform or network protocol. Resolution is dependent on
the structure of the data and on the needs of the application using it.

Though maybe not designed with the CAP theorem explicitly in mind, a distributed
version control system can be thought of as a small-scale distributed system
that takes the availability-first approach to the extreme. Rather than a set of
connected nodes that may occasionally lose contact in a network partition, a
distributed version control system's repositories are self-contained, allowing
writes to local data at any time, and only connecting to other repositories
intermittently by user command to exchange updates. Concurrent updates are not
only allowed but embraced as different \newterm{branches} of development. A
distributed version control system can track many different branches at the same
time, and conflicting branches can be combined and resolved by the user.

The distributed version control concept may have something to teach larger-scale
systems about availability.

%



\section{Version Control, Git, and the DAG}

\askottoinline{Most of the claims Otto asks me to cite here come from
\cite{history_of_version_control}. How to indicate that?}

Version control started as a way to efficiently store different versions of a
source code files by encoding them as a series of deltas
\cite{history_of_version_control}. CVS introduced a collaborative client-server
model\perotto{cite}. Subversion kept the client-server model but began focusing on the
versions of the whole collection of files together, rather than individual
files\perotto{cite}. Doing so made branching easier, and branching quickly became a key
feature for collaborative work.

BitKeeper, a commercial source code manager, pioneered the distributed version
control concept by giving each developer a local copy of the whole repository,
allowing local writes, and then making it easy to push the local changes to a
central server\perotto{cite}. BitKeeper was for a time the main source code manager for the
Linux kernel, but licensing issues caused a rift between Linux developers and
BitMover, the company behind BitKeeper\perotto{cite}. Unsatisfied with the other source code
managers available at the time, Linus Torvalds, the creator of Linux, wrote his
own, Git.

The ability to always write locally separates concerns such as handling security from the
underlying problem of storing the data. Repositories are private by default
unless they are specifically hosted on a public server, any user can write to
their own repositories, and any ad-hoc group of users can exchange updates. Each
developer can decide what updates they want to incorporate into their particular
repository, and the group of developers can decide which repositories and which
versions are official and what is to be included into official releases. These
are human questions that groups of collaborators can solve by arranging their
networks and policies how they see fit, enabled by the tool rather than
constrained by it. As Torvalds put it:

\blockcquote{git_10_years_interview}{The big thing about distributed source
    control is that it makes one of the main issues with SCM's go away - the
    politics around \enquote{who can make changes.} BK \textins{BitKeeper}
    showed that you can avoid that by just giving everybody their own source
repository. }

\towrite{key: pull rather than push}

Since the release of Git, distributed version control has become the dominant
paradigm for source code control. Git itself is the dominant version control
system in use today, followed by Mercurial, another distributed version control
system that has its roots in Subversion\todo{find citation}.

While Git and Mercurial both have a similar data model, with committed versions
representing states of the whole collection of files, Mercurial's actual storage
implementation is rooted in the per-file delta encoding of previous systems.
Git's storage implementation is more directly related to the data model.
\perotto{Say 1-2 sentences more about this or just say as expanded on below}
\askotto{I thought this was a nice segue}

%


\subsection{How Data is Stored in Git}

One of the key aspects of Git is that all data-- files, directories, and
history-- is stored according to its content; it is \newterm{content-addressable
storage}.

When Git stores a file, it creates a \gls{blob} by copying the file's content
and prepending a short header. Git then calculates the SHA-1 hash of the
\gls{blob}, and stores the \gls{blob} in an \newterm{object store}, using the
SHA-1 hash as the \gls{blob}'s ID. To store a directory, Git creates a
\gls{tree} object that maps filenames to SHA-1 \gls{blob} IDs for each file
in the directory. This \gls{tree} object is also stored in the database with it's
SHA-1 hash as its ID. \Gls{tree} objects can refer not only to \gls{blob}, but to
other \glspl{tree}, representing subdirectories.

Thus, a file hierarchy in a given state is represented by a hash tree, with
\gls{tree} objects as nodes and \glspl{blob} as leaves, and the entire state can
be referred to by a single hash ID, that of the top-level \gls{tree} object. A
simple example is shown in \autoref{dia_git_dag_example_simple_tree}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{dia_git_dag_example_simple_tree}
    \caption{A simple file hierarchy represented by Git tree and blob objects
    and their SHA-1 hash IDs}
    \label{dia_git_dag_example_simple_tree}
\end{figure}

Git then links different file hierarchy states with \gls{commit} objects. A
\gls{commit} object includes a hash ID for a \gls{tree}, representing the state
of the file hierarchy, and one or more hash IDs of parent \glspl{commit},
representing the previous states that this one was built from. Like with
\glspl{blob} and \glspl{tree}, the \gls{commit} object is also hashed and stored
with the hash as its ID.

The resulting data structure is a directed graph, with new \glspl{commit}
pointing to previous \glspl{commit}, and with each \gls{commit} pointing to a
\gls{tree} that represents the state of the file set at the time. This graph is
append-only: objects are immutable and referenced by cryptographic hash of their
content, which includes the hashes of all objects that they depend on. New
objects can only refer to existing objects, which makes the graph acyclic.
Storing history in this way will naturally de-duplicate unchanged files and
directories, because the resulting \glspl{blob} and \glspl{tree} will be
identical in content and thus have the same hash ID. The \acrlong{DAG} is
referred to as the \acrshort{DAG}. It can be uniquely identified by the hash IDs of
those \glspl{commit} which do not yet have child \glspl{commit} to refer to
them, the \newterm{heads} of each current branch of development. A simple
example is shown in \autoref{dia_git_dag_example_three_commits}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{dia_git_dag_example_three_commits}
    \caption{A simple Git DAG with three commits}
    \label{dia_git_dag_example_three_commits}
\end{figure}

A Git repository, then, is a collection of the \gls{blob}, \gls{tree}, and
\gls{commit} objects that make up the file set's history, stored by hash ID in
the object store, with references (\newterm{refs}) to the hash IDs of the
current head \glspl{commit}\cite{git_initial_readme}.

%

\subsection{The Power of the DAG}

Such a \gls{DAG} has many properties that make it useful for distributed
collaborative work and for long-term data storage.

\begin{description}

    \item[De-duplication] As noted above, unchanged and duplicate files are
        naturally de-duplicated by the content addressing, if two files are
        identical, they will have the same hash and thus be the same \gls{blob}.

    \item[Distributability and Availability] Because the \gls{DAG} is immutable
        and append-only, it can be replicated simply by copying all of its
        objects. Any replica can make its own updates by appending to the
        \gls{DAG}. Rather than have a centralized notion of "current" version,
        development in Git naturally diverges into different branches, as
        different users with their own replica of the \gls{DAG} make changes.
        Branches created on one replica can be synchronized to another simply by
        comparing sets of objects and transferring new objects that that the
        other does not have. Branches are reconciled with a \newterm{merge}
        operation, creating a new \gls{commit} that incorporates changes from
        both branches.

    \item[Atomicity] The \gls{DAG}'s append-only nature makes \glspl{commit}
        atomic. Objects are added to the object store, and then once all
        necessary objects are stored, the ref can be updated to point to the new
        \gls{commit} object. The ref is the size of the hash digest (in Git's
        case, \num{160} bytes for an SHA-1 hash), so it can be updated
        atomically. If the ref is updated, the \gls{commit} was successful. The
        objects do not have to be added to the object store atomically because
        their presence does not change the existing \gls{DAG}. An interrupted
        object transfer may leave orphaned objects in the object store, but it
        cannot corrupt previously-written data, nor can it leave the repository
        in an inconsistent state. Orphaned objects can be swept up during a
        garbage-collection phase, walking the \gls{DAG} and marking all objects
        that are reachable from the current refs.

    \item[Verifiability] Perhaps most importantly, since objects are identified
        by a cryptographic hash, data integrity can be verified at any time by
        re-computing an object's hash and comparing it to its ID. Corrupt
        objects can be replaced with an intact copy from another replica.

\end{description}

The main weakness of Git's \gls{DAG} is that \glspl{blob} and files are one and
the same. This makes the file the unit of de-duplication, which can lead to
inefficient storage of larger files. Git gets around this by "packing" objects
during its garbage-collection phase, storing similar objects as bases and deltas
behind the scenes. But this is an optimization.

Calculating deltas for during this packing phase requires loading the objects
into memory, and so it can cause an out-of-memory error if an object is too
large to fit into available RAM. Because Git stores files whole in \glspl{blob}, it
cannot pack files that are larger than available RAM.

If the \gls{DAG} operated at a granularity smaller than the file, it could
become even more powerful. It could naturally de-duplicate chunks of files the
way that Git already de-duplicates whole files, and it could ensure that all
objects fit into RAM for packing or other operations.

This sub-file granularity and de-duplication is the core idea behind our new
data storage system, \gls{DMV}.
