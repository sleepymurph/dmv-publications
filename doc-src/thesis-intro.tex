\chapter{Introduction}

It is still surprisingly difficult to keep backups and synchronize data. Many of
us have several computers, perhaps a laptop, a phone, and a work computer, and
we would like to synchronize data between them. We want to keep a Word document
synchronized between home and work. We want to put new music on our phones, and
pull photos off of camera SD cards. We have backups on removable drives, but we
don't remember when it was that we last did a backup, or what is new since then.
We have these sets of files that tend to fragment themselves across our devices,
and we lose track of what is where.

Cloud computing offers to centralize and safeguard our data, keeping it all in
one place and taking care of the backups for us. Google Drive gives us a shared
document that many people can edit in real time. Spotify offers endless music
streams. Instagram lets us save and share photos. DropBox gives us a folder that
syncs\todo{cite "folder that syncs"?}. But many of these solutions are
specialized applications for specific media, which can limit their general
usefulness; most require constant network connectivity, making them ill-suited
for intermittent or high-latency connections; and all require entrusting your
data to a third-party service, which raises concerns about privacy and storage
longevity. Why can't we simply track the files we have across the devices we
have?

% Origin of "folder that syncs"
% https://www.quora.com/Why-is-Dropbox-more-popular-than-other-programs-with-similar-functionality

Software programmers have an excellent system for backup and sync right at their
fingertips: \newterm{distributed version control systems (DVCSs)}, such as Git
and Mercurial. Version control systems track all changes to a collection of
files, allowing collaborators to work independently and then synchronize and
share their work. Additionally, in a distributed version control system, every
collaborator has a full copy of the project's history. That redundancy not only
allows collaborators to work offline, but it also functions as a backup. Linus
Torvalds, the creator of Linux and Git, once famously joked that he doesn't keep
backups, he simply publishes his work on the internet and lets others copy it
\cite{linus_no_backups}.

The major limitation distributed version control systems is that they are
designed to store program source code: plain text files in the range of tens of
kilobytes. They often have trouble with larger media files, becoming sluggish or
wasteful of disk space. Many a web-design team has come to regret checking their
graphical assets into version control. In addition, many will have trouble
scaling up as the number of files increases or the history grows increasingly
long.

What if we could generalize the distributed version control concept to store a
wide variety of file sizes, from kilobyte text files to multi-gigabyte videos?
In addition, what if we relaxed the assumption that every replica contain the
complete history, and allowed each replica to choose what subsets of the files
and the history to store, according to the replica's capacity and need? The
answer could be a new abstraction for tracking a data set and its history as a
cohesive whole, even when it is physically spread over many different nodes.

%



\section{CAP Theorem and the Importance of Availability}

Distributed systems are ruled by the \newterm{CAP-theorem} \cite{cap_origin},
which states that a system cannot be completely consistent (\newterm{C}),
available (\newterm{A}), and tolerant of network partitions (\newterm{P}) all at
the same time. One area must always suffer, and since network partitions are
always a possibility, a distributed system must make trade-offs between
availability and consistency.

\towrite{Traditional ACID}

\towrite{Modern NoSQL/BASE}

\towrite{New thinking (CAP years later)
   - partitions rare
   - partitions are just latency
   - "the partition decision": cancel and decrease availability, proceed and
   risk inconsistency
   - general problem of resolving conflicts is not solvable
   \cite{cap_years_later}
}

\towrite{DVCS is always available
    - Branches: No global concept of recent version
}

%



\section{Version Control, Git, and The DAG}

\towrite{Quick recap of history of version
control\cite{history_of_version_control}: CVS, SVN, Git}

\towrite{Version control checks files in and out to filesystem, does not require
special access or editing modes}

\towrite{Git grew out of Linus's frustration with other VCSs}

%

\subsection{How Data is Stored in Git}

One of the key aspects of Git is that data is stored according to its content. A
file is stored by prepending a short Git header and then taking the SHA-1 hash
of the resulting \newterm{blob} (binary large object). That SHA-1 hash becomes
the blob's ID. A directory is stored as a list of file names to each file's
SHA-1 blob ID. This list is called a \newterm{tree}, and its binary
representation is also hashed by SHA-1 to get a tree ID. Trees can refer not
only to blobs, but to other trees, representing subdirectories. Thus, a file
hierarchy in a given state is represented by a hash tree, with \newterm{tree}
objects as nodes and \newterm{blobs} as leaves, and the entire state can be
referred to by a single hash ID, that of the top-level tree object. A simple
example is shown in \autoref{dia_git_dag_example_simple_tree}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{dia_git_dag_example_simple_tree}
    \caption{A simple file hierarchy represented by Git \newterm{tree} and
        \newterm{blob} objects and their SHA-1 hash IDs}
    \label{dia_git_dag_example_simple_tree}
\end{figure}

Git then links different file hierarchy states with \newterm{commit} objects,
which include the hash ID of a tree plus the hash IDs of the commit (or multiple
commits) that represent the previous state of the file hierarchy. The resulting
data structure is a directed graph, with new commits pointing to previous
commits, and with each commit pointing to a tree that represents the state of
the file set at the time. This graph is append-only-- objects are immutable and
referenced by cryptographic hash, and new objects can only refer to existing
objects, which makes the graph acyclic. Storing history in this way will
naturally de-duplicate unchanged files and directories, because the resulting
blobs and trees will be the same and have the same hash ID. The directed acyclic
graph is referred to as the \newterm{DAG}. It can be referenced by the hash IDs
of those commits which do not yet have child commits to refer to them, the
\newterm{heads} of each current branch of development. A simple example is shown
in \autoref{dia_git_dag_example_three_commits}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{dia_git_dag_example_three_commits}
    \caption{A simple Git DAG with three commits}
    \label{dia_git_dag_example_three_commits}
\end{figure}

A Git repository, then, is a collection of the blob, tree, and commit objects
that make up the file set's history, stored by hash ID in an \newterm{object
store}, with references (\newterm{refs}) to the hash IDs of the current head
commits\cite{git_initial_readme}.

%

\subsection{The Power of the DAG}

Such a DAG has many properties that make it useful for long-term data storage.

\begin{description}

    \item[De-duplication] As noted above, unchanged and duplicate files are
        naturally de-duplicated by the content addressing, if two files are
        identical, they will have the same hash and thus be the same blob.

    \item[Distributability and Availability] Because the DAG is immutable and
        append-only, it can be replicated simply by copying all of its objects.
        Then any replica can make its own updates by appending to the DAG, and
        replicas can synchronize simply by exchanging new objects that the other
        does not have.

    \item[Atomicity] Commits are atomic, since all objects that they refer to
        have to be added first, then the commit object, then finally the ref is
        updated to point to the new commit. If the ref is updated, the commit
        was successful. An interrupted commit operation may leave orphaned
        objects in the object database, but it cannot corrupt previously-written
        data, nor can it leave the repository in an inconsistent state. Orphaned
        objects can be swept up during a garbage-collection phase, walking the
        DAG and marking all objects that are reachable from the current refs.

    \item[Verifiability] Perhaps most importantly, since objects are identified
        by a cryptographic hash, data integrity can be verified at any time by
        re-computing an object's hash and comparing it to its ID. Corrupt
        objects can be replaced with an intact copy from another replica.

\end{description}

The main weakness of Git's DAG is that blobs and files are one and the same.
This makes the file the unit of de-duplication, which can lead to inefficient
storage of larger files. Git gets around this by "packing" objects during its
garbage-collection phase, storing similar objects as bases and deltas behind the
scenes. But this is an optimization. There can also be trouble when a very large
file becomes a large object, breaking assumptions that algorithms might have
about fitting objects into memory or transferring objects across the network.

If the DAG operated at a granularity smaller than the file, it could become even
more powerful. It could naturally de-duplicate chunks of files the way that Git
already de-duplicates whole files, and it could ensure that all objects are of a
reasonable size. That is the core idea behind the our new distributed media
versioning system.
