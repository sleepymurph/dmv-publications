\chapter{Introduction}

It is still strangely difficult to keep backups and synchronize data. Many of us
have several computers, perhaps a laptop, a phone, and a work computer, and we
would like to synchronize data between them. We want to keep a Word document
synchronized between home and work. We want to put new music on our phones, and
pull photos off of camera SD cards. We have backups on removable drives, but we
don't remember when it was that we last did a backup, or what is new since then.
We have these sets of files that tend to fragment themselves across our devices,
and we lose track of what is where.

Cloud computing offers to centralize and safeguard our data, keeping it all in
one place and taking care of the backups for us. Google Drive gives us a shared
document that many people can edit in real time. Spotify offers endless music
streams. Instagram lets us save and share photos. DropBox gives us a centralized
cloud filesystem. But many of these solutions are specialized applications for
specific media, which can limit their general usefulness; most require constant
network connectivity, making them ill-suited for intermittent or high-latency
connections; and all require entrusting your data to a third-party service,
which raises concerns about privacy and storage longevity. Why can't we simply
track the files we have across the devices we have?

Software developers have an excellent system for backup and sync right at their
fingertips: \newterm{distributed version control systems (DVCSs)}, such as Git
and Mercurial. Version control systems track all changes to a collection of
files, allowing collaborators to work independently and then synchronize and
share their work. Additionally, in a distributed version control system, every
collaborator has a full copy of the project's history. That redundancy not only
allows collaborators to work offline, but it also functions as a backup. Linus
Torvalds, the creator of Linux and Git, once famously joked that he doesn't keep
backups, he simply publishes his work on the internet and lets others copy it
\cite{linus_no_backups}.

The major limitation distributed version control systems is that they are
designed to store program source code: plain text files in the range of tens of
kilobytes. They often have trouble with larger media files, becoming sluggish or
wasteful of disk space. Many a web-design team has come to regret checking their
graphical assets into version control. In addition, many will have trouble
scaling up as the number of files increases or the history grows increasingly
long.

What if we could generalize the distributed version control concept to store a
wide variety of file sizes, from kilobyte text files to multi-gigabyte videos?
In addition, what if we relaxed the assumption that every replica contain the
complete history, and allowed each replica to choose what subsets of the files
and the history to store, according to the replica's capacity and need? The
answer could be a new abstraction for tracking a data set and its history as a
cohesive whole, even when it is physically spread over many different nodes.

%



\section{CAP Theorem and the Importance of Availability}

Traditional databases that operated on one powerful server could focus on data
integrity and the \newterm{ACID} guarantees: Atomicity, Consistency, Isolation,
and Durability. As demand increased, the server would be scaled up, beefing up
the server hardware with more disk space, more RAM, and more and faster CPUs.
But there is a limit to this kind of scaling, and as data kept growing, a new
wave of systems appeared that scaled \emph{out} to many commodity servers,
distributed systems.

Spreading data across different computers creates new problems of
synchronization. How does one ensure that replicated data is updated correctly
on all replicas? How can separate machines agree on the order of updates?

Distributed systems are ruled by the \newterm{CAP-theorem} \cite{cap_origin},
which states that a system cannot be completely consistent (\newterm{C}),
available (\newterm{A}), and tolerant of network partitions (\newterm{P}) all at
the same time. When communication between replicas breaks down and they cannot
all acknowledge an operation, the system is faced with \newterm{the partition
decision}: block the operation and decrease availability, or proceed and risk
inconsistency\cite{cap_years_later}.

Much research is aimed at improving consistency. Vector
clocks\cite{lamport_ordering} and consensus algorithms such as
Paxos\cite{paxos_made_simple,paxos_made_moderately_complex} make sure the same
updates are applied in the same order on all replicas even if a minority cannot
respond. Clever data types are designed to be commutative, so that the resulting
data will be the same regardless of the order in which updates are
applied\cite{crdt_orig}. But in general, when systems cannot communicate, the
CAP theorem cannot be avoided\cite{cap_proof}, and the system is still faced
with the partition decision.

Amazon's Dynamo\cite{dynamo} was a pioneer in relaxing consistency guarantees to
ensure availability. Dynamo is a key-value store that can accept updates to to a
value even if not all replicas respond. This can lead to inconsistencies, where
different replicas have differing versions of a particular value. In such
situations, Dynamo keeps each version and presents them together during a read.
That way, the higher-level application that is using Dynamo as a data store can
resolve the conflict and write a new, reconciled value.

\towrite{DVCS is always available
    - Branches: No global concept of recent version
}

\towrite{Data locality}

%



\section{Version Control, Git, and The DAG}

\towrite{Quick recap of history of version
control\cite{history_of_version_control}: CVS, SVN, Git}

\towrite{Git grew out of Linus's frustration with other VCSs}

\towrite{DVCS in terms of distributed system and CAP theorem: partition by
default, local writes}

\towrite{Partitions not just the norm but the default}

\towrite{ "The big thing about distributed source control is that it makes one
of the main issues with SCM's go away - the politics around "who can make
changes." BK showed that you can avoid that by just giving everybody their own
source repository." }

\towrite{Ability to always write locally moves problems like security outside of
the system, separate from data storage. Access to repositories, which
repositories are considered official, follows network of developers and their
needs.}

\towrite{DVCS revolutionized how we develop and collaborate on software}

\towrite{Version control checks files in and out to filesystem, does not require
special access or editing modes}

%

\section{How Data is Stored in Git}

One of the key aspects of Git is that data is stored according to its content. A
file is stored by prepending a short Git header and then taking the SHA-1 hash
of the resulting \newterm{blob} (binary large object). That SHA-1 hash becomes
the blob's ID. A directory is stored as a list of file names to each file's
SHA-1 blob ID. This list is called a \newterm{tree}, and its binary
representation is also hashed by SHA-1 to get a tree ID. Trees can refer not
only to blobs, but to other trees, representing subdirectories. Thus, a file
hierarchy in a given state is represented by a hash tree, with \newterm{tree}
objects as nodes and \newterm{blobs} as leaves, and the entire state can be
referred to by a single hash ID, that of the top-level tree object. A simple
example is shown in \autoref{dia_git_dag_example_simple_tree}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{dia_git_dag_example_simple_tree}
    \caption{A simple file hierarchy represented by Git \newterm{tree} and
        \newterm{blob} objects and their SHA-1 hash IDs}
    \label{dia_git_dag_example_simple_tree}
\end{figure}

Git then links different file hierarchy states with \newterm{commit} objects,
which include the hash ID of a tree plus the hash IDs of the commit (or multiple
commits) that represent the previous state of the file hierarchy. The resulting
data structure is a directed graph, with new commits pointing to previous
commits, and with each commit pointing to a tree that represents the state of
the file set at the time. This graph is append-only-- objects are immutable and
referenced by cryptographic hash, and new objects can only refer to existing
objects, which makes the graph acyclic. Storing history in this way will
naturally de-duplicate unchanged files and directories, because the resulting
blobs and trees will be the same and have the same hash ID. The directed acyclic
graph is referred to as the \newterm{DAG}. It can be referenced by the hash IDs
of those commits which do not yet have child commits to refer to them, the
\newterm{heads} of each current branch of development. A simple example is shown
in \autoref{dia_git_dag_example_three_commits}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{dia_git_dag_example_three_commits}
    \caption{A simple Git DAG with three commits}
    \label{dia_git_dag_example_three_commits}
\end{figure}

A Git repository, then, is a collection of the blob, tree, and commit objects
that make up the file set's history, stored by hash ID in an \newterm{object
store}, with references (\newterm{refs}) to the hash IDs of the current head
commits\cite{git_initial_readme}.

%

\subsection{The Power of the DAG}

Such a DAG has many properties that make it useful for distributed collaborative
work and for long-term data storage.

\begin{description}

    \item[De-duplication] As noted above, unchanged and duplicate files are
        naturally de-duplicated by the content addressing, if two files are
        identical, they will have the same hash and thus be the same blob.

    \item[Distributability and Availability] Because the DAG is immutable and
        append-only, it can be replicated simply by copying all of its objects.
        Any replica can make its own updates by appending to the DAG. Rather
        than have a centralized notion of "current" version, development in Git
        naturally diverges into different branches, as different users with
        their own replica of the DAG make changes. Branches created on one
        replica can be synchronized to another simply by comparing sets of
        objects and transferring new objects that that the other does not have.
        Branches are reconciled with a \newterm{merge} operation, creating a new
        commit that incorporates changes from both branches.

    \item[Atomicity] Commits are atomic, since all objects that they refer to
        have to be added first, then the commit object, then finally the ref is
        updated to point to the new commit. If the ref is updated, the commit
        was successful. An interrupted commit operation may leave orphaned
        objects in the object database, but it cannot corrupt previously-written
        data, nor can it leave the repository in an inconsistent state. Orphaned
        objects can be swept up during a garbage-collection phase, walking the
        DAG and marking all objects that are reachable from the current refs.

    \item[Verifiability] Perhaps most importantly, since objects are identified
        by a cryptographic hash, data integrity can be verified at any time by
        re-computing an object's hash and comparing it to its ID. Corrupt
        objects can be replaced with an intact copy from another replica.

\end{description}

The main weakness of Git's DAG is that blobs and files are one and the same.
This makes the file the unit of de-duplication, which can lead to inefficient
storage of larger files. Git gets around this by "packing" objects during its
garbage-collection phase, storing similar objects as bases and deltas behind the
scenes. But this is an optimization. There can also be trouble when a very large
file becomes a large object, breaking assumptions that algorithms might have
about fitting objects into memory or transferring objects across the network.

If the DAG operated at a granularity smaller than the file, it could become even
more powerful. It could naturally de-duplicate chunks of files the way that Git
already de-duplicates whole files, and it could ensure that all objects are of a
reasonable size. That is the core idea behind our new distributed media
versioning system.
