\begin{table}[hp]
    \caption{Observations as file size increases}
    \label{file-sizes-table}
    \centering
    \begin{tabular}{r l}
        Size & Observation \\
        \midrule
        \SI{1.5}{\gibi\byte} & Largest successful commit with Mercurial \\
        \SI{2}{\gibi\byte} & Mercurial commit rejected \\
        \SI{8}{\gibi\byte} & Largest successful commit with Git \\
        \SI{12}{\gibi\byte} & Git false-alarm errors begin, but commit still intact \\
        \SI{16}{\gibi\byte} & Largest successful Git fsck command \\
        \SI{24}{\gibi\byte} & Git false-alarm errors begin during fsck, but commit still intact \\
        \SI{64}{\gibi\byte} & Largest successful DMV commit \\
        \SI{96}{\gibi\byte} & DMV timeout after \SI{5.5}{\hour} \\
        \SI{96}{\gibi\byte} & Last successful commit with Bup (and Git, ignoring false-alarm errors) \\
        \SI{128}{\gibi\byte} & All fail due to size of test partition \\
    \end{tabular}
\end{table}

\begin{table}[hp]
    \caption{Effective size limits for VCSs evaluated}
    \label{vcs-size-limits-table}
    \centering
    \begin{tabular}{l l}

        VCS & Effective limit \\
        \midrule

        Git & Commit intact at all sizes, UI reports errors at \SI{12}{\gibi\byte} and larger \\

        Mercurial & Commit rejected at \SI{2}{\gibi\byte} and larger \\

        Bup & Successful commits at all sizes tried, up to \SI{96}{\gibi\byte} \\

        DMV & Successful commits up to \SI{64}{\gibi\byte}, timeout at
        \SI{5.5}{\hour} during \SI{96}{\gibi\byte} trial

    \end{tabular}
\end{table}

\cleardoublepage

\section{Results: File Size}
\label{file-size-results}

\subsection{File Size Limits}
\label{file-size-limits-results}

Both Git and Mercurial had limits to the size of file they could store
successfully. With a \SI{2}{\gibi\byte} file, Mercurial would issue a warning,
saying "up to 6442 MB of RAM may be required to manage this file," but the
\gls{commit} would be stored successfully. With a file \SI{4}{\gibi\byte} or
larger, Mercurial's \gls{commit} operation would exit with an error message and
the \gls{commit} would not be stored. However, the repository would be left in a
consistent empty state. The atomicity of the \gls{commit} operation held.

Git's behavior was more erratic. Starting with a file \SI{12}{\gibi\byte} and
larger, Git's \gls{commit} operation would exit with an error code, reporting a
fatal out-of-memory error that \lstinline{malloc} failed to allocate
\SI{12}{\gibi\byte}. However, the \gls{commit} would be successfully stored with
no consistency errors in the repository reported by \lstinline{git fsck}.
Starting at \SI{24}{\gibi\byte}, the \gls{commit} operation would report the
same error, the \gls{commit} would still be written, but then the 
\lstinline{git fsck} integrity check would also exit with an error code.
However, the error it reported in its output was a similar "fatal" error as
Git's \lstinline{malloc} error as the \gls{commit} operation, and it did not
report any actual integrity errors in the repository.

So to check the \gls{commit}, we extracted the \SI{24}{\gibi\byte} file from the
repository and compared it. It was the same as the original. So the \gls{commit}
was intact. We also deliberately corrupted the Git \gls{packfile} that stored
the \SI{24}{\gibi\byte} by overwriting one \SI{1}{\mebi\byte} block at an offset
of \SI{22}{\gibi\byte} with new pseudo-random data. When we ran the fsck command
again with the corrupted repository, it reported the integrity error, but it did
not report the \lstinline{malloc} error that it did before.

Interestingly Git's fsck command found the integrity error surprisingly quickly,
reporting the error and exiting instantaneously (from the user's perspective),
whereas it had taken \SI{7}{\minute} to complete before. So it does not seem to
have hashed the whole file again to find the error. We took care to preserve the
modification time for the pack file when we corrupted it, so we are not sure how
Git detected the error so quickly. Perhaps our corruption overwrote some
internal indexing structure of the pack file along with the file data. More
investigation would be necessary to know for sure.

We continued the experiment under the assumption that Git's errors were a false
alarm, and allowed the trials to continue at even larger sizes.

The \gls{DMV} prototype was able to store a file up to \SI{64}{\gibi\byte} in
size, but time became a limiting factor as file size increased. At
\SI{96}{\gibi\byte}, our experiment script timed out and terminated the
\gls{commit} after five and a half hours. This sluggishness is due to the way
DMV stores chunks of the file as individual files on the filesystem, turning the
problem of storing one large file into the problem of storing many small files.
Storing many small files in this way incurs filesystem overhead, as we
discovered in the results of the number-of-files experiment
(\autoref{results-num-files--c1-time}), and later performed more experiments to
examine in detail (\autoref{perf-tuning-exp-chapter}).

Our experiment environment itself limited the largest file stored by any
\gls{VCS} to \SI{96}{\gibi\byte}. Any larger and it was simply impossible to
store a second copy of the file on our \SI{197}{\gibi\byte} test partition. Bup
was able to store a \SI{96}{\gibi\byte} file with no errors in just under two
hours. Git could also store such a large file, but one must ignore the
false-alarm "fatal" errors being reported by the user interface.

These findings are summarized in \autoref{file-sizes-table} and
\autoref{vcs-size-limits-table}.

%

\clearpage ~~~

\begin{figure}[p]
    \begin{leftfullpage}
        \caption{Wall-clock time to commit one large file to a fresh repository}
        \label{fig:plot-file-size--c1-time}
        \centering

        \vspace{\baselineskip}
        \explainlogsubfig
        % even up with other figure on opposite page that has more caption
        \vspace{\baselineskip}

        \includegraphics[]{plot-file-size--c1-time}
    \end{leftfullpage}
\end{figure}

\cleardoublepage

\subsection{Time for File-Size Initial Commit}
\label{results-file-size--c1-time}

\autoref{fig:plot-file-size--c1-time} shows the wall-clock time required for the
initial \gls{commit}, adding a single file of the given size to a fresh
repository. Over all, the trend is clear and unsurprising: \gls{commit} time
increases with file size. It increases linearly for Git, Mercurial, and Bup.
DMV's commit times increase in a more parabolic fashion, which is most apparent
in \autoref{fig:plot-file-size--c1-time}e.

At file sizes below around \SI{2}{\mebi\byte}
(\autoref{fig:plot-file-size--c1-time}a and b), \gls{commit} times are dominated
by overhead-- around \SI{5}{\ms} for Git, \SI{100}{\ms} for Mercurial,
\SI{180}{\ms} for \gls{DMV}, and \SI{900}{\ms} for Bup, vs only \SI{2}{\ms} for
the copy.

Bup, after starting with the highest overhead, goes on to have the fastest
initial \gls{commit} of all the systems evaluated for large files. It takes the
lead at \SI{2}{\gibi\byte}, where Mercurial drops out
(\autoref{fig:plot-file-size--c1-time}d). To \gls{commit} the \SI{2}{\gibi\byte}
file, Git's average time is \SI{91.1}{\s}, Bup's is \SI{89.1}{\s}, and
\gls{DMV}'s is \SI{90.8}{\s}. All of these are a factor of around ten times
slower than the direct copy at \SI{9.1}{\s}. The differences get more pronounced
as the file sizes continue to increase. At \SI{64}{\gibi\byte}, Git's average
time is \SI{110}{\minute}, Bup's is \SI{72}{\minute}, \gls{DMV}'s is
\SI{298}{\minute}. The average \SI{64}{\gibi\byte} copy takes \SI{35}{\minute}.

DMV's parabolic increase is due to the way it breaks the large file into chunks
and stores objects as individual files on the filesystem. While it is reading
one large file, it is writing many small files, which incurs filesystem
overhead. So its performance characteristic for storing a large file is closer
to that of storing many files (\autoref{results-num-files}). Bup also breaks the
file into many chunks, but it avoids the filesystem overhead by recombining the
chunks into \glspl{packfile}. We investigate the filesystem overhead further in
\autoref{perf-tuning-exp-chapter}.

Bup's commit times behave strangely in that there are places where Bup is
actually faster that it was with a smaller file. This is most apparent in the
slow downward slope of \autoref{fig:plot-file-size--c1-time}b and the zig-zag of
\autoref{fig:plot-file-size--c1-time}c. Git also has a point where the time
decreases, taking \SI{393}{\s} (SD \SI{.76}{\s}) to commit a \SI{4}{\gib} file
and only \SI{361}{\s} (SD \SI{3.90}{\s}) to commit an \SI{8}{\gib} file. Even
more interestingly, these decreases are consistent across the four trials on
separate hardware. We do not know what might be causing this.

%


\begin{figure}[p]
    \begin{leftfullpage}
        \caption{Wall-clock time to commit one updated large file}
        \label{fig:plot-file-size--c2-time}
        \centering

        X axis shows the total size of the file. The updated portion was
        \SI{1/1024}{th} of the total file size.
        \explainlogsubfig

        \includegraphics[]{plot-file-size--c2-time}
    \end{leftfullpage}
\end{figure}

\cleardoublepage

\subsection{Time for File-Size Update Commit}
\label{results-file-size--c2-time}

\autoref{fig:plot-file-size--c2-time} shows the wall-clock time required for the
second \gls{commit}, after updating \num{1/1024}th of the file. Ideally this
operation should be faster than the first \gls{commit}, because the system
should only be storing the changed portion of the file. Indeed this is the case
for Mercurial, Bup, and \gls{DMV}, which do store only the changed portion. Git,
however, copies the entire updated file into its repository as a new object, and
so its \gls{commit} time is virtually identical. The same is true of the copy
control, though for sizes smaller than \SI{8}{\gib} it is still faster than all
the other systems.

As with the initial commit, Bup gets faster as file size increases at certain
points, with the same gradual downward slope in the sub-megabyte and low
megabyte-ranges, leading to a prominent jump up then fall back down. The jump
appears at a slightly larger size with this update commit, at \num{8}, \num{12},
\num{16}, and \SI{24}{\mib}, as opposed to \num{4}, \num{6}, \num{8}, and
\SI{12}{\mib} in the initial commit. This puts it just outside the range of
\autoref{fig:plot-file-size--c2-time}c, but it can still be seen in miniature in
\autoref{fig:plot-file-size--c2-time}a. The shift to larger sizes in the update
commit suggests that the decrease is related to the amount of data written to
the disk, since Bup breaks the file into chunks and only writes the updated
chunks.

Git also shows a commit time decrease between \SI{4}{\gib} (\SI{518}{\s} with SD
\SI{11.1}{\s}) and \SI{8}{\gib} (\SI{429}{\s} with SD \SI{4.0}{\s}) just as it
did with the initial commit. Unlike Bup, its decrease is not shifted to higher
file sizes, which is another hint that the decrease has something to do with the
amount of data written, since Git writes the whole file again, rather than just
the updated portion.

%


\begin{figure}[p]
    \caption{CPU utilization while committing one large file to a fresh repository}
    \label{fig:plot-file-size--c1-cpu}
    \centering
    \includegraphics[]{plot-file-size--c1-cpu}
\end{figure}

\begin{figure}[p]
    \caption{CPU utilization while committing changes to one large file}
    \label{fig:plot-file-size--c2-cpu}
    \centering
    \includegraphics[]{plot-file-size--c2-cpu}
\end{figure}

\cleardoublepage

\subsection{CPU Usage During File-Size Commits}

\autoref{fig:plot-file-size--c1-cpu} shows CPU usage during the initial
\gls{commit}, and \autoref{fig:plot-file-size--c2-cpu} shows CPU usage during
the update commit. "User" indicates user-space computation, "system" indicates
kernel-space computation, and "iowait" indicates that the CPU was waiting on an
I/O operation.

We expected the commit operations to be I/O bound, and that seems to be the
case, especially at file sizes \SI{1}{\gib} and larger, and especially with DMV
and the copy. But there is also a significant amount of user-space work going on
in all of the version-control systems, such as hashing the data and, in
Mercurial's case, calculating deltas.

DMV's high I/O wait is probably primarily due to the filesystem overhead that is
slowing down its commit times, but the current single-threaded implementation
might also be contributing, and the prototype might benefit from having separate
threads to load data, hash it, and write it to dish.

There is a small peak in the DMV I/O wait percentage for the first commit
(\autoref{fig:plot-file-size--c1-cpu}d), peaking at \SI{386}{\mib} with
\SI{337}{ticks} (SD \SI{13.8}{ticks}), then falling at \SI{512}{\mib} to
\SI{89}{ticks} (SD \SI{49.2}{ticks}), and then rising again at \SI{768}{\mib} to
\SI{972}{ticks} (SD \SI{54.3}{ticks}). We are not sure what is causing this. We
also do not know why the DMV I/O wait during the update commit actually
decreases between \SI{1}{\gib} and \SI{32}{\gib}
(\autoref{fig:plot-file-size--c2-cpu}d).

  %402653184         337.25   13.7727085208
  %536870912          89.25   49.1801535175
  %805306368         972.25   54.2972144774

The copy operation shows an erratic amount of system activity at low sizes,
which is surprising since the copy operation is almost pure I/O. This is a
result of how the data was collected rather than any unexpected behavior in the
copy operation. At sizes below about \SI{10}{\mib}, the copy operation is faster
than the \lstinline{/proc/stat} ticks used to CPU measure activity
(\num{1/100}{th} of a second\cite{proc_man_page}). At such small intervals, most
measurements will yield zero ticks, though one of the states will occasionally
measure one tick (see \autoref{copy-cpu-data}). Then, when the graph normalizes
the total usage to a percentage, \num{1} system tick out of \num{1} total ticks
makes \SI{100}{\percent}, so the graph will jump from \SI{0}{\percent}
\SI{100}{\percent}, or somewhere in between depending on how many of the four
trials measured one tick. It isn't until around \SI{48}{\mib} that the
measurement includes enough CPU ticks to yield useful percentages, which is
where we see the graph start to stabilize.


\begin{table}[hp]
    \caption{Selected CPU usage data for copy operation}
    \label{copy-cpu-data}

    \centering
    \begin{tabular}{r r r r r}
        Size & User & System & Idle & Iowait \\
        \midrule
  4.0MiB &    0    &      0    &      0    &      0 \\
  6.0MiB &    0    &      0    &      1    &      0 \\
  8.0MiB &    0    &      1    &      1    &      0 \\
 12.0MiB &    1    &      1    &      2    &      0 \\
 16.0MiB &    0    &      2    &      2    &      0 \\
 24.0MiB &    1    &      3    &      3    &      0 \\
 32.0MiB &    0    &      3    &      4    &      0 \\
 48.0MiB &    0    &      5    &      5    &      0 \\
 64.0MiB &    0    &      7    &      6    &      0 \\
 96.0MiB &    0    &     10    &     10    &      0 \\
    \end{tabular}
\end{table}

%


\begin{figure}[p]
    %\begin{leftfullpage}
        \caption{Total repository size after committing, editing, and committing again}
        \label{fig:plot-file-size--repo-size}
        \centering

        \explaindiskspaceplot

        \includegraphics[]{plot-file-size--repo-size}
    %\end{leftfullpage}
\end{figure}

\cleardoublepage

\subsection{Repository Size after File-Size Update Commit}

\autoref{fig:plot-file-size--repo-size} shows the total repository size after
the update \gls{commit}, including the original file. This is after committing,
updating \num{1/1024}th of the file, and committing again.

The stored data overtakes the initial repository overhead after a file size of
around \SI{1}{\mib}, and the repository size for all systems converges to about
twice the size of the file. This is to be expected, since each measurement
includes the original file, the first copy of the file, and the updated
\num{1/1024}th. The exception is Git, which stores the entire updated file
during the update \gls{commit}, leading to a total disk space usage of three
times the file size. However, Git has a separate garbage collection stage where
it cleans up the repository and aggregates similar objects together in
\glspl{packfile}. The post-garbage collection size for Git is shown as a
separate line on the graph. This post-GC size converges to double the original
file size, but then jumps to three times at a file size of \SI{1.5}{\gib}. This
suggests that the \glsdisp{packfile}{pack} step is failing silently at
\SI{1.5}{\gib} and larger. This is probably related to the way Mercurial's
\glspl{commit} begin failing at \SI{2}{\gib} and larger. Both operations are
trying to load multiple versions of the file into memory to calculate deltas for
\glsdisp{packfile}{packing}.

%


\cleardoublepage

\section{Results: Number of Files}
\label{results-num-files}

\subsection{File Quantity Limits}

\glsunset{inode}

Git, Mercurial, DMV, and the copy operation all failed when trying to store
\num{7.5} million files or more, reporting that the disk was full. However, it
wasn't actually out of space. The disk was out of \glspl{inode}.

\glsreset{inode}

Unix filesystems, ext4 included, store file and directory metadata in a data
structure called an \gls{inode}, which reside in a fixed-length
table\cite{unix_timesharing_system}. When all of the \glspl{inode} in the table
are allocated, the filesystem cannot store any more files or directories.

Git, Mercurial, DMV, and the copy all create one file in their
\glspl{objectstore} for each input file. So to store \num{7.5} million files,
they will create \num{7.5} million more, resulting in \num{15} million files on
the filesystem. However, the \SI{197}{\gib} experiment partition has only
\num{13107200} total inodes, so storing \num{15} million files is impossible.

Bup is able to store more files because it does not write a separate file for
each input file. Bup aggregates its DAG objects into \glspl{packfile}, writing
several large files instead many small files. As such, it does not exhaust the
disk's inodes, and can continue until the experiment itself exhausts the
system's inodes when trying to run a trial with \num{25} million files.

%

\begin{figure}[p]
    \begin{leftfullpage}
        \caption{Wall-clock time to commit many 1KiB files to a fresh repository}
        \label{fig:plot-num-files--c1-time}
        \centering

        \explainlogsubfig

        \includegraphics[]{plot-num-files--c1-time}
    \end{leftfullpage}
\end{figure}

\begin{figure}[p]
    \begin{leftfullpage}
        \caption{Wall-clock time to commit many updated files}
        \label{fig:plot-num-files--c2-time}
        \centering

        X axis shows the total number of files. 1 out of every 16 files was updated.
        \explainlogsubfig

        \includegraphics[]{plot-num-files--c2-time}
    \end{leftfullpage}
\end{figure}

\cleardoublepage

\subsection{Time for Number-of-Files Initial Commit}
\label{results-num-files--c1-time}

\autoref{fig:plot-num-files--c1-time} shows the time required for the initial
\gls{commit}, storing all files into a fresh empty repository. Here we see the
commit times for Git and DMV increasing quadratically with the number of files,
while Mercurial, Bup, and the copy increase linearly.

We saw in the file-size commit times (\autoref{results-file-size--c1-time}) that
DMV's time increased quadratically, and we suspected that was because it was
creating many small files and incurring filesystem overhead. This effect would
explain why both Git and DMV do so poorly here while Bup would fare much better.
But why then would Mercurial and the copy also have a linear increase instead of
an quadratic one?

The difference is the naming schemes of stored files. Git and DMV name each
object file according to the SHA-1 hash of the object's contents, while
Mercurial, like the copy, uses the original input file's name. This means that
Git and DMV write files in a random order with respect to their names, jumping
between different \gls{objectstore} subdirectories, while Mercurial and the copy
can write files in the order they read them, one subdirectory at a time. The
filesystem is most likely optimized for that kind of sequential write.

At \num{500}, \num{750}, and \num{1000} files
(\autoref{fig:plot-num-files--c1-time}b), Bup's commit times have an unusually
high variance compared to the other systems and the other experiments. At those
file counts, the standard deviations for Bup's commit times are
\SI{18.3}{\percent}, \SI{29.6}{\percent}, \SI{28.3}{\percent} of the means,
respectively (see \autoref{bup-commit-times-high-variance}), whereas the mean of
the other such standard deviation percentages is only \SI{6.1}{\percent}. We are
not sure what is causing this.

\begin{table}[b]
    \caption{Bup initial commit times with unusually high variance}
    \label{bup-commit-times-high-variance}
    \centering
    \begin{tabular}{rccc}
        Num. files & Mean time (s) & SD (s) & SD as \% of mean \\
           500  &   0.965  &   0.177  &  18.3 \\
           750  &   1.199  &   0.355  &  29.6 \\
          1000  &   1.407  &   0.398  &  28.3 \\
    \end{tabular}
\end{table}

%


\cleardoublepage
\subsection{Time for Number-of-Files Update Commit}

\autoref{fig:plot-num-files--c2-time} shows the wall-clock time required for the
second \gls{commit}, after updating \num{1} out of every \num{16} files. As with
the file-size experiment (\autoref{results-file-size--c2-time}), storing only
the updated files is faster, and in this case the difference is more pronounced.
This is because every system understands the file as a unit of data, and can
naturally separate the changed and files from the unchanged files.

The exception is the copy operation, which blindly copies all files again. This
is why it is actually slower than the other systems at some points. It would be
interesting to run this experiment with Rsync as well (\autoref{related-rsync}),
to get a baseline for comparing and copying only the files that have changed.

Here all commit times appear to increase linearly with respect to number of
files, except for Git, which shows some quadratic growth as the number of files
gets into the millions.

Bup does not exhibit the interesting variance here that it did in the initial
commit. All standard deviations are small enough that the error bars are barely
discernible on the graph.

%


\begin{figure}[p]
    \caption{CPU utilization while committing many 1KiB files to a fresh
    repository}
    \label{fig:plot-num-files--c1-cpu}
    \centering
    \includegraphics[]{plot-num-files--c1-cpu}
\end{figure}

\begin{figure}[p]
    \caption{CPU utilization while committing many 1KiB files after one of every
        \num{16} files has been updated}
    \label{fig:plot-num-files--c2-cpu}
    \centering
    \includegraphics[]{plot-num-files--c2-cpu}
\end{figure}

\cleardoublepage

\subsection{CPU Usage During Number-of-Files Commits}

\autoref{fig:plot-num-files--c1-cpu} shows CPU utilization during the initial
\gls{commit} and \autoref{fig:plot-num-files--c1-cpu} shows CPU utilization
during the update \gls{commit}.

With the initial commit, Git, DMV, and the copy spend more time waiting on I/O
than Bup or Mercurial, and they also spend more time in system mode. In the case
of the copy, this is probably because the operation is almost pure I/O. In the
case of Git and \gls{DMV}, this is more evidence to suggest that writing files
with effectively random names incurs more filesystem overhead than writing
sequentially, as Mercurial does, or appending to large files, as Bup does. We
can see that Bup and Mercurial both spend more time processing in user mode than
waiting for I/O.

With the update commit, Mercurial loses its sequential write advantage, since it
has to seek to the \gls{filelog} that corresponds to the current input file and
append to it. And so we see much more I/O wait with Mercurial. Bup continues to
simply append objects to its \glspl{packfile} as always, and so it retains a low
I/O wait profile.

Interestingly, Mercurial's and \gls{DMV}'s I/O wait in the update commit has a
gradual rise starting around 100 thousand files, while Git's is a sudden rise
starting just before 100 thousand files. The copy is somewhere in between. We
are not sure why that is, since \gls{DMV} has much more in common with Git in
terms of disk usage patterns.

Git, \gls{DMV}, and the copy all decrease in I/O wait from \num{1} million to
\num{5} million files. And Mercurial also shows a slight decrease from \num{2.5}
million to \num{5} million files. Again, we are not sure what would be causing
that.

%


\begin{figure}[p]
    \begin{leftfullpage}
        \caption{Real time required to check the status of many files after
        initial commit}
        \label{fig:plot-num-files--stat1-time}
        \centering

        \explainlogsubfig

        \includegraphics[]{plot-num-files--stat1-time}
    \end{leftfullpage}
\end{figure}

\cleardoublepage

\subsection{Time for Number-of-Files Status Check}

\autoref{fig:plot-num-files--stat1-time} shows the time
required to check the changed status of all files just after committing.

\towrite{Prose description of results}

%


\begin{figure}[p]
    \caption{Total repository size after committing, editing, and committing again}
    \label{fig:plot-num-files--repo-size}
    \centering

    \explaindiskspaceplot

    \includegraphics[]{plot-num-files--repo-size}
\end{figure}

\cleardoublepage

\subsection{Repository Size after Number-of-Files Update Commit}

% Analyze:
%
% ../data/meantable.py --columns filecount c2_size \
%       --files ../data/exp--num-files--v03--less-dirs/git-* \
%   | tail -n+2 \
%   | awk -e '{printf "%7d  %5.3f\n", $1, $2/($1*1024)}'

% At 5M files...
% git: 8.478
% hg: 8.186
% bup: 5.374
% prototype2x1mem: 8.538
% copy: 8.043

\autoref{fig:plot-num-files--repo-size} shows the total repository size after
the update commit, including the original files. This is after committing once,
changing a single byte in every sixteenth file, and committing again.

Git, Mercurial, DMV, and the copy all converge to using just over \num{8} times
the theoretical size of the data set, while Bup is closer to \num{5} times. This
has to do with the block size of the filesystem. The underlying filesystem uses
a \SI{4}{\kib} block size, so each \SI{1}{\kib} file uses one \SI{4}{\kib}
block. So the input data itself takes \num{4} times its theoretical size. So
each will end up with \num{4} times for the input files, and \num{4} times for
the copied files, plus some overhead. So a storage ratio of just over \num{8} is to be
expected for those systems that store objects as individual files.

Naturally the copy would have the least overhead, with a ratio of \num{8.043} at
\SI{5} million files. This shows that the directory hierarchy of the input files
itself probably adds around \num{2.1}{percent}. Mercurial has the second lowest
overhead, with a storage ratio of \num{8.186}. This makes sense, because while
Mercurial creates a \gls{filelog} for each input files, it reuses
\glspl{filelog} to store the updates versions, so it does not create any new
files during the second commit.

Git and \gls{DMV} have similar, higher ratios because they do create new object
files for each new version of each input file. At \SI{5} million files Git's
ratio is \num{8.478} and \gls{DMV}'s is \num{8.538}. It would be interesting to
see Git's ration after garbage collection and \glsdisp{packfile}{packing}, but
unfortunately we did not run Git's garbage collection as part of this experiment
as we did in the file-size experiment. We assume the results would be similar to
Bup.

Bup uses significantly less disk space, with a ratio of \num{5.374} at \num{5}
million files. The input files themselves account for \num{4} times the
theoretical size, so by aggregating them into \glspl{packfile}, Bup is storing
the data in a form that is closer to its theoretical size, taking \num{1.374}
times the space.

%
