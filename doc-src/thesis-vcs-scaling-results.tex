\section{Results: File Size}

\subsection{File Size Limits}
\label{file-size-limits-results}

Both Git and Mercurial had limits to the size of file they could store
successfully. With a \SI{2}{\gibi\byte} file, Mercurial would issue a warning,
saying "up to 6442 MB of RAM may be required to manage this file," but the
\gls{commit} would be stored successfully. With a file \SI{4}{\gibi\byte} or
larger, Mercurial's \gls{commit} operation would exit with an error message and
the \gls{commit} would not be stored. However, the repository would be left in a
consistent empty state. The atomicity of the \gls{commit} operation held.

Git's behavior was more erratic. Starting with a file \SI{12}{\gibi\byte} and
larger, Git's \gls{commit} operation would exit with an error code, reporting a
fatal out-of-memory error that \lstinline{malloc} failed to allocate
\SI{12}{\gibi\byte}. However, the \gls{commit} would be successfully stored with
no consistency errors in the repository reported by \lstinline{git fsck}.
Starting at \SI{24}{\gibi\byte}, the \gls{commit} operation would report the
same error, the \gls{commit} would still be written, but then the 
\lstinline{git fsck} integrity check would also exit with an error code.
However, the error it reported in its output was a similar "fatal" error as
Git's \lstinline{malloc} error as the \gls{commit} operation, and it did not
report any actual integrity errors in the repository.

So to test the \gls{commit}, we extracted the \SI{24}{\gibi\byte} file from the
repository and compared it. It was the same as the original. So the \gls{commit}
was intact. We also deliberately corrupted the Git \gls{packfile} that stored
the \SI{24}{\gibi\byte} by overwriting one \SI{1}{\mebi\byte} block at an offset
of \SI{22}{\gibi\byte} with new pseudo-random data. When we ran the fsck command
again with the corrupted repository, it reported the integrity error, but it did
not report the \lstinline{malloc} error that it did before. So for the other
trials with larger files, we assumed that Git's errors were a false alarm and
allowed the trail to continue.

\towrite{After corruption, answer instantaneous rather than 7m before. Some
sorcery detected tampering, even though we fixed the mod time?}

The \gls{DMV} prototype was able to store a file up to \SI{64}{\gibi\byte} in
size, but time became a limiting factor as file size increased. At
\SI{96}{\gibi\byte}, our experiment script timed out and terminated the
\gls{commit} after five and a half hours.\perotto{Bottleneck was...?}

Our test environment itself limited the largest file stored by any \gls{VCS} to
\SI{96}{\gibi\byte}. Any larger and it was simply impossible to store a second
copy of the file on our \SI{197}{\gibi\byte} test partition. Bup was able to
store a \SI{96}{\gibi\byte} file with no errors in just under two hours. Git
could also store such a large file, but one must ignore the false-alarm "fatal"
errors being reported by the user interface.

These findings are summarized in \autoref{file-sizes-table} and
\autoref{vcs-size-limits-table}.

\begin{table}[p]
    \caption{Observations as file size increases}
    \label{file-sizes-table}
    \centering
    \begin{tabular}{r l}
        Size & Observation \\
        \midrule
        \SI{1.5}{\gibi\byte} & Largest successful commit with Mercurial \\
        \SI{2}{\gibi\byte} & Mercurial commit rejected \\
        \SI{8}{\gibi\byte} & Largest successful commit with Git \\
        \SI{12}{\gibi\byte} & Git false-alarm errors begin, but commit still intact \\
        \SI{16}{\gibi\byte} & Largest successful Git fsck command \\
        \SI{24}{\gibi\byte} & Git false-alarm errors begin during fsck, but commit still intact \\
        \SI{64}{\gibi\byte} & Largest successful DMV commit \\
        \SI{96}{\gibi\byte} & DMV timeout after \SI{5.5}{\hour} \\
        \SI{96}{\gibi\byte} & Last successful commit with Bup (and Git, ignoring false-alarm errors) \\
        \SI{128}{\gibi\byte} & All fail due to size of test partition \\
    \end{tabular}
\end{table}

\begin{table}[p]
    \caption{Effective size limits for VCSs tested}
    \label{vcs-size-limits-table}
    \centering
    \begin{tabular}{l l}

        VCS & Effective limit \\
        \midrule

        Git & Commit intact at all sizes, UI reports errors at \SI{12}{\gibi\byte} and larger \\

        Mercurial & Commit rejected at \SI{2}{\gibi\byte} and larger \\

        Bup & Successful commits at all sizes tested, up to \SI{96}{\gibi\byte} \\

        DMV & Successful commits up to \SI{64}{\gibi\byte}, timeout at
        \SI{5.5}{\hour} during \SI{96}{\gibi\byte} trial

    \end{tabular}
\end{table}

%


\begin{figure}[p]
    \begin{leftfullpage}
        \caption{Wall-clock time to commit one large file to a fresh repository}
        \label{fig:plot-file-size--c1-time}
        \centering

        \vspace{\baselineskip}
        \explainlogsubfig
        % even up with other figure on opposite page that has more caption
        \vspace{\baselineskip}

        \includegraphics[]{plot-file-size--c1-time}
    \end{leftfullpage}
\end{figure}

\cleardoublepage

\subsection{Time for File-Size Initial Commit}

\autoref{fig:plot-file-size--c1-time} shows the wall-clock time required for the
initial \gls{commit}, adding a single file of the given size to a fresh
repository. Over all, the trend is clear and unsurprising: \gls{commit} time
increases with file size. It increases linearly for Git, Mercurial, and Bup.
DMV's commit times increase in a more parabolic fashion, which is most apparent
in \autoref{fig:plot-file-size--c1-time}e.

At file sizes below around \SI{2}{\mebi\byte}
(\autoref{fig:plot-file-size--c1-time}a and b), \gls{commit} times are dominated
by overhead-- around \SI{5}{\ms} for Git, \SI{100}{\ms} for Mercurial,
\SI{180}{\ms} for \gls{DMV}, and \SI{900}{\ms} for Bup, vs only \SI{2}{\ms} for
the copy.

Bup, after starting with the highest overhead, goes on to have the fastest
initial \gls{commit} of all the systems tested for large files. It takes the
lead at \SI{2}{\gibi\byte}, where Mercurial drops out
(\autoref{fig:plot-file-size--c1-time}d). To \gls{commit} the \SI{2}{\gibi\byte}
file, Git's average time is \SI{91.1}{\s}, Bup's is \SI{89.1}{\s}, and
\gls{DMV}'s is \SI{90.8}{\s}. All of these are a factor of around ten times
slower than the direct copy at \SI{9.1}{\s}. The differences get more pronounced
as the file sizes continue to increase. At \SI{64}{\gibi\byte}, Git's average
time is \SI{110}{\minute}, Bup's is \SI{72}{\minute}, \gls{DMV}'s is
\SI{298}{\minute}. The average \SI{64}{\gibi\byte} copy takes \SI{35}{\minute}.

DMV's parabolic increase is due to the way it breaks the large file into chunks
and stores objects as individual files on the filesystem. While it is reading
one large file, it is writing many small files, which incurs filesystem
overhead. So its performance characteristic for storing a large file is closer
to that of storing many files (\autoref{results-num-files}). Bup also breaks the
file into many chunks, but it avoids the filesystem overhead by recombining the
chunks into \glspl{packfile}. We investigate the filesystem overhead further in
\autoref{perf-tuning-exp-chapter}.

Bup's commit times behave strangely in that there are places where Bup is
actually faster that it was with a smaller file. This is most apparent in the
slow downward slope of \autoref{fig:plot-file-size--c1-time}b and the zig-zag of
\autoref{fig:plot-file-size--c1-time}c. Git also has a point where the time
decreases, taking \SI{393}{\s} (SD \SI{.76}{\s}) to commit a \SI{4}{\gib} file
and only \SI{361}{\s} (SD \SI{3.90}{\s}) to commit an \SI{8}{\gib} file. Even
more interestingly, these decreases are consistent across the four trials on
separate hardware. We do not know what might be causing this.

%


\begin{figure}[p]
    \begin{leftfullpage}
        \caption{Wall-clock time to commit one updated large file}
        \label{fig:plot-file-size--c2-time}
        \centering

        X axis shows the total size of the file. The updated portion was
        \SI{1/1024}{th} of the total file size.
        \explainlogsubfig

        \includegraphics[]{plot-file-size--c2-time}
    \end{leftfullpage}
\end{figure}

\cleardoublepage

\subsection{Time for File-Size Update Commit}

\autoref{fig:plot-file-size--c2-time} shows the wall-clock time required for the
second \gls{commit}, after updating \num{1/1024}th of the file. Ideally this
operation should be faster than the first \gls{commit}, because the system
should only be storing the changed portion of the file. Indeed this is the case
for Mercurial, Bup, and \gls{DMV}, which do store only the changed portion. Git,
however, copies the entire updated file into its repository as a new object, and
so its \gls{commit} time is virtually identical. The same is true of the copy
control, though for sizes smaller than \SI{8}{\gib} it is still faster than all
the other systems.

As with the initial commit, Bup gets faster as file size increases at certain
points, with the same gradual downward slope in the sub-megabyte and low
megabyte-ranges, leading to a prominent jump up then fall back down. The jump
appears at a slightly larger size with this update commit, at \num{8}, \num{12},
\num{16}, and \SI{24}{\mib}, as opposed to \num{4}, \num{6}, \num{8}, and
\SI{12}{\mib} in the initial commit. This puts it just outside the range of
\autoref{fig:plot-file-size--c2-time}c, but it can still be seen in miniature in
\autoref{fig:plot-file-size--c2-time}a. The shift to larger sizes in the update
commit suggests that the decrease is related to the amount of data written to
the disk, since Bup breaks the file into chunks and only writes the updated
chunks.

Git also shows a commit time decrease between \SI{4}{\gib} (\SI{518}{\s} with SD
\SI{11.1}{\s}) and \SI{8}{\gib} (\SI{429}{\s} with SD \SI{4.0}{\s}) just as it
did with the initial commit. Unlike Bup, its decrease is not shifted to higher
file sizes, which is another hint that the decrease has something to do with the
amount of data written, since Git writes the whole file again, rather than just
the updated portion.

%


\begin{figure}[p]
    \caption{CPU utilization while committing one large file to a fresh repository}
    \label{fig:plot-file-size--c1-cpu}
    \centering
    \includegraphics[]{plot-file-size--c1-cpu}
\end{figure}

\begin{figure}[p]
    \caption{CPU utilization while committing changes to one large file}
    \label{fig:plot-file-size--c2-cpu}
    \centering
    \includegraphics[]{plot-file-size--c2-cpu}
\end{figure}

\cleardoublepage

\subsection{CPU Usage During File-Size Commits}

\autoref{fig:plot-file-size--c1-cpu} shows CPU usage during the initial
\gls{commit}, and \autoref{fig:plot-file-size--c2-cpu} shows CPU usage during
the update commit. We expected the commit operations to be IO bound, and that
seems to be the case, especially at file sizes \SI{1}{\gib} and larger, and
especially with DMV and the copy.

\towrite{Prose description of results}
\perottoinline{Explain peaks in prototype CPU graph}
\perottoinline{Discuss somewhere "what if" iowait could be reduced
significantly. What would happen with memory, disk, remote}

%


\begin{figure}[p]
    \begin{leftfullpage}
        \caption{Total repository size after committing, editing, and committing again}
        \label{fig:plot-file-size--repo-size}
        \centering

        \explaindiskspaceplot

        \includegraphics[]{plot-file-size--repo-size}
    \end{leftfullpage}
\end{figure}

\cleardoublepage

\subsection{Repository Size after File-Size Update Commit}

\autoref{fig:plot-file-size--repo-size} shows the total repository size after
the update \gls{commit}, including the original file. This is after committing,
updating \num{1/1024}th of the file, and committing again.

The stored data overtakes the initial repository overhead after a file size of
around \SI{1}{\mib}, and the repository size for all systems converges to about
twice the size of the file. This is to be expected, since each measurement
includes the original file, the first copy of the file, and the updated
\num{1/1024}th. The exception is Git, which stores the entire updated file
during the update \gls{commit}, leading to a total disk space usage of three
times the file size. However, Git has a separate garbage collection stage where
it cleans up the repository and aggregates similar objects together in
\glspl{packfile}. The post-garbage collection size for Git is shown as a
separate line on the graph. This post-GC size converges to double the original
file size, but then jumps to three times at a file size of \SI{1.5}{\gib}. This
suggests that the \glsdisp{packfile}{pack} step is failing silently at
\SI{1.5}{\gib} and larger. This is probably related to the way Mercurial's
\glspl{commit} begin failing at \SI{2}{\gib} and larger. Both operations are
trying to load multiple versions of the file into memory to calculate deltas for
\glsdisp{packfile}{packing}.

%



\begin{figure}[p]
    \begin{leftfullpage}
        \caption{Wall-clock time to commit many 1KiB files to a fresh repository}
        \label{fig:plot-num-files--c1-time}
        \centering

        \explainlogsubfig

        \includegraphics[]{plot-num-files--c1-time}
    \end{leftfullpage}
\end{figure}

\cleardoublepage

\section{Results: Number of Files}
\label{results-num-files}


\subsection{Time for Number-of-Files Initial Commit}

\autoref{fig:plot-num-files--c1-time} shows the time required for the initial
\gls{commit}, copying all files into a fresh empty repository.

\towrite{Prose description of results}

%


\begin{figure}[p]
    \begin{leftfullpage}
        \caption{Wall-clock time to commit many updated files}
        \label{fig:plot-num-files--c2-time}
        \centering

        X axis shows the total number of files. 1 out of every 16 files was updated.
        \explainlogsubfig

        \includegraphics[]{plot-num-files--c2-time}
    \end{leftfullpage}
\end{figure}

\cleardoublepage

\subsection{Time for Number-of-Files Update Commit}

\towrite{Prose description of results}

%


\begin{figure}[p]
    \caption{CPU utilization while committing many 1KiB files to a fresh
    repository}
    \label{fig:plot-num-files--c1-cpu}
    \centering
    \includegraphics[]{plot-num-files--c1-cpu}
\end{figure}

\begin{figure}[p]
    \caption{CPU utilization while committing many 1KiB files after one of every
        \num{16} files has been updated}
    \label{fig:plot-num-files--c2-cpu}
    \centering
    \includegraphics[]{plot-num-files--c2-cpu}
\end{figure}

\cleardoublepage

\subsection{CPU Usage During Number-of-Files Commits}

\autoref{fig:plot-num-files--c1-cpu} shows CPU utilization during the
\gls{commit}.

\towrite{Prose description of results}

%


\begin{figure}[p]
    \begin{leftfullpage}
        \caption{Real time required to check the status of many files after
        initial commit}
        \label{fig:plot-num-files--stat1-time}
        \centering

        \explainlogsubfig

        \includegraphics[]{plot-num-files--stat1-time}
    \end{leftfullpage}
\end{figure}

\cleardoublepage

\subsection{Time for Number-of-Files Status Check}

\autoref{fig:plot-num-files--stat1-time} shows the time
required to check the changed status of all files just after committing.

\towrite{Prose description of results}

%


\begin{figure}[p]
    \caption{Total repository size after committing, editing, and committing again}
    \label{fig:plot-num-files--repo-size}
    \centering

    \explaindiskspaceplot

    \includegraphics[]{plot-num-files--repo-size}
\end{figure}

\cleardoublepage

\subsection{Repository Size after Number-of-Files Update Commit}

\autoref{fig:plot-num-files--repo-size} shows the total
repository size, including the original files, after committing once, editing
1/1024th of every sixteenth file, and committing again.

\towrite{Prose description of results}


\todo[inline]{Update these points and move them to appropriate prose
descriptions}

Unlike the experiment with a single large file, the numerous small files did not
quickly hit error-causing disk space or RAM limitations. The version control
systems happily crunched\perotto{too informal} the data as trial times grew into
hours.

\begin{itemize}

    \item Again, after some initial overhead, commit and times increase
        linearly. However, Git's initial commit times actually decrease at
        certain points (128Ki, 1.5Mi, and 2Mi files). We are not sure how to
        explain this.

    \item Git in general is faster then Mercurial up to about half a million
        files. At 512Ki files is Mercurial and Git are about neck and neck. At
        768Ki and over, Mercurial is faster.

    \item Status check times are more erratic, though still increasing linearly
        overall. The variations may have to do with the output of the status
        commands and whether our terminal multiplexer was focused on the
        execution at the time. The status commands print one status line per
        file changed, which can be significant when hundreds of thousands of
        files involved. This output is significantly slower when the terminal
        multiplexer we used to monitor the experiments is connected, because it
        sends every line over the network to the monitoring machine.

    \item Mercurial update status is consistently slower than initial status,
        often by about 2-3x.

    \item Both Git and Mercurial converge to a little over 8x the space
        required. This probably has more to do with the filesystem block size
        than anything else. The underlying file system uses a 4KiB block size,
        so each 1KiB file will still use 4KiB of disk space. And since there are
        two copies of each file, that's 8KiB total for each 1KiB file, 8x the
        disk space.

    \item Mercurial converges towards the 8x limit faster though. We speculate
        this is because of lower repo overhead, and also because Git is creating
        \gls{tree} objects for each of the subdirectories in the file set. These
        files will be small, but each will take up another 4KiB block on the
        disk.

    \item Mercurial commits began to abort with disk space errors at 8Mi files,
        8GiB of data. This was surprising. Even at 8x disk usage, that should
        only be 64GiB of disk usage, well below the 192GiB free on the test
        disk.

\end{itemize}

%
