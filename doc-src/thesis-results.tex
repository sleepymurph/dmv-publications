\chapter{Results}

\section{VCS Scaling: File Size}

\subsection{File Size Limits}

Both Git and Mercurial had limits to the size of file they could store
successfully. With a \SI{2}{\gibi\byte} file, Mercurial would issue a warning,
saying "up to 6442 MB of RAM may be required to manage this file," but the
commit would be stored successfully. With a file \SI{4}{\gibi\byte} or larger,
Mercurial's commit operation would exit with an error message and the commit
would not be stored. However, the repository would be left in a consistent empty
state. The atomicity of the commit operation held.

Git's behavior was more erratic. Starting with a file \SI{12}{\gibi\byte} and
larger, Git's commit operation would exit with an error code, reporting a fatal
out-of-memory error that \lstinline{malloc} failed to allocate
\SI{12}{\gibi\byte}. However, the commit would be successfully stored with no
consistency errors in the repository reported by \lstinline{git fsck}. Starting
at \SI{24}{\gibi\byte}, the commit operation would report the same error, the
commit would still be written, but the \lstinline{git fsck} integrity check
would exit with an error code. However, the error it reported in its output was
a similar "fatal" error as Git's \lstinline{malloc} error as the commit
operation, and it did not report any actual integrity errors in the repository.

We thought it was possible that, like the commit operation, the fsck operation
was reporting a fatal error but still completing its task. So to test, we
deliberately corrupted the repository by writing some random data to a file in
the Git's object store and then reran \lstinline{git fsck}. It reported both the
\lstinline{malloc} and the integrity error (\autoref{git-fsck-error-output}).
So, when evaluating Git data collected by the experiment script, we assume that
the Git repository was intact even when the \lstinline{git fsck} exited with an
error code.

\begin{figure}[]
    \caption{Git fsck output after committing a \SI{24}{\gibi\byte} file and
        deliberately corrupting the repository, showing both "fatal" memory
        error and actual data integrity error}
    \label{git-fsck-error-output}
    \missingfigure{Git fsck output showing "fatal" error and integrity error}
\end{figure}

We were able to store a file up to \SI{64}{\gibi\byte} in size with our DMV
prototype, but time became a limiting factor as file size increased. At
\SI{96}{\gibi\byte}, our experiment script timed out and terminated the commit
after five and a half hours.

Our test environment itself limited the largest file stored by any VCS to
\SI{96}{\gibi\byte}. Any larger and it was simply impossible to store a second
copy of the file on our \SI{197}{\gibi\byte} test partition. Bup was able to
store a \SI{96}{\gibi\byte} file with no errors in just under two hours. Git
could also store such a large file, but one must ignore the false-alarm "fatal"
errors being reported by the user interface.


\subsection{Commit Time}

\begin{figure}[]
  \caption{Committing one large file to a fresh repository}
  \label{fig:plot-file-size--c1-time}
  \centering
    \includegraphics[]{plot-file-size--c1-time}
\end{figure}

\autoref{fig:plot-file-size--c1-time} shows the time required for
the initial commit, adding a file of the given size to a fresh repository.


\begin{figure}[]
    \caption{Committing changes to a large file}
  \label{fig:plot-file-size--c2-time}
  \centering
    \includegraphics[]{plot-file-size--c2-time}
\end{figure}

\autoref{fig:plot-file-size--c2-time} shows the time required for the second
commit, after \num{1/1024}th of the file was updated.


\begin{figure}[]
  \caption{CPU utilization while committing one file to an empty repository}
  \label{fig:plot-file-size--c1-cpu}
  \centering
    \includegraphics[]{plot-file-size--c1-cpu-a}
    \includegraphics[]{plot-file-size--c1-cpu-b}
\end{figure}

\autoref{fig:plot-file-size--c1-cpu} shows the
CPU usage during the initial commit.


\begin{figure}[]
  \caption{Total repository size after committing, editing, and committing again}
  \label{fig:plot-file-size--repo-size}
  \centering
    \includegraphics[]{plot-file-size--repo-size}
\end{figure}

\autoref{fig:plot-file-size--repo-size} shows the total
repository size, including the original file, after committing once, editing
1/1024th of the file, and committing again.


\iffalse

\subsubsection{Observations}

Performance observations:

\begin{itemize}

    \item After some initial overhead, performance increases linearly with size.
        This is to be expected, since the operations are IO bound, copying all
        data to the repository.

    \item Times for the Mercurial update are faster than for Git's update,
        because Mercurial's archive format only saves deltas to the file.

    \item Mercurial has more initial time overhead, this is probably due to the
        fact that it is written in Python and requires starting the Python
        interpreter each time. This overhead is only 50 or 100ms, and quickly
        becomes insignificant compared to the IO operation time.

    \item Mercurial has less initial space overhead than Git.

        \begin{itemize}
            \setlength{\itemsep}{0pt}
            \setlength{\parskip}{0pt}
            \setlength{\parsep}{0pt}
            \item Minimum Mercurial repository size: 80KiB.
            \item Minimum Git repository size: 16KiB.
        \end{itemize}

        This can be seen in the disk space graph in the way the Mercurial usage
        converges towards 2x faster than Git usage does. But again, this quickly
        becomes insignificant compared to the size of the file.

\end{itemize}

\fi

\section{VCS Scaling: Number of Files}

\subsubsection{Results}

\begin{figure}[p]
  \caption{Real time required to commit many 1KiB files to an empty repository}
  \label{fig:plot-num-files--c1-time}
  \centering
    \includegraphics[]{plot-num-files--c1-time}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to commit many 1KiB files to an empty repository
  (detail)}
  \label{fig:plot-num-files--c1-time-detail}
  \centering
    \includegraphics[]{plot-num-files--c1-time-detail}
\end{figure}

\begin{figure}[p]
  \caption{CPU utilization while committing many 1KiB files to an empty
  repository}
  \label{fig:plot-num-files--c1-cpu}
  \centering
    \includegraphics[]{plot-num-files--c1-cpu-a}
    \includegraphics[]{plot-num-files--c1-cpu-b}
\end{figure}

\begin{figure}[p]
  \caption{Real time required to check the status of many 1KiB files after
  initial commit}
  \label{fig:plot-num-files--stat1-time}
  \centering
    \includegraphics[]{plot-num-files--stat1-time}
\end{figure}

\begin{figure}[p]
  \caption{Total repository size after committing, editing, and committing again}
  \label{fig:plot-num-files--repo-size}
  \centering
    \includegraphics[]{plot-num-files--repo-size}
\end{figure}

\autoref{fig:plot-num-files--c1-time} shows the time
required for the initial commit, copying all files into a fresh empty
repository.

\autoref{fig:plot-num-files--c1-cpu} shows CPU utilization
during the commit.

\autoref{fig:plot-num-files--stat1-time} shows the time
required to check the changed status of all files just after committing.

\autoref{fig:plot-num-files--repo-size} shows the total
repository size, including the original files, after committing once, editing
1/1024th of every sixteenth file, and committing again.


\iffalse

We performed a test where increasingly large sets of files were committed to the
different version control repositories. The procedure was as follows:

\begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item Initialize an empty repository
    \item Generate a test file set of the given size. Each file is 1KiB of
        random binary data
    \item Commit the file set
    \item Check the status of the files
    \item Overwrite a small part of some of the files (1/1024th of the data in
        1/16 files)
    \item Check the status of the files again
    \item Commit the file set again
\end{enumerate}

Unlike the test with a single large file, the numerous small files did not
quickly hit error-causing disk space or RAM limitations. The version control
systems happily crunched the data as test times grew into hours.

Observations:

\begin{itemize}

    \item Again, after some initial overhead, commit and times increase
        linearly. However, Git's initial commit times actually decrease at
        certain points (128Ki, 1.5Mi, and 2Mi files). We are not sure how to
        explain this.

    \item Git in general is faster then Mercurial up to about half a million
        files. At 512Ki files is Mercurial and Git are about neck and neck. At
        768Ki and over, Mercurial is faster.

    \item Status check times are more erratic, though still increasing linearly
        overall. The variations may have to do with the output of the status
        commands and whether our terminal multiplexer was focused on the
        execution at the time. The status commands print one status line per
        file changed, which can be significant when hundreds of thousands of
        files involved. This output is significantly slower when the terminal
        multiplexer we used to monitor the experiments is connected, because it
        sends every line over the network to the monitoring machine.

    \item Mercurial update status is consistently slower than initial status,
        often by about 2-3x.

    \item Both Git and Mercurial converge to a little over 8x the space
        required. This probably has more to do with the filesystem block size
        than anything else. The underlying file system uses a 4KiB block size,
        so each 1KiB file will still use 4KiB of disk space. And since there are
        two copies of each file, that's 8KiB total for each 1KiB file, 8x the
        disk space.

    \item Mercurial converges towards the 8x limit faster though. We guess this
        is because of lower repo overhead, and also because Git is creating tree
        objects for each of the subdirectories in the file set. These files will
        be small, but each will take up another 4KiB block on the disk.

    \item Mercurial commits began to abort with disk space errors at 8Mi files,
        8GiB of data. This was surprising. Even at 8x disk usage, that should
        only be 64GiB of disk usage, well below the 192GiB free on the test
        disk.

\end{itemize}

\fi


\section{Performance Tuning}


\subsection{Rolling Hash Chunk Size}\label{rolling-hash-exp}

\subsubsection{Methodology}

This experiment is written as a unit test in the prototype Rust code. The test
is found in the \texttt{rolling\_hash} module, under the name
\texttt{chunk\_size\_experiment}.

\begin{itemize}

  \item Generate random data and pass it through the rolling hash algorithm with
    the given window size and match parameters.

  \item Continue generating data until the rolling hash has flagged 100 chunks.

  \item Calculate the mean chunk size and standard deviation.

\end{itemize}

\subsubsection{Results}

\begin{figure}[p]
  \caption{Mean chunk size}
  \label{fig:plot-rolling-hash}
  \centering
    \includegraphics[]{plot-rolling-hash}
\end{figure}

\autoref{fig:plot-rolling-hash} shows the mean chunk size.

\subsubsection{Observations}

\begin{itemize}

  \item The mean chunk size is approximately the sum of the window size and
    match parameter.

  \item When the match parameter is less than the window size, the standard
    deviation is approximately the match parameter.

\end{itemize}


\subsection{Directory Structure}

\subsubsection{Methodology}

\begin{itemize}

  \item Create an empty 100MiB partition.

  \item Write 4KiB objects according to the given object directory scheme, until
    the disk is reported as full.

  \item Track the total number of files written and total number of directories
    created.

\end{itemize}

\subsubsection{Results}

\begin{figure}[p]
  \caption{Overwhelmed by subdirectories}
  \label{fig:plot-filesystem-limits--directory-takeover}
  \centering
    \includegraphics[]{plot-filesystem-limits--directory-takeover}
\end{figure}

\autoref{fig:plot-filesystem-limits--directory-takeover} shows directories
overtaking files as nesting goes deeper.

\subsubsection{Observations}

\begin{itemize}

  \item In hindsight, it should have been easy to see that the number of
    directories would grow exponentially.

\end{itemize}
